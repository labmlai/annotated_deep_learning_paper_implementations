{
 "<h1>Feedback Transformer</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2002.09402\">Accessing Higher-level Representations in Sequential Transformers with Feedback Memory</a>.</p>\n<p>Normal transformers process tokens in parallel. Each transformer layer pays attention to the outputs of the previous layer. Feedback transformer pays attention to the output of all layers in previous steps. So this adds recurrence, and we need to process token-by-token. This slows down the training significantly (about 5X - 10X depending on the sequence length). However, when predicting Feedback Transformer is faster because you can predict the next token if you cache the memory vectors.</p>\n<p>In order to speed up the training, the paper discusses starting with a short sequence length and gradually increasing it. They also discuss using a pretrained parallel transformer as the starting point.</p>\n<p>The original feedback transformer doesn&#x27;t keep the outputs of all layers. Instead it keeps weighted sum of the output of all layers. This reduces the memory used for caching during prediction. The first half of this file implements this.</p>\n<p>The updated feedback transformer shares weights <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> used to calculate keys and values among the layers. We then calculate the keys and values for each step only once and keep them cached. The <a href=\"#shared_kv\">second half</a> of this file implements this. We implemented a custom PyTorch function to improve performance.</p>\n<p>Here&#x27;s <a href=\"experiment.html\">the training code</a> and a notebook for training a feedback transformer on Tiny Shakespeare dataset.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/feedback/experiment.ipynb\"><span translate=no>_^_2_^_</span></a></p>\n": "<h1>\u53cd\u9988\u53d8\u538b\u5668</h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch \u5bf9</a>\u300a\u4f7f\u7528<a href=\"https://arxiv.org/abs/2002.09402\">\u53cd\u9988\u5b58\u50a8\u5668\u8bbf\u95ee\u5e8f\u5217\u53d8\u538b\u5668\u4e2d\u7684\u66f4\u9ad8\u5c42\u6b21\u8868\u793a\u300b\u4e00\u6587\u7684 PyT</a> orch \u5b9e\u73b0\u3002</p>\n<p>\u666e\u901a\u7684\u53d8\u538b\u5668\u4f1a\u5e76\u884c\u5904\u7406\u4ee3\u5e01\u3002\u6bcf\u4e2a\u53d8\u538b\u5668\u5c42\u90fd\u6ce8\u610f\u524d\u4e00\u5c42\u7684\u8f93\u51fa\u3002\u53cd\u9988\u53d8\u538b\u5668\u6ce8\u610f\u524d\u9762\u6b65\u9aa4\u4e2d\u6240\u6709\u5c42\u7684\u8f93\u51fa\u3002\u56e0\u6b64\uff0c\u8fd9\u4f1a\u589e\u52a0\u91cd\u590d\u6027\uff0c\u6211\u4eec\u9700\u8981\u9010\u4e2a\u4ee3\u5e01\u8fdb\u884c\u5904\u7406\u3002\u8fd9\u4f1a\u663e\u8457\u51cf\u6162\u8bad\u7ec3\u901f\u5ea6\uff08\u5927\u7ea6 5 \u5230 10 \u500d\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u5e8f\u5217\u957f\u5ea6\uff09\u3002\u4f46\u662f\uff0c\u5728\u9884\u6d4b\u53cd\u9988\u53d8\u6362\u5668\u65f6\uff0c\u901f\u5ea6\u66f4\u5feb\uff0c\u56e0\u4e3a\u5982\u679c\u4f60\u7f13\u5b58\u4e86\u5185\u5b58\u5411\u91cf\uff0c\u4f60\u53ef\u4ee5\u9884\u6d4b\u4e0b\u4e00\u4e2a\u6807\u8bb0\u3002</p>\n<p>\u4e3a\u4e86\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6\uff0c\u672c\u6587\u8ba8\u8bba\u4e86\u4ece\u77ed\u5e8f\u5217\u957f\u5ea6\u5f00\u59cb\u5e76\u9010\u6e10\u589e\u52a0\u5e8f\u5217\u957f\u5ea6\u7684\u95ee\u9898\u3002\u4ed6\u4eec\u8fd8\u8ba8\u8bba\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u5e76\u884c\u53d8\u538b\u5668\u4f5c\u4e3a\u8d77\u70b9\u3002</p>\n<p>\u539f\u59cb\u53cd\u9988\u53d8\u538b\u5668\u4e0d\u4fdd\u7559\u6240\u6709\u5c42\u7684\u8f93\u51fa\u3002\u76f8\u53cd\uff0c\u5b83\u4fdd\u7559\u6240\u6709\u56fe\u5c42\u8f93\u51fa\u7684\u52a0\u6743\u603b\u548c\u3002\u8fd9\u51cf\u5c11\u4e86\u9884\u6d4b\u671f\u95f4\u7528\u4e8e\u7f13\u5b58\u7684\u5185\u5b58\u3002\u8fd9\u4e2a\u6587\u4ef6\u7684\u524d\u534a\u90e8\u5206\u5b9e\u73b0\u4e86\u8fd9\u4e00\u70b9\u3002</p>\n<p>\u66f4\u65b0\u540e\u7684\u53cd\u9988\u53d8\u538b\u5668\u5171\u4eab\u6743\u91cd<span translate=no>_^_0_^_</span>\uff0c<span translate=no>_^_1_^_</span>\u7528\u4e8e\u8ba1\u7b97\u5404\u5c42\u4e4b\u95f4\u7684\u5bc6\u94a5\u548c\u503c\u3002\u7136\u540e\uff0c\u6211\u4eec\u53ea\u8ba1\u7b97\u6bcf\u4e2a\u6b65\u9aa4\u7684\u952e\u548c\u503c\u4e00\u6b21\uff0c\u5e76\u5c06\u5176\u7f13\u5b58\u3002\u8fd9\u4e2a\u6587\u4ef6\u7684<a href=\"#shared_kv\">\u540e\u534a</a>\u90e8\u5206\u5b9e\u73b0\u4e86\u8fd9\u4e00\u70b9\u3002\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u81ea\u5b9a\u4e49 PyTorch \u51fd\u6570\u6765\u63d0\u9ad8\u6027\u80fd\u3002</p>\n<p>\u8fd9\u662f<a href=\"experiment.html\">\u8bad\u7ec3\u4ee3\u7801\u548c\u4e00\u672c</a>\u7528\u4e8e\u5728 Tiny Shakespeare \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u53cd\u9988\u8f6c\u6362\u5668\u7684\u7b14\u8bb0\u672c\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/feedback/experiment.ipynb\"><span translate=no>_^_2_^_</span></a></p>\n",
 "<h2>Feedback Attention</h2>\n<p>This module computes recurrent attention similar to attention from original transformers paper.</p>\n<p><span translate=no>_^_0_^_</span></p>\n": "<h2>\u53cd\u9988\u5173\u6ce8</h2>\n<p>\u672c\u6a21\u5757\u8ba1\u7b97\u91cd\u590d\u6ce8\u610f\u529b\uff0c\u7c7b\u4f3c\u4e8e\u539f\u7248\u300a\u53d8\u5f62\u91d1\u521a\u300b\u8bba\u6587\u4e2d\u7684\u6ce8\u610f\u529b\u3002</p>\n<p><span translate=no>_^_0_^_</span></p>\n",
 "<h2>Feedback Transformer Layer</h2>\n<p>This implements a single transformer layer in the feedback transformer.</p>\n": "<h2>\u53cd\u9988\u53d8\u538b\u5668\u5c42</h2>\n<p>\u8fd9\u5728\u53cd\u9988\u53d8\u538b\u5668\u4e2d\u5b9e\u73b0\u4e86\u5355\u4e2a\u53d8\u538b\u5668\u5c42\u3002</p>\n",
 "<h2>Feedback Transformer Module</h2>\n": "<h2>\u53cd\u9988\u53d8\u538b\u5668\u6a21\u5757</h2>\n",
 "<h2>Updated Feedback Transformer Module</h2>\n<p>This is the updated feedback transformer module that caches the keys and values.</p>\n": "<h2>\u66f4\u65b0\u4e86\u53cd\u9988\u53d8\u538b\u5668\u6a21\u5757</h2>\n<p>\u8fd9\u662f\u66f4\u65b0\u7684\u53cd\u9988\u8f6c\u6362\u5668\u6a21\u5757\uff0c\u7528\u4e8e\u7f13\u5b58\u952e\u548c\u503c\u3002</p>\n",
 "<h3>Get attention scores</h3>\n<p>We use relative positional encodings for attention, similar to <a href=\"../relative_mha.html\">relative multi-head attention form Transformer-XL paper</a>.</p>\n<p>Attention from current step&#x27;s query to key in step <span translate=no>_^_0_^_</span> (relative to current step) is,</p>\n<span translate=no>_^_1_^_</span><p>where <span translate=no>_^_2_^_</span>, are linear transformations of  original embeddings <span translate=no>_^_3_^_</span>  and <span translate=no>_^_4_^_</span> are linear transformations of  positional encodings <span translate=no>_^_5_^_</span>.</p>\n<p>We replace term <span translate=no>_^_6_^_</span> with <span translate=no>_^_7_^_</span>.</p>\n": "<h3>\u83b7\u5f97\u6ce8\u610f\u529b\u5206\u6570</h3>\n<p>\u6211\u4eec\u4f7f\u7528\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u6765\u8868\u793a\u6ce8\u610f\u529b\uff0c\u7c7b\u4f3c\u4e8e Transf <a href=\"../relative_mha.html\">ormer-XL \u7eb8\u5f20\u7684\u76f8\u5bf9\u591a\u5934\u6ce8\u610f\u529b</a>\u3002</p>\n<p>\u4ece\u5f53\u524d\u6b65\u9aa4\u7684\u67e5\u8be2\u5230\u952e\u5165\u6b65\u9aa4<span translate=no>_^_0_^_</span>\uff08\u76f8\u5bf9\u4e8e\u5f53\u524d\u6b65\u9aa4\uff09\u7684\u6ce8\u610f\u662f\uff0c</p>\n<span translate=no>_^_1_^_</span><p>\u5176\u4e2d<span translate=no>_^_2_^_</span>\uff0c\u662f\u539f\u59cb\u5d4c\u5165\u7684\u7ebf\u6027\u53d8\u6362<span translate=no>_^_3_^_</span>\uff0c<span translate=no>_^_4_^_</span>\u662f\u4f4d\u7f6e\u7f16\u7801\u7684\u7ebf\u6027\u53d8\u6362<span translate=no>_^_5_^_</span>\u3002</p>\n<p>\u6211\u4eec\u5c06\u672f\u8bed<span translate=no>_^_6_^_</span>\u66ff\u6362\u4e3a<span translate=no>_^_7_^_</span>\u3002</p>\n",
 "<h3>Stack Function implementation</h3>\n<p>We implement a custom function instead of appending to a python list and then doing <span translate=no>_^_0_^_</span>. This greatly improves the performance over calling <span translate=no>_^_1_^_</span> at each step along the sequence. Everytime <span translate=no>_^_2_^_</span> is called, it creates a new tensor, while this method and the accompanying class <span translate=no>_^_3_^_</span> share memory for each step.</p>\n": "<h3>\u5806\u6808\u51fd\u6570\u5b9e\u73b0</h3>\n<p>\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u81ea\u5b9a\u4e49\u51fd\u6570\uff0c\u800c\u4e0d\u662f\u8ffd\u52a0\u5230python\u5217\u8868\u7136\u540e\u505a<span translate=no>_^_0_^_</span>\u3002\u4e0e\u987a\u5e8f\u4e2d\u6bcf\u4e2a\u6b65\u9aa4\u7684\u8c03\u7528\u76f8\u6bd4<span translate=no>_^_1_^_</span>\uff0c\u8fd9\u5927\u5927\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u6bcf\u6b21\u8c03\u7528<span translate=no>_^_2_^_</span>\u65f6\uff0c\u5b83\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u800c\u6b64\u65b9\u6cd5\u548c\u968f\u9644\u7684\u7c7b<span translate=no>_^_3_^_</span>\u5171\u4eab\u6bcf\u4e2a\u6b65\u9aa4\u7684\u5185\u5b58\u3002</p>\n",
 "<h3>Stack Module</h3>\n<p>This uses the stack function defined above, and does the necessary initializations.</p>\n": "<h3>\u5806\u53e0\u6a21\u5757</h3>\n<p>\u8fd9\u4f7f\u7528\u4e0a\u9762\u5b9a\u4e49\u7684\u5806\u6808\u51fd\u6570\uff0c\u5e76\u8fdb\u884c\u5fc5\u8981\u7684\u521d\u59cb\u5316\u3002</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> Returns the stack</p>\n": "<p>\u8fd4\u56de\u5806\u6808</p>\n",
 "<p> To release memory</p>\n": "<p>\u91ca\u653e\u5185\u5b58</p>\n",
 "<p><a id=\"shared_kv\"></a></p>\n<h1>Shared keys and values among layers</h1>\n": "<p><a id=\"shared_kv\"></a></p>\n<h1>\u5c42\u95f4\u5171\u4eab\u5bc6\u94a5\u548c\u503c</h1>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>Add the feed-forward results back </p>\n": "<p>\u5c06\u524d\u9988\u7ed3\u679c\u6dfb\u52a0\u56de\u6765</p>\n",
 "<p>Add the gradients </p>\n": "<p>\u6dfb\u52a0\u6e10\u53d8</p>\n",
 "<p>Add the self attention results </p>\n": "<p>\u6dfb\u52a0\u81ea\u6211\u5173\u6ce8\u7684\u7ed3\u679c</p>\n",
 "<p>Append the output to results </p>\n": "<p>\u5c06\u8f93\u51fa\u8ffd\u52a0\u5230\u7ed3\u679c\u4e2d</p>\n",
 "<p>Append them to the list of layer outputs </p>\n": "<p>\u5c06\u5b83\u4eec\u8ffd\u52a0\u5230\u56fe\u5c42\u8f93\u51fa\u5217\u8868\u4e2d</p>\n",
 "<p>Apply dropout </p>\n": "<p>\u7533\u8bf7\u9000\u5b66</p>\n",
 "<p>Cache accumulated gradients </p>\n": "<p>\u7f13\u5b58\u7d2f\u79ef\u6e10\u53d8</p>\n",
 "<p>Cache the size of the stack </p>\n": "<p>\u7f13\u5b58\u5806\u6808\u7684\u5927\u5c0f</p>\n",
 "<p>Calculate the keys from memory and add it to the stack </p>\n": "<p>\u4ece\u5185\u5b58\u4e2d\u8ba1\u7b97\u5bc6\u94a5\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u5806\u6808\u4e2d</p>\n",
 "<p>Calculate the memory vector as a weighted sum of layer outputs </p>\n": "<p>\u5c06\u5185\u5b58\u77e2\u91cf\u8ba1\u7b97\u4e3a\u56fe\u5c42\u8f93\u51fa\u7684\u52a0\u6743\u603b\u548c</p>\n",
 "<p>Calculate the values from memory and add it to the stack </p>\n": "<p>\u8ba1\u7b97\u5185\u5b58\u4e2d\u7684\u503c\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u5806\u6808\u4e2d</p>\n",
 "<p>Compute attention scores. Results in a tensor of shape <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\u3002\u7ed3\u679c\u4e3a\u5f62\u72b6\u7684\u5f20\u91cf<span translate=no>_^_0_^_</span></p>\n",
 "<p>Concatenate multiple heads </p>\n": "<p>\u8fde\u63a5\u591a\u4e2a\u5934</p>\n",
 "<p>Create a tensor for the stack </p>\n": "<p>\u4e3a\u5806\u6808\u521b\u5efa\u5f20\u91cf</p>\n",
 "<p>Create a tensor to accumulate the gradients </p>\n": "<p>\u521b\u5efa\u5f20\u91cf\u6765\u7d2f\u79ef\u68af\u5ea6</p>\n",
 "<p>Do this without gradients </p>\n": "<p>\u5728\u6ca1\u6709\u6e10\u53d8\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u6b64\u64cd\u4f5c</p>\n",
 "<p>Dropout </p>\n": "<p>\u8f8d\u5b66</p>\n",
 "<p>Final normalization layer </p>\n": "<p>\u6700\u7ec8\u5f52\u4e00\u5316\u5c42</p>\n",
 "<p>For each input step </p>\n": "<p>\u5bf9\u4e8e\u6bcf\u4e2a\u8f93\u5165\u6b65\u9aa4</p>\n",
 "<p>Get layer output </p>\n": "<p>\u83b7\u53d6\u56fe\u5c42\u8f93\u51fa</p>\n",
 "<p>Get the accumulated gradients </p>\n": "<p>\u83b7\u53d6\u7d2f\u79ef\u7684\u68af\u5ea6</p>\n",
 "<p>Get the current size of the stack </p>\n": "<p>\u83b7\u53d6\u5806\u6808\u7684\u5f53\u524d\u5927\u5c0f</p>\n",
 "<p>Get the keys and values tensors if we are beyond the initial step </p>\n": "<p>\u5982\u679c\u6211\u4eec\u8d85\u51fa\u4e86\u521d\u59cb\u6b65\u9aa4\uff0c\u5219\u83b7\u53d6\u952e\u548c\u503c\u5f20\u91cf</p>\n",
 "<p>If there is memory </p>\n": "<p>\u5982\u679c\u6709\u8bb0\u5fc6</p>\n",
 "<p>If there is memory, stack them into a vector </p>\n": "<p>\u5982\u679c\u6709\u5185\u5b58\uff0c\u5219\u5c06\u5b83\u4eec\u5806\u53e0\u6210\u4e00\u4e2a\u5411\u91cf</p>\n",
 "<p>Initialize the shared memory tensor to keep the stack </p>\n": "<p>\u521d\u59cb\u5316\u5171\u4eab\u5185\u5b58\u5f20\u91cf\u4ee5\u4fdd\u7559\u5806\u6808</p>\n",
 "<p>Keep track of the last value added to the stack. We need this to be passed on to <span translate=no>_^_0_^_</span> in order to get the gradients propagated backwards. </p>\n": "<p>\u8ddf\u8e2a\u6dfb\u52a0\u5230\u5806\u6808\u7684\u6700\u540e\u4e00\u4e2a\u503c\u3002\u4e3a\u4e86\u8ba9\u68af\u5ea6\u5411\u540e\u4f20\u64ad\uff0c\u6211\u4eec\u9700\u8981\u5c06\u5176\u4f20\u9012\u7ed9\u3002<span translate=no>_^_0_^_</span></p>\n",
 "<p>Keep track of the size of the stack when it was used. This is used for a sanity check in <span translate=no>_^_0_^_</span>. </p>\n": "<p>\u8ddf\u8e2a\u4f7f\u7528\u65f6\u5806\u6808\u7684\u5927\u5c0f\u3002\u8fd9\u7528\u4e8e\u5065\u5168\u6027\u68c0\u5165<span translate=no>_^_0_^_</span>\u3002</p>\n",
 "<p>Keep track of the stack (for debugging) </p>\n": "<p>\u8ddf\u8e2a\u5806\u6808\uff08\u7528\u4e8e\u8c03\u8bd5\uff09</p>\n",
 "<p>Keys and values are already calculated </p>\n": "<p>\u952e\u548c\u503c\u5df2\u8ba1\u7b97</p>\n",
 "<p>List to store layer outputs </p>\n": "<p>\u5b58\u50a8\u56fe\u5c42\u8f93\u51fa\u7684\u5217\u8868</p>\n",
 "<p>List to store the memory vectors </p>\n": "<p>\u5b58\u50a8\u8bb0\u5fc6\u5411\u91cf\u7684\u5217\u8868</p>\n",
 "<p>List to store the outputs </p>\n": "<p>\u5b58\u50a8\u8f93\u51fa\u7684\u5217\u8868</p>\n",
 "<p>Make copies of the transformer layer </p>\n": "<p>\u5236\u4f5c\u53d8\u538b\u5668\u5c42\u7684\u526f\u672c</p>\n",
 "<p>Memory for stacked keys </p>\n": "<p>\u5806\u53e0\u6309\u952e\u7684\u5b58\u50a8\u7a7a\u95f4</p>\n",
 "<p>Memory for stacked values </p>\n": "<p>\u5806\u53e0\u503c\u7684\u5185\u5b58</p>\n",
 "<p>Memory vectors are computed as a weighted sum of representations of each layer. This is the weights parameter for that. </p>\n": "<p>\u5185\u5b58\u5411\u91cf\u8ba1\u7b97\u4e3a\u6bcf\u4e2a\u56fe\u5c42\u8868\u793a\u7684\u52a0\u6743\u603b\u548c\u3002\u8fd9\u662f\u8be5\u53c2\u6570\u7684\u6743\u91cd\u53c2\u6570\u3002</p>\n",
 "<p>Module to transform embeddings (memory) to get keys </p>\n": "<p>\u8f6c\u6362\u5d4c\u5165\uff08\u5185\u5b58\uff09\u4ee5\u83b7\u53d6\u5bc6\u94a5\u7684\u6a21\u5757</p>\n",
 "<p>Multiply by the values </p>\n": "<p>\u4e58\u4ee5\u503c</p>\n",
 "<p>Normalization layers </p>\n": "<p>\u5f52\u4e00\u5316\u5c42</p>\n",
 "<p>Normalize for feed-forward </p>\n": "<p>\u6807\u51c6\u5316\u4ee5\u8fdb\u884c\u524d\u9988</p>\n",
 "<p>Normalize the output </p>\n": "<p>\u89c4\u8303\u5316\u8f93\u51fa</p>\n",
 "<p>Normalize the vectors before doing self attention </p>\n": "<p>\u5728\u8fdb\u884c\u81ea\u6211\u6ce8\u610f\u4e4b\u524d\u5bf9\u5411\u91cf\u8fdb\u884c\u5f52\u4e00\u5316</p>\n",
 "<p>Number of features in a head </p>\n": "<p>\u5934\u90e8\u7279\u5f81\u7684\u6570\u91cf</p>\n",
 "<p>Number of features per head </p>\n": "<p>\u6bcf\u5934\u7279\u5f81\u6570</p>\n",
 "<p>Number of relative positions </p>\n": "<p>\u76f8\u5bf9\u4f4d\u7f6e\u7684\u6570\u91cf</p>\n",
 "<p>Output layer </p>\n": "<p>\u8f93\u51fa\u5c42</p>\n",
 "<p>Pass through the feed-forward network </p>\n": "<p>\u901a\u8fc7\u524d\u9988\u7f51\u7edc</p>\n",
 "<p>Positional embeddings for the query is independent of the position of the query </p>\n": "<p>\u67e5\u8be2\u7684\u4f4d\u7f6e\u5d4c\u5165\u4e0e\u67e5\u8be2\u7684\u4f4d\u7f6e\u65e0\u5173</p>\n",
 "<p>Prepare <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> for attention computation <span translate=no>_^_3_^_</span> and <span translate=no>_^_4_^_</span> will then have shape <span translate=no>_^_5_^_</span> and <span translate=no>_^_6_^_</span> will have shape <span translate=no>_^_7_^_</span> </p>\n": "<p>\u505a\u597d\u51c6\u5907<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\uff0c<span translate=no>_^_2_^_</span>\u8fdb\u884c\u6ce8\u610f\u529b\u8ba1\u7b97<span translate=no>_^_3_^_</span>\uff0c\u7136\u540e<span translate=no>_^_4_^_</span>\u5c31\u4f1a\u6709\u5f62\u72b6<span translate=no>_^_5_^_</span>\u800c\u4e14<span translate=no>_^_6_^_</span>\u4f1a\u6709\u5f62\u72b6<span translate=no>_^_7_^_</span></p>\n",
 "<p>Relative positional embedding bias for key relative to the query. </p>\n": "<p>\u952e\u76f8\u5bf9\u4e8e\u67e5\u8be2\u7684\u76f8\u5bf9\u4f4d\u7f6e\u5d4c\u5165\u504f\u5dee\u3002</p>\n",
 "<p>Relative positional embeddings for key relative to the query. </p>\n": "<p>\u952e\u76f8\u5bf9\u4e8e\u67e5\u8be2\u7684\u76f8\u5bf9\u4f4d\u7f6e\u5d4c\u5165\u3002</p>\n",
 "<p>Reset accumulated gradients </p>\n": "<p>\u91cd\u7f6e\u7d2f\u79ef\u68af\u5ea6</p>\n",
 "<p>Return the gradients w.r.t to last value in the stack </p>\n": "<p>\u5c06 w.r.t \u7684\u68af\u5ea6\u8fd4\u56de\u5230\u5806\u6808\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u503c</p>\n",
 "<p>Return the stack </p>\n": "<p>\u8fd4\u56de\u5806\u6808</p>\n",
 "<p>Run through each layer </p>\n": "<p>\u7a7f\u8fc7\u6bcf\u4e00\u5c42</p>\n",
 "<p>Run through self attention, i.e. keys and values are from self </p>\n": "<p>\u901a\u8fc7\u81ea\u6211\u5173\u6ce8\uff0c\u5373\u5173\u952e\u548c\u4ef7\u503c\u6765\u81ea\u81ea\u6211</p>\n",
 "<p>Scale scores <span translate=no>_^_0_^_</span> </p>\n": "<p>\u97f3\u9636\u5206\u6570<span translate=no>_^_0_^_</span></p>\n",
 "<p>Scaling factor before the softmax </p>\n": "<p>softmax \u4e4b\u524d\u7684\u7f29\u653e\u7cfb\u6570</p>\n",
 "<p>Set the value in the correct position of the stack </p>\n": "<p>\u5c06\u503c\u8bbe\u7f6e\u5728\u5806\u6808\u7684\u6b63\u786e\u4f4d\u7f6e</p>\n",
 "<p>Softmax </p>\n": "<p>\u8f6f\u6700\u5927</p>\n",
 "<p>Softmax for attention along the time dimension of <span translate=no>_^_0_^_</span> </p>\n": "<p>Softmax \u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u5f15\u8d77\u4eba\u4eec\u7684\u6ce8\u610f<span translate=no>_^_0_^_</span></p>\n",
 "<p>Softmax for weights before taking the weighted sum </p>\n": "<p>Softmax \u7528\u4e8e\u8ba1\u7b97\u52a0\u6743\u603b\u548c\u4e4b\u524d\u7684\u6743\u91cd</p>\n",
 "<p>Split the input to a list along the sequence axis </p>\n": "<p>\u6cbf\u5e8f\u5217\u8f74\u5c06\u8f93\u5165\u62c6\u5206\u4e3a\u4e00\u4e2a\u5217\u8868</p>\n",
 "<p>Stack of keys and values </p>\n": "<p>\u952e\u548c\u503c\u7684\u5806\u6808</p>\n",
 "<p>Stack the layer outputs to a tensor </p>\n": "<p>\u5c06\u5c42\u8f93\u51fa\u5806\u53e0\u5230\u5f20\u91cf</p>\n",
 "<p>Stack the output tensors </p>\n": "<p>\u5806\u53e0\u8f93\u51fa\u5f20\u91cf</p>\n",
 "<p>Take it all through <span translate=no>_^_0_^_</span> so that <span translate=no>_^_1_^_</span> is called by PyTorch during backpropagation. </p>\n": "\u5168\u529b@@ <p>\u4ee5\u8d74\uff0c<span translate=no>_^_0_^_</span>\u8fd9\u6837 Py<span translate=no>_^_1_^_</span> Torch \u5728\u53cd\u5411\u4f20\u64ad\u671f\u95f4\u5c31\u4f1a\u8c03\u7528\u5b83\u3002</p>\n",
 "<p>The memory is already initialized but we are resetting the stack.</p>\n<p>This could have been another function like <span translate=no>_^_0_^_</span>, but we found this easier to use. </p>\n": "<p>\u5185\u5b58\u5df2\u7ecf\u521d\u59cb\u5316\uff0c\u4f46\u6211\u4eec\u6b63\u5728\u91cd\u7f6e\u5806\u6808\u3002</p>\n<p>\u8fd9\u53ef\u80fd\u662f\u53e6\u4e00\u4e2a\u7c7b\u4f3c\u7684\u51fd\u6570<span translate=no>_^_0_^_</span>\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u5b83\u66f4\u5bb9\u6613\u4f7f\u7528\u3002</p>\n",
 "<p>These transform the <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> for multi-headed attention. </p>\n": "<p>\u8fd9\u4e9b\u6539\u53d8\u4e86\u591a<span translate=no>_^_1_^_</span>\u5934\u6ce8\u610f\u529b\u7684<span translate=no>_^_0_^_</span>\u548c\u3002</p>\n",
 "<p>These transform the <span translate=no>_^_0_^_</span> multi-headed attention. </p>\n": "<p>\u8fd9\u4e9b\u6539\u53d8\u4e86<span translate=no>_^_0_^_</span>\u591a\u5934\u6ce8\u610f\u529b\u3002</p>\n",
 "<p>This should only happen when the stack is empty </p>\n": "<p>\u53ea\u6709\u5f53\u5806\u6808\u4e3a\u7a7a\u65f6\u624d\u4f1a\u53d1\u751f\u8fd9\u79cd\u60c5\u51b5</p>\n",
 "<p>Transformer size <span translate=no>_^_0_^_</span> </p>\n": "<p>\u53d8\u538b\u5668\u5c3a\u5bf8<span translate=no>_^_0_^_</span></p>\n",
 "<p>We store attentions so that it can be used for logging, or other computations if needed </p>\n": "<p>\u6211\u4eec\u5b58\u50a8\u6ce8\u610f\u4e8b\u9879\uff0c\u4ee5\u4fbf\u5728\u9700\u8981\u65f6\u5c06\u5176\u7528\u4e8e\u65e5\u5fd7\u8bb0\u5f55\u6216\u8fdb\u884c\u5176\u4ed6\u8ba1\u7b97</p>\n",
 "<p>You need to get (use) the stack after adding a value. Otherwise this implementation fails </p>\n": "<p>\u6dfb\u52a0\u503c\u540e\uff0c\u4f60\u9700\u8981\u83b7\u53d6\uff08\u4f7f\u7528\uff09\u5806\u6808\u3002\u5426\u5219\uff0c\u6b64\u5b9e\u73b0\u5c06\u5931\u8d25</p>\n",
 "<ul><li>&#x27;heads&#x27; is the number of attention heads </li>\n<li><span translate=no>_^_0_^_</span> is the number of features in the transformer </li>\n<li><span translate=no>_^_1_^_</span> is the attention dropout probability </li>\n<li><span translate=no>_^_2_^_</span> is whether key, value tensors are already calculated</li></ul>\n": "<ul><li>\u201cheads\u201d \u662f\u6ce8\u610f\u5934\u7684\u6570\u91cf</li>\n<li><span translate=no>_^_0_^_</span>\u662f\u53d8\u538b\u5668\u4e2d\u7684\u7279\u5f81\u6570</li>\n<li><span translate=no>_^_1_^_</span>\u6ce8\u610f\u529b\u4e22\u5931\u7684\u6982\u7387\u662f\u591a\u5c11</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u952e\u503c\u3001\u503c\u5f20\u91cf\u662f\u5426\u5df2\u7ecf\u8ba1\u7b97\u8fc7</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> has shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> and <span translate=no>_^_3_^_</span> has shape <span translate=no>_^_4_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u6709\u5f62\u72b6<span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u800c\u4e14<span translate=no>_^_3_^_</span>\u6709\u5f62\u72b6<span translate=no>_^_4_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the context of the function (which lets us cache stuff) </li>\n<li><span translate=no>_^_1_^_</span> is the shared memory tensor where we stack and store the values of each step (keys &amp; values) </li>\n<li><span translate=no>_^_2_^_</span> is the shared memory tensor to store and accumulate gradients of each step </li>\n<li><span translate=no>_^_3_^_</span> is the last value stacked </li>\n<li><span translate=no>_^_4_^_</span> is the number of steps (i.e. size of the stack)</li></ul>\n<p>This returns the stacked tensor for steps upto <span translate=no>_^_5_^_</span>.</p>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u51fd\u6570\u7684\u4e0a\u4e0b\u6587\uff08\u5b83\u5141\u8bb8\u6211\u4eec\u7f13\u5b58\u4e1c\u897f\uff09</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u5171\u4eab\u5185\u5b58\u5f20\u91cf\uff0c\u6211\u4eec\u5728\u5176\u4e2d\u5806\u53e0\u548c\u5b58\u50a8\u6bcf\u4e2a\u6b65\u9aa4\u7684\u503c\uff08\u952e\u548c\u503c\uff09</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u5b58\u50a8\u548c\u7d2f\u79ef\u6bcf\u6b65\u68af\u5ea6\u7684\u5171\u4eab\u5185\u5b58\u5f20\u91cf</li>\n<li><span translate=no>_^_3_^_</span>\u662f\u6700\u540e\u4e00\u4e2a\u5806\u53e0\u7684\u503c</li>\n<li><span translate=no>_^_4_^_</span>\u662f\u6b65\u6570\uff08\u5373\u5806\u6808\u7684\u5927\u5c0f\uff09</li></ul>\n<p>\u8fd9\u5c06\u8fd4\u56de\u6b65\u957f\u5230\u7684\u5806\u53e0\u5f20\u91cf<span translate=no>_^_5_^_</span>\u3002</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the feedback transformer layer, which we clone for each layer </li>\n<li><span translate=no>_^_1_^_</span> is the number of layers in the transformer </li>\n<li><span translate=no>_^_2_^_</span> is the number of features in the transformer </li>\n<li>&#x27;heads&#x27; is the number of attention heads</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u53cd\u9988\u53d8\u538b\u5668\u5c42\uff0c\u6211\u4eec\u4e3a\u6bcf\u5c42\u514b\u9686\u5b83</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u53d8\u538b\u5668\u4e2d\u7684\u5c42\u6570</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u53d8\u538b\u5668\u4e2d\u7684\u7279\u5f81\u6570</li>\n<li>\u201cheads\u201d \u662f\u6ce8\u610f\u5934\u7684\u6570\u91cf</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the feedback transformer layer, which we clone for each layer </li>\n<li><span translate=no>_^_1_^_</span> is the number of layers in the transformer</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u53cd\u9988\u53d8\u538b\u5668\u5c42\uff0c\u6211\u4eec\u4e3a\u6bcf\u5c42\u514b\u9686\u5b83</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u53d8\u538b\u5668\u4e2d\u7684\u5c42\u6570</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the gradient with respect to the output of about <span translate=no>_^_1_^_</span> function</li></ul>\n<p>This accumulates the gradients in the shared memory tensor and return the gradients with respect to the <span translate=no>_^_2_^_</span> result in the stack.</p>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u76f8\u5bf9\u4e8eabout<span translate=no>_^_1_^_</span> \u51fd\u6570\u8f93\u51fa\u7684\u68af\u5ea6</li></ul>\n<p>\u8fd9\u4f1a\u7d2f\u79ef\u5171\u4eab\u5185\u5b58\u5f20\u91cf\u4e2d\u7684\u68af\u5ea6\uff0c\u5e76\u8fd4\u56de\u76f8\u5bf9\u4e8e\u5806\u6808\u4e2d<span translate=no>_^_2_^_</span>\u7ed3\u679c\u7684\u68af\u5ea6\u3002</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input with shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u5e26\u5f62\u72b6\u7684\u8f93\u5165<span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the maximum size of the stack</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u5806\u6808\u7684\u6700\u5927\u5927\u5c0f</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features in the transformer </li>\n<li><span translate=no>_^_1_^_</span> is the feedback attention module </li>\n<li><span translate=no>_^_2_^_</span> is the position-wise feed forward layer </li>\n<li><span translate=no>_^_3_^_</span> is the dropout probability for dropout layers after attention and feed-forward</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u53d8\u538b\u5668\u4e2d\u7684\u7279\u5f81\u6570</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u53cd\u9988\u5173\u6ce8\u6a21\u5757</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u4f4d\u7f6e\u524d\u9988\u5c42</li>\n<li><span translate=no>_^_3_^_</span>\u662f\u6ce8\u610f\u548c\u524d\u9988\u540e\u8f8d\u5b66\u5c42\u7684\u4e22\u5931\u6982\u7387</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the size of the stack </li>\n<li><span translate=no>_^_1_^_</span> is the tensor that needs to be added to the stack</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u5806\u6808\u7684\u5927\u5c0f</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u9700\u8981\u6dfb\u52a0\u5230\u5806\u6808\u7684\u5f20\u91cf</li></ul>\n",
 "Feedback Transformer": "\u53cd\u9988\u53d8\u538b\u5668",
 "This is an annotated implementation/tutorial the Feedback Transformer in PyTorch.": "\u8fd9\u662f PyTorch \u4e2d\u7684\u53cd\u9988\u8f6c\u6362\u5668\u5e26\u6ce8\u91ca\u7684\u5b9e\u73b0/\u6559\u7a0b\u3002"
}