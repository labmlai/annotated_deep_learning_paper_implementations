{
 "<h1><a href=\"https://nn.labml.ai/transformers/xl/index.html\">Transformer XL</a></h1>\n<p>This is an implementation of <a href=\"https://arxiv.org/abs/1901.02860\">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n<p>Transformer has a limited attention span, equal to the length of the sequence trained in parallel. All these positions have a fixed positional encoding. Transformer XL increases this attention span by letting each of the positions pay attention to precalculated past embeddings. For instance if the context length is <span translate=no>_^_0_^_</span>, it will keep the embeddings of all layers for previous batch of length <span translate=no>_^_1_^_</span> and feed them to current step. If we use fixed-positional encodings these pre-calculated embeddings will have the same positions as the current context. They introduce relative positional encoding, where the positional encodings are introduced at the attention calculation.</p>\n<p>Annotated implementation of relative multi-headed attention is in <a href=\"https://nn.labml.ai/transformers/xl/relative_mha.html\"><span translate=no>_^_2_^_</span></a>.</p>\n<p>Here&#x27;s <a href=\"https://nn.labml.ai/transformers/xl/experiment.html\">the training code</a> and a notebook for training a transformer XL model on Tiny Shakespeare dataset.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/xl/experiment.ipynb\"><span translate=no>_^_3_^_</span></a> </p>\n": "<h1><a href=\"https://nn.labml.ai/transformers/xl/index.html\">\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc XL</a></h1>\n<p><a href=\"https://pytorch.org\">\u3053\u308c\u306f PyTorch \u306e <a href=\"https://arxiv.org/abs/1901.02860\">Transformer-XL: \u56fa\u5b9a\u9577\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u8d85\u3048\u305f\u6ce8\u610f\u6df1\u3044\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</a></p>\n<p>Transformer \u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30b9\u30d1\u30f3\u306f\u3001\u4e26\u884c\u3057\u3066\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u305f\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9577\u3055\u3068\u540c\u3058\u304f\u3089\u3044\u306e\u5236\u9650\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u4f4d\u7f6e\u306f\u3059\u3079\u3066\u56fa\u5b9a\u3055\u308c\u305f\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002Transformer XL\u306f\u3001\u4e8b\u524d\u306b\u8a08\u7b97\u3055\u308c\u305f\u904e\u53bb\u306e\u57cb\u3081\u8fbc\u307f\u306b\u5404\u30dd\u30b8\u30b7\u30e7\u30f3\u306b\u6ce8\u76ee\u3055\u305b\u308b\u3053\u3068\u3067\u3001\u3053\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30b9\u30d1\u30f3\u3092\u5897\u3084\u3057\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306e\u9577\u3055\u304c\u306e\u5834\u5408<span translate=no>_^_0_^_</span>\u3001<span translate=no>_^_1_^_</span>\u524d\u306e\u30d0\u30c3\u30c1\u306e\u9577\u3055\u306e\u3059\u3079\u3066\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u57cb\u3081\u8fbc\u307f\u3092\u4fdd\u6301\u3057\u3001\u305d\u308c\u3089\u3092\u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u306b\u9001\u308a\u307e\u3059\u3002\u56fa\u5b9a\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u3053\u308c\u3089\u306e\u4e8b\u524d\u306b\u8a08\u7b97\u3055\u308c\u305f\u57cb\u3081\u8fbc\u307f\u306f\u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3068\u540c\u3058\u4f4d\u7f6e\u306b\u306a\u308a\u307e\u3059\u3002\u76f8\u5bfe\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u304c\u5c0e\u5165\u3055\u308c\u3001\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\u6642\u306b\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u304c\u5c0e\u5165\u3055\u308c\u307e\u3059</p>\u3002\n<p>\u76f8\u5bfe\u7684\u591a\u9762\u7684\u6ce8\u610f\u306e\u6ce8\u91c8\u4ed8\u304d\u5b9f\u88c5\u304c\u5c0e\u5165\u3055\u308c\u307e\u3057\u305f\u3002<a href=\"https://nn.labml.ai/transformers/xl/relative_mha.html\"><span translate=no>_^_2_^_</span></a></p>\n<p>Tiny <a href=\"https://nn.labml.ai/transformers/xl/experiment.html\">Shakespeare\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fcXL\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u3068\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u3059</a>\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/xl/experiment.ipynb\"><span translate=no>_^_3_^_</span></a></p>\n",
 "Transformer XL": "\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc XL"
}