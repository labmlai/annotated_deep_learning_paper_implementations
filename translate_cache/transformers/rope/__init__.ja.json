{
 "<h1>Rotary Positional Embeddings (RoPE)</h1>\n<p>This is an implementation of <a href=\"https://arxiv.org/abs/2104.09864\">Rotary Positional Embeddings (RoPE)</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n<p>Rotary Positional Embeddings (RoPE) encode position information of tokens with a rotation matrix that naturally incorporates explicit relative position dependency.</p>\n<p>Here&#x27;s <a href=\"experiment.html\">the training code</a> for training a transformer model with RoPE  on Tiny Shakespeare dataset.</p>\n": "<h1>\u30ed\u30fc\u30bf\u30ea\u30fc\u30dd\u30b8\u30b7\u30e7\u30ca\u30eb\u30a8\u30f3\u30d9\u30c7\u30a3\u30f3\u30b0 (RoPE)</h1>\n<p><a href=\"https://pytorch.org\">\u3053\u308c\u306f PyTorch <a href=\"https://arxiv.org/abs/2104.09864\">\u306e\u30ed\u30fc\u30bf\u30ea\u30fc\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f (RoPE</a>) \u306e\u5b9f\u88c5\u3067\u3059\u3002</a></p>\n<p>Rotary Positional Embeddings\uff08RoPE\uff09\u306f\u3001\u30c8\u30fc\u30af\u30f3\u306e\u4f4d\u7f6e\u60c5\u5831\u3092\u56de\u8ee2\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u3067\u30a8\u30f3\u30b3\u30fc\u30c9\u3057\u307e\u3059\u3002\u56de\u8ee2\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u306b\u306f\u3001\u660e\u793a\u7684\u306a\u76f8\u5bfe\u4f4d\u7f6e\u4f9d\u5b58\u6027\u304c\u81ea\u7136\u306b\u7d44\u307f\u8fbc\u307e\u308c\u3066\u3044\u307e\u3059\u3002</p>\n<p>Tiny <a href=\"experiment.html\">Shakespeare\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067RoPE\u3092\u4f7f\u7528\u3057\u3066\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059</a>\u3002</p>\n",
 "<h2>Multi-head attention with rotary positional embeddings</h2>\n<p>We override <a href=\"../mha.html\">multi-head attention from original transformer</a>.</p>\n": "<h2>\u56de\u8ee2\u5f0f\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u306b\u3088\u308b\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3</h2>\n<p><a href=\"../mha.html\">\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092\u7121\u52b9\u306b\u3057\u307e\u3059</a>\u3002</p>\n",
 "<h2>RoPE module</h2>\n<p>Rotary encoding transforms pairs of features by rotating in the 2D plane. That is, it organizes the <span translate=no>_^_0_^_</span> features as <span translate=no>_^_1_^_</span> pairs. Each pair can be considered a coordinate in a 2D plane, and the encoding will rotate it by an angle depending on the position of the token.</p>\n<h3>For a pair of features</h3>\n<p>Let <span translate=no>_^_2_^_</span> and <span translate=no>_^_3_^_</span> be two features of the key or query of any head at position <span translate=no>_^_4_^_</span>. Or for simplicity assume <span translate=no>_^_5_^_</span> has only two features. Then the transformation is,</p>\n<span translate=no>_^_6_^_</span><p>where <span translate=no>_^_7_^_</span> is a constant angle. The other pairs of features are transformed similarly.</p>\n<h3>Attention is relative</h3>\n<p>For a pair of features, dot-product attention score between two positions <span translate=no>_^_8_^_</span> and <span translate=no>_^_9_^_</span> would be</p>\n<span translate=no>_^_10_^_</span><p>This shows that for dot-production attention the rotary encodings gives relative attention.</p>\n<h3>For all features</h3>\n<p>The features are grouped into pairs and handled as above. They use a different <span translate=no>_^_11_^_</span> for each pair.</p>\n<p>The paper suggests using <span translate=no>_^_12_^_</span> for the <span translate=no>_^_13_^_</span> pairs of features.</p>\n<p>We pair feature <span translate=no>_^_14_^_</span> with feature <span translate=no>_^_15_^_</span>. So for position <span translate=no>_^_16_^_</span> we transform</p>\n<span translate=no>_^_17_^_</span><p>to</p>\n<span translate=no>_^_18_^_</span>": "<h2>\u30ed\u30fc\u30d7\u30e2\u30b8\u30e5\u30fc\u30eb</h2>\n<p>\u30ed\u30fc\u30bf\u30ea\u30fc\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3067\u306f\u30012 \u3064\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u3092 2D \u5e73\u9762\u4e0a\u3067\u56de\u8ee2\u3055\u305b\u3066\u5909\u63db\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u30d5\u30a3\u30fc\u30c1\u30e3\u3092\u30da\u30a2\u3068\u3057\u3066\u6574\u7406\u3057\u307e\u3059\u3002\u5404\u30da\u30a2\u306f2D\u5e73\u9762\u5185\u306e\u5ea7\u6a19\u3068\u898b\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u3001\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3067\u306f\u30c8\u30fc\u30af\u30f3\u306e\u4f4d\u7f6e\u306b\u5fdc\u3058\u3066\u89d2\u5ea6\u3060\u3051\u56de\u8ee2\u3057\u307e\u3059\u3002</p>\n<h3>2 \u3064\u306e\u6a5f\u80fd\u306b\u3064\u3044\u3066</h3>\n<p><span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span>\u4efb\u610f\u306e\u4f4d\u7f6e\u3067\u4efb\u610f\u306e\u30d8\u30c3\u30c9\u306e\u30ad\u30fc\u307e\u305f\u306f\u30af\u30a8\u30ea\u306e2\u3064\u306e\u7279\u5fb4\u3068\u3057\u307e\u3057\u3087\u3046<span translate=no>_^_4_^_</span>\u3002\u307e\u305f\u306f\u3001\u7c21\u5358\u306b\u3059\u308b\u305f\u3081\u306b\u30012 <span translate=no>_^_5_^_</span> \u3064\u306e\u6a5f\u80fd\u3057\u304b\u6301\u3063\u3066\u3044\u306a\u3044\u3068\u4eee\u5b9a\u3057\u307e\u3059\u3002\u305d\u3046\u3059\u308b\u3068\u3001\u5909\u63db\u306f\u3001</p>\n<span translate=no>_^_6_^_</span><p>\u3053\u3053\u3067<span translate=no>_^_7_^_</span>\u3001\u306f\u4e00\u5b9a\u306e\u89d2\u5ea6\u3067\u3059\u3002\u4ed6\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u30da\u30a2\u3082\u540c\u69d8\u306b\u5909\u63db\u3055\u308c\u307e\u3059</p>\u3002\n<h3>\u6ce8\u610f\u306f\u76f8\u5bfe\u7684\u3067\u3059</h3>\n<p>2 \u3064\u306e\u7279\u5fb4\u306b\u3064\u3044\u3066\u30012 \u3064\u306e\u4f4d\u7f6e\u9593\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30b9\u30b3\u30a2\u3092\u70b9\u7a4d\u3059\u308b\u3068<span translate=no>_^_8_^_</span>\u3001<span translate=no>_^_9_^_</span></p>\n<span translate=no>_^_10_^_</span><p>\u3053\u306e\u3053\u3068\u304b\u3089\u3001\u30c9\u30c3\u30c8\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u3067\u6ce8\u76ee\u3055\u308c\u308b\u5834\u5408\u306f\u3001\u30ed\u30fc\u30bf\u30ea\u30fc\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u304c\u76f8\u5bfe\u7684\u306b\u6ce8\u76ee\u3055\u308c\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002</p>\n<h3>\u3059\u3079\u3066\u306e\u6a5f\u80fd\u306b\u5bfe\u5fdc</h3>\n<p>\u30d5\u30a3\u30fc\u30c1\u30e3\u306f\u30da\u30a2\u306b\u30b0\u30eb\u30fc\u30d7\u5316\u3055\u308c\u3001\u4e0a\u8a18\u306e\u3088\u3046\u306b\u51e6\u7406\u3055\u308c\u307e\u3059\u3002<span translate=no>_^_11_^_</span>\u5f7c\u3089\u306f\u30da\u30a2\u3054\u3068\u306b\u9055\u3046\u3082\u306e\u3092\u4f7f\u3044\u307e\u3059\u3002</p>\n<p>\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001<span translate=no>_^_12_^_</span><span translate=no>_^_13_^_</span>\u3053\u308c\u3089\u306e\u6a5f\u80fd\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u4f7f\u7528\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u307e\u3059\u3002</p>\n<p><span translate=no>_^_14_^_</span><span translate=no>_^_15_^_</span>\u6a5f\u80fd\u3068\u6a5f\u80fd\u3092\u7d44\u307f\u5408\u308f\u305b\u307e\u3059\u3002<span translate=no>_^_16_^_</span>\u3060\u304b\u3089\u4f4d\u7f6e\u306f\u5909\u8eab\u3059\u308b</p>\n<span translate=no>_^_17_^_</span><p>\u306b</p>\n<span translate=no>_^_18_^_</span>",
 "<h3>Calculate scores between queries and keys</h3>\n": "<h3>\u30af\u30a8\u30ea\u3068\u30ad\u30fc\u9593\u306e\u30b9\u30b3\u30a2\u306e\u8a08\u7b97</h3>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> Cache <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> values</p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u30ad\u30e3\u30c3\u30b7\u30e5\u3068\u5024</p>\n",
 "<p> Testing RoPE with a simple example</p>\n": "<p>\u7c21\u5358\u306a\u4f8b\u3067\u306e RoPE \u306e\u30c6\u30b9\u30c8</p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>Cache <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> values </p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u30ad\u30e3\u30c3\u30b7\u30e5\u3068\u5024</p>\n",
 "<p>Cache them </p>\n": "<p>\u305d\u308c\u3089\u3092\u30ad\u30e3\u30c3\u30b7\u30e5\u3059\u308b</p>\n",
 "<p>Calculate <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8a08\u7b97 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate dot-product with RoPE </p>\n": "<p>RoPE \u306b\u3088\u308b\u30c9\u30c3\u30c8\u7a4d\u306e\u8a08\u7b97</p>\n",
 "<p>Calculate the product of position index and <span translate=no>_^_0_^_</span> </p>\n": "<p>\u4f4d\u7f6e\u6307\u6570\u306e\u7a4d\u3092\u8a08\u7b97\u3057\u3001<span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate</p>\n<span translate=no>_^_0_^_</span><p>for <span translate=no>_^_1_^_</span> </p>\n": "<p>\u8a08\u7b97</p>\n<span translate=no>_^_0_^_</span><p>\u306b\u3068\u3063\u3066 <span translate=no>_^_1_^_</span></p>\n",
 "<p>Concatenate so that for row <span translate=no>_^_0_^_</span> we have <span translate=no>_^_1_^_</span> </p>\n": "<p>\u884c\u304c\u6b21\u306e\u3088\u3046\u306b\u306a\u308b\u3088\u3046\u306b\u9023\u7d50\u3057\u307e\u3059 <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span></p>\n",
 "<p>Create position indexes <span translate=no>_^_0_^_</span> </p>\n": "<p>\u4f4d\u7f6e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u4f5c\u6210 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Get sequence length </p>\n": "<p>\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9577\u3055\u3092\u53d6\u5f97</p>\n",
 "<p>Return if cache is already built </p>\n": "<p>\u30ad\u30e3\u30c3\u30b7\u30e5\u304c\u65e2\u306b\u69cb\u7bc9\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u8fd4\u3059</p>\n",
 "<p>Rotary positional embedding layers </p>\n": "<p>\u30ed\u30fc\u30bf\u30ea\u30fc\u30dd\u30b8\u30b7\u30e7\u30ca\u30eb\u57cb\u3081\u8fbc\u307f\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Split the features, we can choose to apply rotary embeddings only to a partial set of features. </p>\n": "<p>\u6a5f\u80fd\u3092\u5206\u5272\u3057\u3066\u3001\u4e00\u90e8\u306e\u6a5f\u80fd\u30bb\u30c3\u30c8\u306b\u306e\u307f\u30ed\u30fc\u30bf\u30ea\u30fc\u57cb\u3081\u8fbc\u307f\u3092\u9069\u7528\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the Tensor at the head of a key or a query with shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30ad\u30fc\u307e\u305f\u306f\u5f62\u72b6\u306e\u3042\u308b\u30af\u30a8\u30ea\u306e\u5148\u982d\u306b\u3042\u308b\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the constant used for calculating <span translate=no>_^_3_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u6a5f\u80fd\u306e\u6570 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u306f\u8a08\u7b97\u306b\u4f7f\u7528\u3055\u308c\u308b\u5b9a\u6570\u3067\u3059 <span translate=no>_^_3_^_</span></li></ul>\n",
 "Annotated implementation of RoPE from paper RoFormer: Enhanced Transformer with Rotary Position Embedding": "\u7d19\u88fd\u30ed\u30fc\u30d5\u30a9\u30fc\u30de\u30fc\u306b\u3088\u308bRoPE\u306e\u6ce8\u91c8\u4ed8\u304d\u5b9f\u88c5:\u56de\u8ee2\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u306b\u3088\u308b\u5f37\u5316\u578b\u5909\u5727\u5668",
 "Rotary Positional Embeddings (RoPE)": "\u30ed\u30fc\u30bf\u30ea\u30fc\u30dd\u30b8\u30b7\u30e7\u30ca\u30eb\u30a8\u30f3\u30d9\u30c7\u30a3\u30f3\u30b0 (RoPE)"
}