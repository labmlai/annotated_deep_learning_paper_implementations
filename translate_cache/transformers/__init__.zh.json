{
 "<h1>Transformers</h1>\n": "<h1>\u53d8\u538b\u5668</h1>\n",
 "<h2><a href=\"aft/index.html\">Attention Free Transformer</a></h2>\n": "<h2><a href=\"aft/index.html\">\u514d\u6ce8\u610f\u53d8\u538b\u5668</a></h2>\n",
 "<h2><a href=\"alibi/index.html\">Attention with Linear Biases</a></h2>\n": "<h2><a href=\"alibi/index.html\">\u6ce8\u610f\u7ebf\u6027\u504f\u5dee</a></h2>\n",
 "<h2><a href=\"compressive/index.html\">Compressive Transformer</a></h2>\n": "<h2><a href=\"compressive/index.html\">\u538b\u7f29\u53d8\u538b\u5668</a></h2>\n",
 "<h2><a href=\"fast_weights/index.html\">Fast Weights Transformer</a></h2>\n": "<h2><a href=\"fast_weights/index.html\">\u5feb\u901f\u91cd\u91cf\u53d8\u538b\u5668</a></h2>\n",
 "<h2><a href=\"feedback/index.html\">Feedback Transformer</a></h2>\n": "<h2><a href=\"feedback/index.html\">\u53cd\u9988\u53d8\u538b\u5668</a></h2>\n",
 "<h2><a href=\"fnet/index.html\">FNet: Mixing Tokens with Fourier Transforms</a></h2>\n": "<h2><a href=\"fnet/index.html\">FNet\uff1a\u5c06\u4ee4\u724c\u4e0e\u5085\u91cc\u53f6\u53d8\u6362\u6df7\u5408</a></h2>\n",
 "<h2><a href=\"glu_variants/simple.html\">GLU Variants</a></h2>\n": "<h2><a href=\"glu_variants/simple.html\">GLU \u53d8\u4f53</a></h2>\n",
 "<h2><a href=\"gmlp/index.html\">Pay Attention to MLPs (gMLP)</a></h2>\n": "<h2><a href=\"gmlp/index.html\">\u6ce8\u610f MLP (gMLP)</a></h2>\n",
 "<h2><a href=\"gpt/index.html\">GPT Architecture</a></h2>\n": "<h2><a href=\"gpt/index.html\">GPT \u67b6\u6784</a></h2>\n",
 "<h2><a href=\"hour_glass/index.html\">Hourglass</a></h2>\n": "<h2><a href=\"hour_glass/index.html\">\u6c99\u6f0f</a></h2>\n",
 "<h2><a href=\"knn/index.html\">kNN-LM</a></h2>\n": "<h2><a href=\"knn/index.html\">knn-lm</a></h2>\n",
 "<h2><a href=\"mlm/index.html\">Masked Language Model</a></h2>\n": "<h2><a href=\"mlm/index.html\">\u5c4f\u853d\u8bed\u8a00\u6a21\u578b</a></h2>\n",
 "<h2><a href=\"mlp_mixer/index.html\">MLP-Mixer: An all-MLP Architecture for Vision</a></h2>\n": "<h2><a href=\"mlp_mixer/index.html\">MLP \u6df7\u97f3\u5668\uff1a\u9762\u5411\u89c6\u89c9\u7684\u5168 MLP \u67b6\u6784</a></h2>\n",
 "<h2><a href=\"primer_ez/index.html\">Primer EZ</a></h2>\n": "<h2><a href=\"primer_ez/index.html\">Primer</a></h2>\n",
 "<h2><a href=\"retro/index.html\">RETRO</a></h2>\n": "<h2><a href=\"retro/index.html\">\u590d\u53e4</a></h2>\n",
 "<h2><a href=\"rope/index.html\">Rotary Positional Embeddings</a></h2>\n": "<h2><a href=\"rope/index.html\">\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165</a></h2>\n",
 "<h2><a href=\"switch/index.html\">Switch Transformer</a></h2>\n": "<h2><a href=\"switch/index.html\">\u5f00\u5173\u53d8\u538b\u5668</a></h2>\n",
 "<h2><a href=\"vit/index.html\">Vision Transformer (ViT)</a></h2>\n": "<h2><a href=\"vit/index.html\">\u89c6\u89c9\u53d8\u538b\u5668 (ViT)</a></h2>\n",
 "<h2><a href=\"xl/index.html\">Transformer XL</a></h2>\n": "<h2><a href=\"xl/index.html\">\u53d8\u538b\u5668 XL</a></h2>\n",
 "<p>This implements Attention with Linear Biases (ALiBi).</p>\n": "<p>\u8fd9\u5b9e\u73b0\u4e86\u7ebf\u6027\u504f\u5dee\u6ce8\u610f\u529b\uff08AliBI\uff09\u3002</p>\n",
 "<p>This implements Rotary Positional Embeddings (RoPE)</p>\n": "<p>\u8fd9\u5b9e\u73b0\u4e86\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165 (roPE)</p>\n",
 "<p>This implements Transformer XL model using <a href=\"xl/relative_mha.html\">relative multi-head attention</a></p>\n": "<p>\u8fd9\u4f7f\u7528<a href=\"xl/relative_mha.html\">\u76f8\u5bf9\u7684\u591a\u5934\u6ce8\u610f\u529b</a>\u5b9e\u73b0\u4e86\u53d8\u5f62\u91d1\u521a XL \u6a21\u578b</p>\n",
 "<p>This implements the Retrieval-Enhanced Transformer (RETRO).</p>\n": "<p>\u8fd9\u5b9e\u73b0\u4e86\u68c0\u7d22\u589e\u5f3a\u578b\u8f6c\u6362\u5668\uff08RETRO\uff09\u3002</p>\n",
 "<p>This is a miniature implementation of the paper <a href=\"https://papers.labml.ai/paper/2101.03961\">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a>. Our implementation only has a few million parameters and doesn&#x27;t do model parallel distributed training. It does single GPU training but we implement the concept of switching as described in the paper.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587\u300a<a href=\"https://papers.labml.ai/paper/2101.03961\">\u5f00\u5173\u53d8\u538b\u5668\uff1a\u4ee5\u7b80\u5355\u9ad8\u6548\u7684\u7a00\u758f\u5ea6\u7f29\u653e\u5230\u4e07\u4ebf\u53c2\u6570\u6a21\u578b</a>\u300b\u7684\u5fae\u578b\u5b9e\u73b0\u3002\u6211\u4eec\u7684\u5b9e\u73b0\u53ea\u6709\u51e0\u767e\u4e07\u4e2a\u53c2\u6570\uff0c\u4e0d\u5bf9\u5e76\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u8fdb\u884c\u5efa\u6a21\u3002\u5b83\u8fdb\u884c\u5355\u4e2a GPU \u8bad\u7ec3\uff0c\u4f46\u6211\u4eec\u5b9e\u73b0\u4e86\u767d\u76ae\u4e66\u4e2d\u63cf\u8ff0\u7684\u5207\u6362\u6982\u5ff5\u3002</p>\n",
 "<p>This is an implementation of GPT-2 architecture.</p>\n": "<p>\u8fd9\u662f GPT-2 \u4f53\u7cfb\u7ed3\u6784\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of Masked Language Model used for pre-training in paper <a href=\"https://papers.labml.ai/paper/1810.04805\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>.</p>\n": "<p>\u8fd9\u662f\u5728\u8bba\u6587\u300aB <a href=\"https://papers.labml.ai/paper/1810.04805\">ERT\uff1a\u7528\u4e8e\u8bed\u8a00\u7406\u89e3\u7684\u6df1\u5ea6\u53cc\u5411\u53d8\u6362\u5668\u7684\u9884\u8bad\u7ec3\u300b\u4e2d\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u8499\u9762\u8bed\u8a00\u6a21\u578b\u7684</a>\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of compressive transformer that extends upon <a href=\"xl/index.html\">Transformer XL</a> by compressing the oldest memories to give a longer attention span.</p>\n": "<p>\u8fd9\u662f\u4e00\u79cd\u538b\u7f29\u53d8\u538b\u5668\u7684\u5b9e\u73b0\uff0c\u5b83\u901a\u8fc7\u538b\u7f29\u6700\u53e4\u8001\u7684\u5b58\u50a8<a href=\"xl/index.html\">\u5668\u6765\u5ef6\u957f\u6ce8\u610f\u529b\u8de8\u5ea6\uff0c\u4ece\u800c\u5728Transformer XL</a> \u4e0a\u6269\u5c55\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://papers.labml.ai/paper/1911.00172\">Generalization through Memorization: Nearest Neighbor Language Models</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587\u300a<a href=\"https://papers.labml.ai/paper/1911.00172\">\u901a\u8fc7\u8bb0\u5fc6\u63a8\u5e7f\uff1a\u6700\u8fd1\u90bb\u8bed\u8a00\u6a21\u578b</a>\u300b\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://papers.labml.ai/paper/2002.05202\">GLU Variants Improve Transformer</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587 <a href=\"https://papers.labml.ai/paper/2002.05202\">GLU \u53d8\u4f53\u6539\u8fdb\u53d8\u538b\u5668\u7684</a>\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://papers.labml.ai/paper/2002.09402\">Accessing Higher-level Representations in Sequential Transformers with Feedback Memory</a>.</p>\n": "<p>\u8fd9\u662f\u4e00\u7bc7\u8bba\u6587\u300a\u4f7f\u7528<a href=\"https://papers.labml.ai/paper/2002.09402\">\u53cd\u9988\u5b58\u50a8\u5668\u8bbf\u95ee\u987a\u5e8f\u53d8\u538b\u5668\u4e2d\u7684\u66f4\u9ad8\u5c42\u6b21\u8868\u793a\u300b\u7684</a>\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://papers.labml.ai/paper/2010.11929\">An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587\u300a<a href=\"https://papers.labml.ai/paper/2010.11929\">\u56fe\u50cf\u503c\u5f97 16x16 Words\uff1a\u5927\u89c4\u6a21\u56fe\u50cf\u8bc6\u522b\u7684\u53d8\u5f62\u91d1\u521a\u300b\u7684</a>\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://papers.labml.ai/paper/2102.11174\">Linear Transformers Are Secretly Fast Weight Memory Systems in PyTorch</a>.</p>\n": "<p>\u8fd9\u662f <a href=\"https://papers.labml.ai/paper/2102.11174\">PyTorch \u4e2d\u7ebf\u6027\u53d8\u538b\u5668\u662f\u79d8\u5bc6\u7684\u5feb\u901f\u91cd\u91cf\u5b58\u50a8\u7cfb\u7edf\u8bba\u6587\u7684</a>\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://papers.labml.ai/paper/2105.01601\">MLP-Mixer: An all-MLP Architecture for Vision</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587 <a href=\"https://papers.labml.ai/paper/2105.01601\">MLP-Mixer\uff1a\u89c6\u89c9\u7684\u5168 MLP \u67b6\u6784\u7684</a>\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://papers.labml.ai/paper/2105.03824\">FNet: Mixing Tokens with Fourier Transforms</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587\u300a<a href=\"https://papers.labml.ai/paper/2105.03824\">FNet\uff1a\u5c06\u4ee4\u724c\u4e0e\u5085\u91cc\u53f6\u53d8\u6362\u6df7\u5408</a>\u300b\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://papers.labml.ai/paper/2105.08050\">Pay Attention to MLPs</a>.</p>\n": "<p>\u8fd9\u662f \u201c<a href=\"https://papers.labml.ai/paper/2105.08050\">\u6ce8\u610f MLP\u201d \u4e00\u6587\u7684</a>\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://papers.labml.ai/paper/2105.14103\">An Attention Free Transformer</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587\u300a<a href=\"https://papers.labml.ai/paper/2105.14103\">\u65e0\u6ce8\u610f\u529b\u53d8\u538b\u5668\u300b\u7684</a>\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://papers.labml.ai/paper/2109.08668\">Primer: Searching for Efficient Transformers for Language Modeling</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587\u300a\u5165<a href=\"https://papers.labml.ai/paper/2109.08668\">\u95e8\uff1a\u4e3a\u8bed\u8a00\u5efa\u6a21\u5bfb\u627e\u9ad8\u6548\u7684\u53d8\u6362\u5668\u300b\u7684</a>\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://papers.labml.ai/paper/2110.13711\">Hierarchical Transformers Are More Efficient Language Models</a></p>\n": "<p>\u8fd9\u662f\u8bba\u6587\u300a<a href=\"https://papers.labml.ai/paper/2110.13711\">\u5206\u5c42\u53d8\u6362\u5668\u662f\u66f4\u6709\u6548\u7684\u8bed\u8a00\u6a21\u578b</a>\u300b\u7684\u5b9e\u73b0</p>\n",
 "<p>This module contains <a href=\"https://pytorch.org/\">PyTorch</a> implementations and explanations of original transformer from paper <a href=\"https://papers.labml.ai/paper/1706.03762\">Attention Is All You Need</a>, and derivatives and enhancements of it.</p>\n": "</a><p>\u672c\u6a21\u5757\u5305\u542b <a href=\"https://pytorch.org/\">PyTorch \u5b9e\u73b0\u548c\u8bba\u6587 Attention Is <a href=\"https://papers.labml.ai/paper/1706.03762\">All You Need</a> \u4e2d\u5bf9\u539f\u521b\u53d8\u538b\u5668\u7684\u89e3\u91ca\uff0c\u4ee5\u53ca\u5b83\u7684\u884d\u751f\u54c1\u548c\u589e\u5f3a\u529f\u80fd\u3002</p>\n",
 "<ul><li><a href=\"mha.html\">Multi-head attention</a> </li>\n<li><a href=\"models.html\">Transformer Encoder and Decoder Models</a> </li>\n<li><a href=\"feed_forward.html\">Position-wise Feed Forward Network (FFN)</a> </li>\n<li><a href=\"positional_encoding.html\">Fixed positional encoding</a></li></ul>\n": "<ul><li><a href=\"mha.html\">\u591a\u5934\u5173\u6ce8</a></li>\n<li><a href=\"models.html\">\u53d8\u538b\u5668\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u578b\u53f7</a></li>\n<li><a href=\"feed_forward.html\">\u4f4d\u7f6e\u524d\u9988\u7f51\u7edc (FFN)</a></li>\n<li><a href=\"positional_encoding.html\">\u56fa\u5b9a\u4f4d\u7f6e\u7f16\u7801</a></li></ul>\n",
 "This is a collection of PyTorch implementations/tutorials of transformers and related techniques.": "\u8fd9\u662f\u53d8\u538b\u5668\u548c\u76f8\u5173\u6280\u672f\u7684 PyTorch \u5b9e\u73b0/\u6559\u7a0b\u7684\u96c6\u5408\u3002",
 "Transformers": "\u53d8\u538b\u5668"
}