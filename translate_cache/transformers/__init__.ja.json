{
 "<h1>Transformers</h1>\n": "<h1>\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc</h1>\n",
 "<h2><a href=\"aft/index.html\">Attention Free Transformer</a></h2>\n": "<h2><a href=\"aft/index.html\">\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d5\u30ea\u30fc\u5909\u5727\u5668</a></h2>\n",
 "<h2><a href=\"alibi/index.html\">Attention with Linear Biases</a></h2>\n": "<h2><a href=\"alibi/index.html\">\u7dda\u5f62\u30d0\u30a4\u30a2\u30b9\u306b\u3088\u308b\u6ce8\u610f</a></h2>\n",
 "<h2><a href=\"compressive/index.html\">Compressive Transformer</a></h2>\n": "<h2><a href=\"compressive/index.html\">\u5727\u7e2e\u5909\u5727\u5668</a></h2>\n",
 "<h2><a href=\"fast_weights/index.html\">Fast Weights Transformer</a></h2>\n": "<h2><a href=\"fast_weights/index.html\">\u9ad8\u901f\u30a6\u30a7\u30a4\u30c8\u30c8\u30e9\u30f3\u30b9</a></h2>\n",
 "<h2><a href=\"feedback/index.html\">Feedback Transformer</a></h2>\n": "<h2><a href=\"feedback/index.html\">\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u5909\u5727\u5668</a></h2>\n",
 "<h2><a href=\"fnet/index.html\">FNet: Mixing Tokens with Fourier Transforms</a></h2>\n": "<h2><a href=\"fnet/index.html\">FNet: \u30d5\u30fc\u30ea\u30a8\u5909\u63db\u306b\u3088\u308b\u30c8\u30fc\u30af\u30f3\u306e\u6df7\u5408</a></h2>\n",
 "<h2><a href=\"glu_variants/simple.html\">GLU Variants</a></h2>\n": "<h2><a href=\"glu_variants/simple.html\">GLU \u30d0\u30ea\u30a2\u30f3\u30c8</a></h2>\n",
 "<h2><a href=\"gmlp/index.html\">Pay Attention to MLPs (gMLP)</a></h2>\n": "<h2><a href=\"gmlp/index.html\">MLP (GMLP) \u306b\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044</a></h2>\n",
 "<h2><a href=\"gpt/index.html\">GPT Architecture</a></h2>\n": "<h2><a href=\"gpt/index.html\">GPT \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3</a></h2>\n",
 "<h2><a href=\"hour_glass/index.html\">Hourglass</a></h2>\n": "<h2><a href=\"hour_glass/index.html\">\u7802\u6642\u8a08</a></h2>\n",
 "<h2><a href=\"knn/index.html\">kNN-LM</a></h2>\n": "<h2><a href=\"knn/index.html\">KNN-LM</a></h2>\n",
 "<h2><a href=\"mlm/index.html\">Masked Language Model</a></h2>\n": "<h2><a href=\"mlm/index.html\">\u30de\u30b9\u30af\u8a00\u8a9e\u30e2\u30c7\u30eb</a></h2>\n",
 "<h2><a href=\"mlp_mixer/index.html\">MLP-Mixer: An all-MLP Architecture for Vision</a></h2>\n": "<h2><a href=\"mlp_mixer/index.html\">MLP\u30df\u30ad\u30b5\u30fc:\u30d3\u30b8\u30e7\u30f3\u7528\u306e\u30aa\u30fc\u30ebMLP\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3</a></h2>\n",
 "<h2><a href=\"primer_ez/index.html\">Primer EZ</a></h2>\n": "<h2><a href=\"primer_ez/index.html\">\u30d7\u30e9\u30a4\u30de\u30fc EZ</a></h2>\n",
 "<h2><a href=\"retro/index.html\">RETRO</a></h2>\n": "<h2><a href=\"retro/index.html\">\u30ec\u30c8\u30ed</a></h2>\n",
 "<h2><a href=\"rope/index.html\">Rotary Positional Embeddings</a></h2>\n": "<h2><a href=\"rope/index.html\">\u30ed\u30fc\u30bf\u30ea\u30fc\u30dd\u30b8\u30b7\u30e7\u30ca\u30eb\u30a8\u30f3\u30d9\u30c7\u30a3\u30f3\u30b0</a></h2>\n",
 "<h2><a href=\"switch/index.html\">Switch Transformer</a></h2>\n": "<h2><a href=\"switch/index.html\">\u30b9\u30a4\u30c3\u30c1\u30c8\u30e9\u30f3\u30b9</a></h2>\n",
 "<h2><a href=\"vit/index.html\">Vision Transformer (ViT)</a></h2>\n": "<h2><a href=\"vit/index.html\">\u30d3\u30b8\u30e7\u30f3\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc (ViT)</a></h2>\n",
 "<h2><a href=\"xl/index.html\">Transformer XL</a></h2>\n": "<h2><a href=\"xl/index.html\">\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc XL</a></h2>\n",
 "<p>This implements Attention with Linear Biases (ALiBi).</p>\n": "<p>\u3053\u308c\u306f\u3001\u7dda\u5f62\u30d0\u30a4\u30a2\u30b9\uff08AliBi\uff09\u306b\u3088\u308b\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059\u3002</p>\n",
 "<p>This implements Rotary Positional Embeddings (RoPE)</p>\n": "<p>\u3053\u308c\u306f\u30ed\u30fc\u30bf\u30ea\u30fc\u30fb\u30dd\u30b8\u30b7\u30e7\u30ca\u30eb\u30fb\u30a8\u30f3\u30d9\u30c7\u30a3\u30f3\u30b0 (RoPE) \u3092\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002</p>\n",
 "<p>This implements Transformer XL model using <a href=\"xl/relative_mha.html\">relative multi-head attention</a></p>\n": "<p>\u3053\u308c\u306f\u3001<a href=\"xl/relative_mha.html\">\u76f8\u5bfe\u7684\u306a\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092\u4f7f\u7528\u3057\u305f\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fcXL\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059</a>\u3002</p>\n",
 "<p>This implements the Retrieval-Enhanced Transformer (RETRO).</p>\n": "<p>\u3053\u308c\u306f\u691c\u7d22\u5f37\u5316\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc (RETRO) \u3092\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002</p>\n",
 "<p>This is a miniature implementation of the paper <a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a>. Our implementation only has a few million parameters and doesn&#x27;t do model parallel distributed training. It does single GPU training but we implement the concept of switching as described in the paper.</p>\n": "<p>\u3053\u308c\u306f\u3001\u8ad6\u6587\u306e\u300c<a href=\"https://arxiv.org/abs/2101.03961\">\u30b9\u30a4\u30c3\u30c1\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\uff1a\u30b7\u30f3\u30d7\u30eb\u3067\u52b9\u7387\u7684\u306a\u30b9\u30d1\u30fc\u30b9\u6027\u3092\u5099\u3048\u305f1\u5146\u30d1\u30e9\u30e1\u30fc\u30bf\u30e2\u30c7\u30eb\u3078\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0</a>\u300d\u306e\u30df\u30cb\u30c1\u30e5\u30a2\u5b9f\u88c5\u3067\u3059\u3002\u79c1\u305f\u3061\u306e\u5b9f\u88c5\u306b\u306f\u6570\u767e\u4e07\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3057\u304b\u306a\u304f\u3001\u30e2\u30c7\u30eb\u306e\u4e26\u5217\u5206\u6563\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306f\u884c\u3044\u307e\u305b\u3093\u3002\u30b7\u30f3\u30b0\u30ebGPU\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u884c\u3044\u307e\u3059\u304c\u3001\u8ad6\u6587\u3067\u8aac\u660e\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u306b\u30b9\u30a4\u30c3\u30c1\u30f3\u30b0\u306e\u6982\u5ff5\u3092\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059</p>\u3002\n",
 "<p>This is an implementation of GPT-2 architecture.</p>\n": "<p>\u3053\u308c\u306f GPT-2 \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u5b9f\u88c5\u3067\u3059\u3002</p>\n",
 "<p>This is an implementation of Masked Language Model used for pre-training in paper <a href=\"https://arxiv.org/abs/1810.04805\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>.</p>\n": "<p>\u3053\u308c\u306f\u3001\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/1810.04805\">BERT\uff1a\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306e\u30c7\u30a3\u30fc\u30d7\u53cc\u65b9\u5411\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u4e8b\u524d\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u300d\u306e\u4e8b\u524d\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306b\u4f7f\u7528\u3055\u308c\u305f\u30de\u30b9\u30af\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</p>\n",
 "<p>This is an implementation of compressive transformer that extends upon <a href=\"xl/index.html\">Transformer XL</a> by compressing the oldest memories to give a longer attention span.</p>\n": "<p>\u3053\u308c\u306f\u5727\u7e2e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u5b9f\u88c5\u3067\u3001<a href=\"xl/index.html\">Transformer XL\u3092\u62e1\u5f35\u3057\u305f\u3082\u306e\u3067</a>\u3001\u6700\u3082\u53e4\u3044\u30e1\u30e2\u30ea\u3092\u5727\u7e2e\u3057\u3066\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30b9\u30d1\u30f3\u3092\u9577\u304f\u3057\u307e\u3059\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/1911.00172\">Generalization through Memorization: Nearest Neighbor Language Models</a>.</p>\n": "<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://arxiv.org/abs/1911.00172\">\u8a18\u61b6\u306b\u3088\u308b\u4e00\u822c\u5316\uff1a\u6700\u8fd1\u508d\u8a00\u8a9e\u30e2\u30c7\u30eb</a>\u300d\u3068\u3044\u3046\u8ad6\u6587\u306e\u5b9f\u88c5\u3067\u3059\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2002.05202\">GLU Variants Improve Transformer</a>.</p>\n": "<p>\u3053\u308c\u306f\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/2002.05202\">GLU\u30d0\u30ea\u30a2\u30f3\u30c8\u6539\u826f\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc</a>\u300d\u306e\u5b9f\u88c5\u3067\u3059\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2002.09402\">Accessing Higher-level Representations in Sequential Transformers with Feedback Memory</a>.</p>\n": "<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://arxiv.org/abs/2002.09402\">\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u30e1\u30e2\u30ea\u3092\u7528\u3044\u305f\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30fb\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u9ad8\u4f4d\u8868\u73fe\u3078\u306e\u30a2\u30af\u30bb\u30b9\u300d\u3068\u3044\u3046\u8ad6\u6587\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2010.11929\">An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale</a>.</p>\n": "<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://arxiv.org/abs/2010.11929\">\u753b\u50cf\u306f16x16\u306e\u8a00\u8449\u306b\u5024\u3059\u308b\u300d\u3068\u3044\u3046\u8ad6\u6587\u300c\u5927\u898f\u6a21\u753b\u50cf\u8a8d\u8b58\u306e\u305f\u3081\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u300d\u306e\u5b9f\u88c5\u3067\u3059\u3002</a></p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2102.11174\">Linear Transformers Are Secretly Fast Weight Memory Systems in PyTorch</a>.</p>\n": "<p>\u3053\u308c\u306f\u3001PyTorch\u306e\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/2102.11174\">\u30ea\u30cb\u30a2\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306f\u5bc6\u304b\u306b\u9ad8\u901f\u30a6\u30a7\u30a4\u30c8\u30e1\u30e2\u30ea\u30b7\u30b9\u30c6\u30e0\u300d\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2105.01601\">MLP-Mixer: An all-MLP Architecture for Vision</a>.</p>\n": "<p>\u3053\u308c\u306f\u3001\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/2105.01601\">MLP\u30df\u30ad\u30b5\u30fc\uff1a\u30d3\u30b8\u30e7\u30f3\u7528\u306e\u30aa\u30fc\u30ebMLP\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3</a>\u300d\u306e\u5b9f\u88c5\u3067\u3059\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2105.03824\">FNet: Mixing Tokens with Fourier Transforms</a>.</p>\n": "<p>\u3053\u308c\u306f\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/2105.03824\">FNet: \u30c8\u30fc\u30af\u30f3\u3092\u30d5\u30fc\u30ea\u30a8\u5909\u63db\u3068\u6df7\u5408\u3059\u308b</a>\u300d\u306e\u5b9f\u88c5\u3067\u3059\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2105.08050\">Pay Attention to MLPs</a>.</p>\n": "<p>\u3053\u308c\u306f\u3001\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/2105.08050\">MLP\u306b\u6ce8\u610f\u3092\u6255\u3046</a>\u300d\u306e\u5b9f\u88c5\u3067\u3059\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2105.14103\">An Attention Free Transformer</a>.</p>\n": "<p>\u3053\u308c\u306f\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/2105.14103\">\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30fb\u30d5\u30ea\u30fc\u30fb\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc</a>\u300d\u306e\u5b9f\u88c5\u3067\u3059\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2109.08668\">Primer: Searching for Efficient Transformers for Language Modeling</a>.</p>\n": "<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://arxiv.org/abs/2109.08668\">\u5165\u9580\u66f8\uff1a\u8a00\u8a9e\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u305f\u3081\u306e\u52b9\u7387\u7684\u306a\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u63a2\u6c42\u300d\u3068\u3044\u3046\u8ad6\u6587\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2110.13711\">Hierarchical Transformers Are More Efficient Language Models</a></p>\n": "<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://arxiv.org/abs/2110.13711\">\u968e\u5c64\u578b\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306f\u3088\u308a\u52b9\u7387\u7684\u306a\u8a00\u8a9e\u30e2\u30c7\u30eb</a>\u300d\u3068\u3044\u3046\u8ad6\u6587\u306e\u5b9f\u88c5\u3067\u3059\u3002</p>\n",
 "<p>This module contains <a href=\"https://pytorch.org/\">PyTorch</a> implementations and explanations of original transformer from paper <a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>, and derivatives and enhancements of it.</p>\n": "</a><p>\u3053\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u306b\u306f\u3001<a href=\"https://pytorch.org/\">PyTorch\u306e\u5b9f\u88c5\u3068\u3001\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/1706.03762\">Attention IsAll You Need</a>\u300d\u306b\u63b2\u8f09\u3055\u308c\u305f\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u8aac\u660e\u3001\u304a\u3088\u3073\u305d\u306e\u6d3e\u751f\u54c1\u3068\u62e1\u5f35\u6a5f\u80fd\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002</p>\n",
 "<ul><li><a href=\"mha.html\">Multi-head attention</a> </li>\n<li><a href=\"models.html\">Transformer Encoder and Decoder Models</a> </li>\n<li><a href=\"feed_forward.html\">Position-wise Feed Forward Network (FFN)</a> </li>\n<li><a href=\"positional_encoding.html\">Fixed positional encoding</a></li></ul>\n": "<ul><li><a href=\"mha.html\">\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3</a></li>\n<li><a href=\"models.html\">\u30c8\u30e9\u30f3\u30b9\u30a8\u30f3\u30b3\u30fc\u30c0\u304a\u3088\u3073\u30c7\u30b3\u30fc\u30c0\u30e2\u30c7\u30eb</a></li>\n<li><a href=\"feed_forward.html\">\u4f4d\u7f6e\u5225\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af (FFN)</a></li>\n<li><a href=\"positional_encoding.html\">\u56fa\u5b9a\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0</a></li></ul>\n",
 "This is a collection of PyTorch implementations/tutorials of transformers and related techniques.": "\u3053\u308c\u306f\u3001\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3068\u95a2\u9023\u6280\u8853\u306e PyTorch \u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u30b3\u30ec\u30af\u30b7\u30e7\u30f3\u3067\u3059\u3002",
 "Transformers": "\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc"
}