{
 "<h1><a href=\"https://nn.labml.ai/transformers/primer_ez/index.html\">Primer: Searching for Efficient Transformers for Language Modeling</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2109.08668\">Primer: Searching for Efficient Transformers for Language Modeling</a>.</p>\n<p>The authors do an evolutionary search for transformer architectures. They name the architecture found using the search as Primer (PRIMitives searched transformER). <strong>Primer EZ</strong> is the architecture with the two most robust modifications in Primer compared to  the original transformer. Primer EZ trains a lot faster than the vanilla transformer.</p>\n<h3>Squared ReLU</h3>\n<p>The most effective modification found by the search is using a square ReLU instead of ReLU in the <a href=\"https://nn.labml.ai/transformers/feed_forward.html\">position-wise feedforward module</a>.</p>\n<h3>Multi-DConv-Head Attention (MDHA)</h3>\n<p>The next effective modification is a depth-wise 3 X 1 convolution after multi-head projection  for queries, keys, and values. The convolution is along the sequence dimension and per channel (depth-wise). To be clear, if the number of channels in each head is d_k the convolution will have 1 X 3 kernels for each of the d_k channels.</p>\n<p><a href=\"https://nn.labml.ai/transformers/primer_ez/experiment.html\">Here is the experiment code</a>, for Primer EZ.</p>\n<p><a href=\"https://app.labml.ai/run/30adb7aa1ab211eca7310f80a114e8a4\"><span translate=no>_^_0_^_</span></a> </p>\n": "<h1><a href=\"https://nn.labml.ai/transformers/primer_ez/index.html\">\u0db4\u0dca\u0dbb\u0dcf\u0dae\u0db8\u0dd2\u0d9a\u0dba: \u0db7\u0dcf\u0dc2\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2 \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0d9a\u0dca\u0dc2\u0db8 \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dd9\u0dc0\u0dd3\u0db8</a></h1>\n<p>\u0db8\u0dd9\u0dba <a href=\"https://pytorch.org\">PyTorch</a> \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 <a href=\"https://arxiv.org/abs/2109.08668\">\u0db4\u0dca\u0dbb\u0dba\u0dd2\u0db8\u0dbb\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8: \u0db7\u0dcf\u0dc2\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2 \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0d9a\u0dca\u0dc2\u0db8 \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dd9\u0dc0\u0dd3\u0db8</a> . </p>\n<p>\u0d9a\u0dad\u0dd4\u0dc0\u0dbb\u0dd4\u0db1\u0dca\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d9c\u0dd8\u0dc4 \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab \u0dc1\u0dd2\u0dbd\u0dca\u0db4\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dbb\u0dd2\u0dab\u0dcf\u0db8\u0dd3\u0dba \u0dc3\u0dd9\u0dc0\u0dd3\u0db8\u0d9a\u0dca \u0d9a\u0dbb\u0dba\u0dd2. \u0dc3\u0dd9\u0dc0\u0dd4\u0db8 \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dba\u0dd9\u0db1\u0dca \u0dc3\u0ddc\u0dba\u0dcf\u0d9c\u0dad\u0dca \u0d9c\u0dd8\u0dc4 \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab \u0dc1\u0dd2\u0dbd\u0dca\u0db4\u0dba \u0d94\u0dc0\u0dd4\u0db1\u0dca \u0db1\u0db8\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0db4\u0dca\u0dbb\u0dba\u0dd2\u0db8\u0dbb\u0dca (\u0db4\u0dca\u0dbb\u0dba\u0dd2\u0db8\u0dca\u0da7\u0dd2\u0dc0\u0dca\u0dc3\u0dca \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dd9\u0dc0\u0dd6) \u0dbd\u0dd9\u0dc3\u0dba\u0dd2. <strong>\u0db4\u0dca\u0dbb\u0dba\u0dd2\u0db8\u0dbb\u0dcaEZ</strong> \u0dba\u0db1\u0dd4 \u0db8\u0dd4\u0dbd\u0dca \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dba \u0dc4\u0dcf \u0dc3\u0dc3\u0db3\u0db1 \u0dc0\u0dd2\u0da7 \u0db4\u0dca\u0dbb\u0dba\u0dd2\u0db8\u0dbb\u0dca \u0dc4\u0dd2 \u0dc0\u0da9\u0dcf\u0dad\u0dca\u0db8 \u0dc1\u0d9a\u0dca\u0dad\u0dd2\u0db8\u0dad\u0dca \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dca \u0daf\u0dd9\u0d9a \u0dc3\u0dc4\u0dd2\u0dad \u0d9c\u0dd8\u0dc4 \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab \u0dc1\u0dd2\u0dbd\u0dca\u0db4\u0dba\u0dba\u0dd2. \u0db4\u0dca\u0dbb\u0dba\u0dd2\u0db8\u0dbb\u0dca EZ \u0dc0\u0dd0\u0db1\u0dd2\u0dbd\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dba \u0dc0\u0da9\u0dcf \u0dc0\u0dda\u0d9c\u0dba\u0dd9\u0db1\u0dca \u0daf\u0dd4\u0db8\u0dca\u0dbb\u0dd2\u0dba. </p>\n<h3>\u0dc0\u0dbb\u0dca\u0d9cRelu</h3>\n<p>\u0dc3\u0dd9\u0dc0\u0dd4\u0db8\u0db8\u0d9c\u0dd2\u0db1\u0dca \u0dc3\u0ddc\u0dba\u0dcf\u0d9c\u0dad\u0dca \u0dc0\u0da9\u0dcf\u0dad\u0dca\u0db8 effective \u0dbd\u0daf\u0dcf\u0dba\u0dd3 \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc0\u0db1\u0dca\u0db1\u0dda <a href=\"https://nn.labml.ai/transformers/feed_forward.html\">\u0dc3\u0dca\u0dae\u0dcf\u0db1-\u0db1\u0dd0\u0dab\u0dc0\u0dad\u0dca \u0db4\u0ddd\u0dc2\u0d9a \u0db8\u0ddc\u0da9\u0dd2\u0dba\u0dd4\u0dbd\u0dba\u0dda</a>RelU \u0dc0\u0dd9\u0db1\u0dd4\u0dc0\u0da7 \u0da0\u0dad\u0dd4\u0dbb\u0dc3\u0dca\u0dbb\u0dcf\u0d9a\u0dcf\u0dbb RelU \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dba\u0dd2. </p>\n<h3>\u0db6\u0dc4\u0dd4-Dconv-\u0dc4\u0dd2\u0dc3\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba (MDHA)</h3>\n<p>\u0d8a\u0dc5\u0d9f\u0db5\u0dbd\u0daf\u0dcf\u0dba\u0dd3 \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca, \u0dba\u0dad\u0dd4\u0dbb\u0dd4, \u0dc3\u0dc4 \u0dc0\u0da7\u0dd2\u0db1\u0dcf\u0d9a\u0db8\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0db6\u0dc4\u0dd4-\u0dc4\u0dd2\u0dc3 \u0db4\u0dca\u0dbb\u0d9a\u0dca\u0dc2\u0dda\u0db4\u0db1\u0dba \u0db4\u0dc3\u0dd4 \u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4-wise 3 X 1 convolution \u0dc0\u0dda. \u0db8\u0dd9\u0db8 convolution \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dba \u0db8\u0dcf\u0db1\u0dba\u0d9a\u0dca \u0d94\u0dc3\u0dca\u0dc3\u0dda \u0dc3\u0dc4 \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf\u0dc0 (\u0d9c\u0dd0\u0db9\u0dd4\u0dbb-\u0d85\u0db1\u0dd4\u0dc0) \u0dc0\u0dda. \u0db4\u0dd0\u0dc4\u0dd0\u0daf\u0dd2\u0dbd\u0dd2 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf, \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0dc4\u0dd2\u0dc3\u0dd9\u0dc4\u0dd2 \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0d9c\u0dab\u0db1 d_k \u0db1\u0db8\u0dca, \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca d_k \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0dc3\u0db3\u0dc4\u0dcf 1 X 3 \u0d9a\u0dbb\u0dca\u0db1\u0dbd\u0dca \u0d87\u0dad. </p>\n<p>\u0db4\u0dca\u0dbb\u0dba\u0dd2\u0db8\u0dbb\u0dcaEZ \u0dc3\u0db3\u0dc4\u0dcf<a href=\"https://nn.labml.ai/transformers/primer_ez/experiment.html\">\u0d85\u0dad\u0dca\u0dc4\u0daf\u0dcf \u0db6\u0dd0\u0dbd\u0dd3\u0db8\u0dda \u0d9a\u0dda\u0dad\u0dba \u0db8\u0dd9\u0db1\u0dca\u0db1</a>. </p>\n<p><a href=\"https://app.labml.ai/run/30adb7aa1ab211eca7310f80a114e8a4\"><span translate=no>_^_0_^_</span></a> </p>\n",
 "Primer: Searching for Efficient Transformers for Language Modeling": "\u0db4\u0dca\u0dbb\u0dcf\u0dae\u0db8\u0dd2\u0d9a\u0dba: \u0db7\u0dcf\u0dc2\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2 \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0d9a\u0dca\u0dc2\u0db8 \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dd9\u0dc0\u0dd3\u0db8"
}