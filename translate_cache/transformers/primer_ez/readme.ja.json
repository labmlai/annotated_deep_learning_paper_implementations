{
 "<h1><a href=\"https://nn.labml.ai/transformers/primer_ez/index.html\">Primer: Searching for Efficient Transformers for Language Modeling</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2109.08668\">Primer: Searching for Efficient Transformers for Language Modeling</a>.</p>\n<p>The authors do an evolutionary search for transformer architectures. They name the architecture found using the search as Primer (PRIMitives searched transformER). <strong>Primer EZ</strong> is the architecture with the two most robust modifications in Primer compared to  the original transformer. Primer EZ trains a lot faster than the vanilla transformer.</p>\n<h3>Squared ReLU</h3>\n<p>The most effective modification found by the search is using a square ReLU instead of ReLU in the <a href=\"https://nn.labml.ai/transformers/feed_forward.html\">position-wise feedforward module</a>.</p>\n<h3>Multi-DConv-Head Attention (MDHA)</h3>\n<p>The next effective modification is a depth-wise 3 X 1 convolution after multi-head projection  for queries, keys, and values. The convolution is along the sequence dimension and per channel (depth-wise). To be clear, if the number of channels in each head is d_k the convolution will have 1 X 3 kernels for each of the d_k channels.</p>\n<p><a href=\"https://nn.labml.ai/transformers/primer_ez/experiment.html\">Here is the experiment code</a>, for Primer EZ. </p>\n": "<h1><a href=\"https://nn.labml.ai/transformers/primer_ez/index.html\">\u5165\u9580:\u8a00\u8a9e\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u305f\u3081\u306e\u52b9\u7387\u7684\u306a\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u63a2\u6c42</a></h1>\n<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://arxiv.org/abs/2109.08668\">\u5165\u9580\u66f8:\u8a00\u8a9e\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u305f\u3081\u306e\u52b9\u7387\u7684\u306a\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u691c\u7d22</a>\u300d<a href=\"https://pytorch.org\">\u3068\u3044\u3046\u8ad6\u6587\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a>\u3002</p>\n<p>\u8457\u8005\u3089\u306f\u3001\u5909\u5727\u5668\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u9032\u5316\u7684\u7814\u7a76\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\u691c\u7d22\u3067\u898b\u3064\u304b\u3063\u305f\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092 Primer (\u30d7\u30ea\u30df\u30c6\u30a3\u30d6\u304c\u691c\u7d22\u3057\u305f Transformer) \u3068\u3044\u3046\u540d\u524d\u3092\u4ed8\u3051\u307e\u3059\u3002<strong>Primer EZ\u306f</strong>\u3001\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3068\u6bd4\u8f03\u3057\u3066\u3001Primer\u3067\u6700\u3082\u5805\u7262\u306a2\u3064\u306e\u5909\u66f4\u3092\u52a0\u3048\u305f\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3067\u3059\u3002Primer EZ\u306f\u30d0\u30cb\u30e9\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3088\u308a\u3082\u306f\u308b\u304b\u306b\u901f\u304f\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u307e\u3059</p>\u3002\n<h3>\u4e8c\u4e57\u30ea\u30eb</h3>\n<p>\u691c\u7d22\u3067\u898b\u3064\u304b\u3063\u305f\u6700\u3082\u52b9\u679c\u7684\u306a\u5909\u66f4\u306f\u3001<a href=\"https://nn.labml.ai/transformers/feed_forward.html\">\u4f4d\u7f6e\u3054\u3068\u306e\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30e2\u30b8\u30e5\u30fc\u30eb\u3067</a> ReLU \u306e\u4ee3\u308f\u308a\u306b\u6b63\u65b9\u5f62\u306e ReLU \u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3067\u3059\u3002</p>\n<h3>\u30de\u30eb\u30c1\u30b3\u30f3\u30d0\u30fc\u30c1\u30f3\u30b0\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3 (MDHA)</h3>\n<p>\u6b21\u306b\u52b9\u679c\u7684\u306a\u5909\u66f4\u306f\u3001\u30af\u30a8\u30ea\u3001\u30ad\u30fc\u3001\u304a\u3088\u3073\u5024\u306e\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30d7\u30ed\u30b8\u30a7\u30af\u30b7\u30e7\u30f3\u306e\u5f8c\u306b\u3001\u6df1\u3055\u65b9\u5411\u306b 3 X 1 \u306e\u7573\u307f\u8fbc\u307f\u3092\u884c\u3046\u3053\u3068\u3067\u3059\u3002\u7573\u307f\u8fbc\u307f\u306f\u3001\u30b7\u30fc\u30b1\u30f3\u30b9\u6b21\u5143\u306b\u6cbf\u3063\u3066\u3001\u30c1\u30e3\u30cd\u30eb\u5358\u4f4d (\u6df1\u3055\u65b9\u5411) \u3067\u884c\u308f\u308c\u307e\u3059\u3002\u308f\u304b\u308a\u3084\u3059\u304f\u8a00\u3046\u3068\u3001\u5404\u30d8\u30c3\u30c9\u306e\u30c1\u30e3\u30cd\u30eb\u6570\u304c d_k \u306e\u5834\u5408\u3001\u7573\u307f\u8fbc\u307f\u3067\u306f d_k \u30c1\u30e3\u30cd\u30eb\u3054\u3068\u306b 1 X 3 \u30ab\u30fc\u30cd\u30eb\u306b\u306a\u308a\u307e\u3059</p>\u3002\n<p><a href=\"https://nn.labml.ai/transformers/primer_ez/experiment.html\">\u3053\u308c\u304c\u30d7\u30e9\u30a4\u30de\u30fcEZ\u306e\u5b9f\u9a13\u30b3\u30fc\u30c9\u3067\u3059</a>\u3002</p>\n",
 "Primer: Searching for Efficient Transformers for Language Modeling": "\u5165\u9580:\u8a00\u8a9e\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u305f\u3081\u306e\u52b9\u7387\u7684\u306a\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u63a2\u6c42"
}