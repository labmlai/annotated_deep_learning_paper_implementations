{
 "<h1>Primer: Searching for Efficient Transformers for Language Modeling</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2109.08668\">Primer: Searching for Efficient Transformers for Language Modeling</a>.</p>\n<p>The authors do an evolutionary search for transformer architectures. They name the architecture found using the search Primer (PRIMitives searched transformER). <strong>Primer EZ</strong> is the architecture with the two most robust modifications in Primer compared to  the original transformer. Primer EZ trains a lot faster than the vanilla transformer.</p>\n<h3>Squared ReLU</h3>\n<p>The most effective modification found by the search is using a square ReLU instead of ReLU in the <a href=\"../feed_forward.html\">position-wise feedforward module</a>.</p>\n<p><span translate=no>_^_0_^_</span></p>\n<h3>Multi-DConv-Head Attention (MDHA)</h3>\n<p>The next effective modification is a depth-wise <span translate=no>_^_1_^_</span> convolution after multi-head projection  for queries, keys, and values. The convolution is along the sequence dimension and per channel (depth-wise). To be clear, if the number of channels in each head is <span translate=no>_^_2_^_</span> the convolution will have <span translate=no>_^_3_^_</span> kernels for each of the <span translate=no>_^_4_^_</span> channels.</p>\n<p><a href=\"experiment.html\">Here is the experiment code</a>, for Primer EZ.</p>\n": "<h1>\u5165\u9580:\u8a00\u8a9e\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u305f\u3081\u306e\u52b9\u7387\u7684\u306a\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u63a2\u6c42</h1>\n<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://arxiv.org/abs/2109.08668\">\u5165\u9580\u66f8:\u8a00\u8a9e\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u305f\u3081\u306e\u52b9\u7387\u7684\u306a\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u691c\u7d22</a>\u300d<a href=\"https://pytorch.org\">\u3068\u3044\u3046\u8ad6\u6587\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a>\u3002</p>\n<p>\u8457\u8005\u3089\u306f\u3001\u5909\u5727\u5668\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u9032\u5316\u7684\u7814\u7a76\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002Primer (\u30d7\u30ea\u30df\u30c6\u30a3\u30d6\u304c\u691c\u7d22\u3057\u305f Transformer) \u3068\u3044\u3046\u691c\u7d22\u3092\u4f7f\u3063\u3066\u898b\u3064\u304b\u3063\u305f\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306b\u540d\u524d\u3092\u4ed8\u3051\u307e\u3059\u3002<strong>Primer EZ\u306f</strong>\u3001\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3068\u6bd4\u8f03\u3057\u3066\u3001Primer\u3067\u6700\u3082\u5805\u7262\u306a2\u3064\u306e\u5909\u66f4\u3092\u52a0\u3048\u305f\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3067\u3059\u3002Primer EZ\u306f\u30d0\u30cb\u30e9\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3088\u308a\u3082\u306f\u308b\u304b\u306b\u901f\u304f\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u307e\u3059</p>\u3002\n<h3>\u4e8c\u4e57\u30ea\u30eb</h3>\n<p>\u691c\u7d22\u3067\u898b\u3064\u304b\u3063\u305f\u6700\u3082\u52b9\u679c\u7684\u306a\u5909\u66f4\u306f\u3001<a href=\"../feed_forward.html\">\u4f4d\u7f6e\u3054\u3068\u306e\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30e2\u30b8\u30e5\u30fc\u30eb\u3067</a> ReLU \u306e\u4ee3\u308f\u308a\u306b\u6b63\u65b9\u5f62\u306e ReLU \u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3067\u3059\u3002</p>\n<p><span translate=no>_^_0_^_</span></p>\n<h3>\u30de\u30eb\u30c1\u30b3\u30f3\u30d0\u30fc\u30c1\u30f3\u30b0\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3 (MDHA)</h3>\n<p>\u6b21\u306b\u52b9\u679c\u7684\u306a\u5909\u66f4\u306f\u3001\u30af\u30a8\u30ea\u3001\u30ad\u30fc\u3001<span translate=no>_^_1_^_</span>\u304a\u3088\u3073\u5024\u306e\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u6295\u5f71\u5f8c\u306e\u6df1\u5ea6\u65b9\u5411\u306e\u7573\u307f\u8fbc\u307f\u3067\u3059\u3002\u7573\u307f\u8fbc\u307f\u306f\u3001\u30b7\u30fc\u30b1\u30f3\u30b9\u6b21\u5143\u306b\u6cbf\u3063\u3066\u3001\u30c1\u30e3\u30cd\u30eb\u5358\u4f4d (\u6df1\u3055\u65b9\u5411) \u3067\u884c\u308f\u308c\u307e\u3059\u3002\u306f\u3063\u304d\u308a\u3055\u305b\u3066\u304a\u304d\u307e\u3059\u304c\u3001\u5404\u30d8\u30c3\u30c9\u306e\u30c1\u30e3\u30f3\u30cd\u30eb\u6570\u304c\u306e\u5834\u5408<span translate=no>_^_2_^_</span>\u3001<span translate=no>_^_3_^_</span>\u7573\u307f\u8fbc\u307f\u306f\u30c1\u30e3\u30f3\u30cd\u30eb\u3054\u3068\u306b\u30ab\u30fc\u30cd\u30eb\u3092\u6301\u3064\u3053\u3068\u306b\u306a\u308a\u307e\u3059</p>\u3002<span translate=no>_^_4_^_</span>\n<p><a href=\"experiment.html\">\u3053\u308c\u304c\u30d7\u30e9\u30a4\u30de\u30fcEZ\u306e\u5b9f\u9a13\u30b3\u30fc\u30c9\u3067\u3059</a>\u3002</p>\n",
 "<h2>Multi-DConv-Head Attention (MDHA)</h2>\n<p>We extend our original implementation of <a href=\"../mha.html#MHA\">Multi-Head Attention</a> and add the spatial depth-wise convolution to query, key and value projections.</p>\n": "<h2>\u30de\u30eb\u30c1\u30b3\u30f3\u30d0\u30fc\u30c1\u30f3\u30b0\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3 (MDHA)</h2>\n<p><a href=\"../mha.html#MHA\">Multi-Head</a> Attention\u306e\u5f53\u521d\u306e\u5b9f\u88c5\u3092\u62e1\u5f35\u3057\u3001\u30af\u30a8\u30ea\u3001\u30ad\u30fc\u3001\u30d0\u30ea\u30e5\u30fc\u30d7\u30ed\u30b8\u30a7\u30af\u30b7\u30e7\u30f3\u306b\u7a7a\u9593\u6df1\u5ea6\u65b9\u5411\u306e\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002</p>\n",
 "<h2>Spatial Depth Wise Convolution</h2>\n": "<h2>\u7a7a\u9593\u6df1\u5ea6\u5358\u4f4d\u306e\u7573\u307f\u8fbc\u307f</h2>\n",
 "<h2>Squared ReLU activation</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p>Squared ReLU is used as the activation function in the  <a href=\"../feed_forward.html\">position wise feedforward module</a>.</p>\n": "<h2>\u4e8c\u4e57ReLU \u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p>Squared ReLU \u306f\u3001<a href=\"../feed_forward.html\">\u4f4d\u7f6e\u3054\u3068\u306e\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u95a2\u6570\u3068\u3057\u3066\u4f7f\u7528\u3055\u308c\u307e\u3059</a>\u3002</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> <span translate=no>_^_0_^_</span> has shape <span translate=no>_^_1_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span>\u5f62\u304c\u3042\u308b <span translate=no>_^_1_^_</span></p>\n",
 "<p>1D convolution accepts input of the form <span translate=no>_^_0_^_</span> </p>\n": "<p>1\u6b21\u5143\u306e\u7573\u307f\u8fbc\u307f\u306f\u6b21\u306e\u5f62\u5f0f\u306e\u5165\u529b\u3092\u53d7\u3051\u4ed8\u3051\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p><a href=\"../mha.html#MHA\">Multi-Head Attention</a> will create query, key and value projection modules <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span>, and <span translate=no>_^_2_^_</span>.</p>\n<p>We combine a spatial depth-wise convolution layer to each of them and replace <span translate=no>_^_3_^_</span>, <span translate=no>_^_4_^_</span>, and <span translate=no>_^_5_^_</span>.</p>\n<p>\ud83d\udcdd <em>We feel this cleaner implementation is easier to understand since it clearly shows the difference between this and vanilla transformer multi-head attention</em>. </p>\n": "<p><a href=\"../mha.html#MHA\">Multi-Head Attention</a> \u306f\u3001\u30af\u30a8\u30ea\u3001\u30ad\u30fc\u3001\u30d0\u30ea\u30e5\u30fc\u30d7\u30ed\u30b8\u30a7\u30af\u30b7\u30e7\u30f3\u30e2\u30b8\u30e5\u30fc\u30eb<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u3001\u304a\u3088\u3073\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002<span translate=no>_^_2_^_</span></p>\n<p>\u305d\u308c\u305e\u308c\u306b\u7a7a\u9593\u6df1\u5ea6\u65b9\u5411\u306e\u7573\u307f\u8fbc\u307f\u5c64\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u3001\u3001\u3001\u3092\u7f6e\u304d\u63db\u3048\u307e\u3059<span translate=no>_^_3_^_</span>\u3002<span translate=no>_^_4_^_</span> <span translate=no>_^_5_^_</span></p>\n<p>\ud83d\udcdd <em>\u3053\u308c\u3068\u30d0\u30cb\u30e9\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u9055\u3044\u304c\u306f\u3063\u304d\u308a\u3068\u308f\u304b\u308b\u306e\u3067\u3001\u3053\u306e\u3088\u308a\u30af\u30ea\u30fc\u30f3\u306a\u5b9f\u88c5\u306e\u65b9\u304c\u7406\u89e3\u3057\u3084\u3059\u3044\u3068\u601d\u3044\u307e\u3059</em>\u3002</p>\n",
 "<p>Apply ReLU </p>\n": "<p>ReLU \u3092\u9069\u7528</p>\n",
 "<p>Change the shape to <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5f62\u72b6\u3092\u6b21\u306e\u3088\u3046\u306b\u5909\u66f4 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Crop the right most <span translate=no>_^_0_^_</span> results since we padded both sides </p>\n": "<p>\u4e21\u5074\u3092\u30d1\u30c7\u30a3\u30f3\u30b0\u3057\u305f\u306e\u3067\u3001<span translate=no>_^_0_^_</span>\u6700\u3082\u9069\u5207\u306a\u7d50\u679c\u304c\u5f97\u3089\u308c\u308b\u3088\u3046\u306b\u30c8\u30ea\u30df\u30f3\u30b0\u3057\u307e\u3059</p>\n",
 "<p>Get the shape </p>\n": "<p>\u5f62\u3092\u53d6\u5f97</p>\n",
 "<p>Permute to <span translate=no>_^_0_^_</span> </p>\n": "<p>\u306b\u4e26\u3079\u66ff\u3048 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Reshape to <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5f62\u72b6\u3092\u6b21\u306e\u5f62\u5f0f\u306b\u5909\u66f4 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Square it </p>\n": "<p>\u30b9\u30af\u30a8\u30a2\u30fb\u30a4\u30c3\u30c8</p>\n",
 "<p>We use PyTorch&#x27;s <span translate=no>_^_0_^_</span> module. We set the number of groups to be equal to the number of channels so that it does a separate convolution (with different kernels) for each channel. We add padding to both sides and later crop the right most <span translate=no>_^_1_^_</span> results </p>\n": "<p><span translate=no>_^_0_^_</span>PyTorch\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\u30b0\u30eb\u30fc\u30d7\u306e\u6570\u3092\u30c1\u30e3\u30cd\u30eb\u6570\u3068\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u8a2d\u5b9a\u3057\u3001\u30c1\u30e3\u30cd\u30eb\u3054\u3068\u306b (\u7570\u306a\u308b\u30ab\u30fc\u30cd\u30eb\u3067) \u500b\u5225\u306e\u7573\u307f\u8fbc\u307f\u3092\u884c\u3044\u307e\u3059\u3002\u4e21\u5074\u306b\u30d1\u30c7\u30a3\u30f3\u30b0\u3092\u8ffd\u52a0\u3057\u3001<span translate=no>_^_1_^_</span>\u5f8c\u3067\u4e00\u756a\u9069\u5207\u306a\u7d50\u679c\u306b\u306a\u308b\u3088\u3046\u306b\u30c8\u30ea\u30df\u30f3\u30b0\u3057\u307e\u3059</p>\u3002\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of channels in each head</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u5404\u30d8\u30c3\u30c9\u306e\u30c1\u30e3\u30f3\u30cd\u30eb\u6570</li></ul>\n",
 "Primer: Searching for Efficient Transformers for Language Modeling": "\u5165\u9580:\u8a00\u8a9e\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u305f\u3081\u306e\u52b9\u7387\u7684\u306a\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u63a2\u6c42",
 "This is an annotated implementation/tutorial of Primer: Searching for Efficient Transformers for Language Modeling for Vision in PyTorch.": "\u3053\u308c\u306f\u3001\u300c\u5165\u9580\u66f8:PyTorch \u306e Vision \u7528\u8a00\u8a9e\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u305f\u3081\u306e\u52b9\u7387\u7684\u306a\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u691c\u7d22\u300d\u306e\u6ce8\u91c8\u4ed8\u304d\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u3059\u3002"
}