{
 "<h1><a href=\"https://nn.labml.ai/transformers/primer_ez/index.html\">Primer: Searching for Efficient Transformers for Language Modeling</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2109.08668\">Primer: Searching for Efficient Transformers for Language Modeling</a>.</p>\n<p>The authors do an evolutionary search for transformer architectures. They name the architecture found using the search as Primer (PRIMitives searched transformER). <strong>Primer EZ</strong> is the architecture with the two most robust modifications in Primer compared to  the original transformer. Primer EZ trains a lot faster than the vanilla transformer.</p>\n<h3>Squared ReLU</h3>\n<p>The most effective modification found by the search is using a square ReLU instead of ReLU in the <a href=\"https://nn.labml.ai/transformers/feed_forward.html\">position-wise feedforward module</a>.</p>\n<h3>Multi-DConv-Head Attention (MDHA)</h3>\n<p>The next effective modification is a depth-wise 3 X 1 convolution after multi-head projection  for queries, keys, and values. The convolution is along the sequence dimension and per channel (depth-wise). To be clear, if the number of channels in each head is d_k the convolution will have 1 X 3 kernels for each of the d_k channels.</p>\n<p><a href=\"https://nn.labml.ai/transformers/primer_ez/experiment.html\">Here is the experiment code</a>, for Primer EZ. </p>\n": "<h1><a href=\"https://nn.labml.ai/transformers/primer_ez/index.html\">\u5165\u95e8\uff1a\u5bfb\u627e\u7528\u4e8e\u8bed\u8a00\u5efa\u6a21\u7684\u9ad8\u6548\u8f6c\u6362\u5668</a></h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">P <a href=\"https://arxiv.org/abs/2109.08668\">rimer\uff1a\u4e3a\u8bed\u8a00\u5efa\u6a21\u5bfb\u627e\u9ad8\u6548\u8f6c\u6362\u5668</a>\u8bba\u6587\u7684 PyTorch</a> \u5b9e\u73b0\u3002</p>\n<p>\u4f5c\u8005\u5bf9\u53d8\u538b\u5668\u67b6\u6784\u8fdb\u884c\u4e86\u8fdb\u5316\u63a2\u7d22\u3002\u4ed6\u4eec\u5c06\u4f7f\u7528\u641c\u7d22\u627e\u5230\u7684\u67b6\u6784\u547d\u540d\u4e3a Primer\uff08Primitives \u641c\u7d22 Transformer\uff09\u3002\u4e0e\u539f\u59cb\u53d8\u538b\u5668\u76f8\u6bd4\uff0cP@@ <strong>rimer EZ</strong> \u662f\u5728 Primer \u4e2d\u8fdb\u884c\u4e86\u4e24\u9879\u6700\u5f3a\u5927\u7684\u4fee\u6539\u7684\u67b6\u6784\u3002Primer EZ \u7684\u8bad\u7ec3\u901f\u5ea6\u6bd4\u539f\u7248\u53d8\u538b\u5668\u5feb\u5f88\u591a\u3002</p>\n<h3>Squared ReLU</h3>\n<p>\u641c\u7d22\u53d1\u73b0\u7684\u6700\u6709\u6548\u7684\u4fee\u6539\u662f\u5728<a href=\"https://nn.labml.ai/transformers/feed_forward.html\">\u4f4d\u7f6e\u524d\u9988\u6a21\u5757\u4e2d\u4f7f\u7528\u65b9\u5f62 ReLU \u800c\u4e0d\u662f Re</a> LU\u3002</p>\n<h3>Multi-conv-Head \u6ce8\u610f\u529b (MDHA)</h3>\n<p>\u4e0b\u4e00\u4e2a\u6709\u6548\u7684\u4fee\u6539\u662f\u5bf9\u67e5\u8be2\u3001\u952e\u548c\u503c\u8fdb\u884c\u591a\u5934\u6295\u5f71\u540e\u7684\u6df1\u5ea6 3 X 1 \u5377\u79ef\u3002\u5377\u79ef\u6cbf\u7740\u5e8f\u5217\u7ef4\u5ea6\u548c\u6bcf\u4e2a\u901a\u9053\uff08\u6df1\u5ea6\uff09\u8fdb\u884c\u3002\u9700\u8981\u660e\u786e\u7684\u662f\uff0c\u5982\u679c\u6bcf\u4e2a\u78c1\u5934\u4e2d\u7684\u901a\u9053\u6570\u4e3a d_k\uff0c\u5219\u6bcf\u4e2a d_k \u901a\u9053\u7684\u5377\u79ef\u5c06\u6709 1 X 3 \u4e2a\u5185\u6838\u3002</p>\n<p><a href=\"https://nn.labml.ai/transformers/primer_ez/experiment.html\">\u4ee5\u4e0b\u662f Primer EZ \u7684\u5b9e\u9a8c\u4ee3\u7801</a>\u3002</p>\n",
 "Primer: Searching for Efficient Transformers for Language Modeling": "\u5165\u95e8\uff1a\u4e3a\u8bed\u8a00\u5efa\u6a21\u5bfb\u627e\u9ad8\u6548\u7684\u53d8\u6362\u5668"
}