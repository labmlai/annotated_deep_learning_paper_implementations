{
 "<h1><a href=\"https://nn.labml.ai/transformers/compressive/index.html\">Compressive Transformer</a></h1>\n<p>This is an implementation of <a href=\"https://arxiv.org/abs/1911.05507\">Compressive Transformers for Long-Range Sequence Modelling</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n<p>This is an extension of <a href=\"https://nn.labml.ai/transformers/xl/index.html\">Transformer XL</a> where past memories are compressed to give a longer attention range. That is, the furthest <span translate=no>_^_0_^_</span> memories are compressed into <span translate=no>_^_1_^_</span> memories, where <span translate=no>_^_2_^_</span> is the compression rate.</p>\n<h2>Compression operation</h2>\n<p>The compression operation is defined as <span translate=no>_^_3_^_</span>. The paper introduces multiple choices for <span translate=no>_^_4_^_</span> and we have only implemented 1D convolution which seems to give the best results. Each layer has a separate compression operation <span translate=no>_^_5_^_</span> where <span translate=no>_^_6_^_</span> is the layer number.</p>\n<h2>Training compression operation</h2>\n<p>Since training compression with BPTT requires maintaining a very large computational graph (many time steps), the paper proposes an <em>auto-encoding loss</em> and an <em>attention reconstruction loss</em>. The auto-encoding loss decodes the original memories from the compressed memories and calculates the loss. Attention reconstruction loss computes the multi-headed attention results on the compressed memory and on uncompressed memory and gets a mean squared error between them. We have implemented the latter here since it gives better results.</p>\n<p>This implementation uses pre-layer normalization while the paper uses post-layer normalization. Pre-layer norm does the layer norm before <a href=\"../feedforward.html\">FFN</a> and self-attention, and the pass-through in the residual connection is not normalized. This is supposed to be more stable in standard transformer setups.</p>\n<p>Here are <a href=\"https://nn.labml.ai/transformers/compressive/experiment.html\">the training code</a> and a notebook for training a compressive transformer model on the Tiny Shakespeare dataset.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/compressive/experiment.ipynb\"><span translate=no>_^_7_^_</span></a> </p>\n": "<h1><a href=\"https://nn.labml.ai/transformers/compressive/index.html\">\u5727\u7e2e\u5909\u5727\u5668</a></h1>\n<p><a href=\"https://pytorch.org\">\u3053\u308c\u306f PyTorch <a href=\"https://arxiv.org/abs/1911.05507\">\u306e\u9577\u8ddd\u96e2\u30b7\u30fc\u30b1\u30f3\u30b9\u30e2\u30c7\u30ea\u30f3\u30b0\u7528\u306e\u5727\u7e2e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</a></p>\n<p><a href=\"https://nn.labml.ai/transformers/xl/index.html\">\u3053\u308c\u306fTransformer XL\u306e\u62e1\u5f35\u7248\u3067</a>\u3001\u904e\u53bb\u306e\u8a18\u61b6\u3092\u5727\u7e2e\u3057\u3066\u6ce8\u610f\u7bc4\u56f2\u3092\u5e83\u3052\u3066\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u3001<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u6700\u3082\u9060\u3044\u30e1\u30e2\u30ea\u304c\u30e1\u30e2\u30ea\u306b\u5727\u7e2e\u3055\u308c\u307e\u3059\u3002\u3053\u3053\u3067\u3001<span translate=no>_^_2_^_</span>\u306f\u5727\u7e2e\u7387\u3067\u3059</p>\u3002\n<h2>\u5727\u7e2e\u64cd\u4f5c</h2>\n<p>\u5727\u7e2e\u64cd\u4f5c\u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059<span translate=no>_^_3_^_</span>\u3002\u3053\u306e\u8ad6\u6587\u3067\u306f\u8907\u6570\u306e\u9078\u629e\u80a2\u3092\u7d39\u4ecb\u3057\u3066\u3044\u307e\u3059\u304c<span translate=no>_^_4_^_</span>\u3001\u6700\u826f\u306e\u7d50\u679c\u304c\u5f97\u3089\u308c\u308b\u3068\u601d\u308f\u308c\u308b1\u6b21\u5143\u306e\u7573\u307f\u8fbc\u307f\u306e\u307f\u3092\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002\u5404\u30ec\u30a4\u30e4\u30fc\u306b\u306f\u500b\u5225\u306e\u5727\u7e2e\u64cd\u4f5c\u304c\u3042\u308a\u307e\u3059\u3002<span translate=no>_^_5_^_</span>\u3053\u3053\u3067<span translate=no>_^_6_^_</span>\u3001\u306f\u30ec\u30a4\u30e4\u30fc\u756a\u53f7\u3067\u3059\u3002</p>\n<h2>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u7528\u5727\u7e2e\u64cd\u4f5c</h2>\n<p><em>BPTT\u306b\u3088\u308b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u5727\u7e2e\u3067\u306f\u3001\u975e\u5e38\u306b\u5927\u304d\u306a\u8a08\u7b97\u30b0\u30e9\u30d5\uff08\u591a\u304f\u306e\u30bf\u30a4\u30e0\u30b9\u30c6\u30c3\u30d7\uff09\u3092\u7dad\u6301\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001<em>\u3053\u306e\u8ad6\u6587\u3067\u306f\u81ea\u52d5\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u640d\u5931\u3068\u6ce8\u610f\u518d\u69cb\u6210\u640d\u5931\u3092\u63d0\u6848\u3057\u3066\u3044\u307e\u3059</em>\u3002</em>\u81ea\u52d5\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u640d\u5931\u306f\u3001\u5727\u7e2e\u3055\u308c\u305f\u30e1\u30e2\u30ea\u304b\u3089\u5143\u306e\u30e1\u30e2\u30ea\u3092\u30c7\u30b3\u30fc\u30c9\u3057\u3001\u640d\u5931\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u518d\u69cb\u6210\u640d\u5931\u3067\u306f\u3001\u5727\u7e2e\u30e1\u30e2\u30ea\u3068\u975e\u5727\u7e2e\u30e1\u30e2\u30ea\u3067\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u7d50\u679c\u3092\u8a08\u7b97\u3057\u3001\u305d\u308c\u3089\u306e\u9593\u306e\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u3092\u6c42\u3081\u307e\u3059\u3002\u5f8c\u8005\u306e\u65b9\u304c\u826f\u3044\u7d50\u679c\u304c\u5f97\u3089\u308c\u308b\u305f\u3081\u3001\u3053\u3053\u3067\u306f\u5f8c\u8005\u3092\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002</p>\n<p>\u3053\u306e\u5b9f\u88c5\u3067\u306f\u30ec\u30a4\u30e4\u30fc\u524d\u306e\u6b63\u898f\u5316\u3092\u4f7f\u7528\u3057\u307e\u3059\u304c\u3001\u30da\u30fc\u30d1\u30fc\u3067\u306f\u30ec\u30a4\u30e4\u30fc\u5f8c\u306e\u6b63\u898f\u5316\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002<a href=\"../feedforward.html\">\u524d\u5c64\u30ce\u30eb\u30e0\u306fFFN\u3084\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u524d\u306e\u5c64\u30ce\u30eb\u30e0\u3092\u884c\u3044</a>\u3001\u6b8b\u5dee\u63a5\u7d9a\u3067\u306e\u30d1\u30b9\u30b9\u30eb\u30fc\u306f\u6b63\u898f\u5316\u3055\u308c\u307e\u305b\u3093\u3002\u3053\u308c\u306f\u6a19\u6e96\u7684\u306a\u5909\u5727\u5668\u306e\u8a2d\u5b9a\u3067\u306f\u3088\u308a\u5b89\u5b9a\u3057\u3066\u3044\u308b\u306f\u305a\u3067\u3059</p>\u3002\n<p>Tiny <a href=\"https://nn.labml.ai/transformers/compressive/experiment.html\">Shakespeare\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u5727\u7e2e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u3068\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059</a>\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/compressive/experiment.ipynb\"><span translate=no>_^_7_^_</span></a></p>\n",
 "Compressive Transformer": "\u5727\u7e2e\u5909\u5727\u5668"
}