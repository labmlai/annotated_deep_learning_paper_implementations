{
 "<h1><a href=\"https://nn.labml.ai/transformers/mlp_mixer/index.html\">MLP-Mixer: An all-MLP Architecture for Vision</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2105.01601\">MLP-Mixer: An all-MLP Architecture for Vision</a>.</p>\n<p>This paper applies the model on vision tasks. The model is similar to a transformer with attention layer being replaced by a MLP that is applied across the patches (or tokens in case of a NLP task).</p>\n<p>Our implementation of MLP Mixer is a drop in replacement for the <a href=\"https://nn.labml.ai/transformers/mha.html\">self-attention layer</a> in <a href=\"https://nn.labml.ai/transformers/models.html\">our transformer implementation</a>. So it&#x27;s just a couple of lines of code, transposing the tensor to apply the MLP across the sequence dimension.</p>\n<p>Although the paper applied MLP Mixer on vision tasks, we tried it on a <a href=\"https://nn.labml.ai/transformers/mlm/index.html\">masked language model</a>. <a href=\"https://nn.labml.ai/transformers/mlp_mixer/experiment.html\">Here is the experiment code</a>. </p>\n": "<h1><a href=\"https://nn.labml.ai/transformers/mlp_mixer/index.html\">MLP-Mixer\uff1a\u9002\u7528\u4e8e\u89c6\u89c9\u7684\u5168 MLP \u67b6\u6784</a></h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch</a> \u5bf9\u8bba\u6587 <a href=\"https://arxiv.org/abs/2105.01601\">MLP-Mixer\uff1a\u9002\u7528\u4e8e\u89c6\u89c9\u7684\u5168 MLP \u67b6\u6784\u7684</a>\u5b9e\u73b0\u3002</p>\n<p>\u672c\u6587\u5c06\u8be5\u6a21\u578b\u5e94\u7528\u4e8e\u89c6\u89c9\u4efb\u52a1\u3002\u8be5\u6a21\u578b\u7c7b\u4f3c\u4e8e\u53d8\u538b\u5668\uff0c\u6ce8\u610f\u529b\u5c42\u88ab\u5e94\u7528\u4e8e\u8865\u4e01\u7684 MLP\uff08\u5982\u679c\u662f NLP \u4efb\u52a1\uff0c\u5219\u4e3a\u4ee3\u5e01\uff09\u3002</p>\n<p>\u6211\u4eec\u5b9e\u73b0\u7684 MLP Mixer \u5b8c\u5168\u53d6\u4ee3\u4e86<a href=\"https://nn.labml.ai/transformers/models.html\">\u53d8\u538b\u5668\u5b9e\u73b0</a>\u4e2d\u7684<a href=\"https://nn.labml.ai/transformers/mha.html\">\u81ea\u6ce8\u610f\u529b\u5c42</a>\u3002\u56e0\u6b64\uff0c\u8fd9\u53ea\u662f\u51e0\u884c\u4ee3\u7801\uff0c\u5bf9\u5f20\u91cf\u8fdb\u884c\u8f6c\u7f6e\u4ee5\u5728\u5e8f\u5217\u7ef4\u5ea6\u4e0a\u5e94\u7528 MLP\u3002</p>\n<p>\u5c3d\u7ba1\u8be5\u8bba\u6587\u5c06 MLP Mixer \u5e94\u7528\u4e8e\u89c6\u89c9\u4efb\u52a1\uff0c\u4f46\u6211\u4eec\u5728<a href=\"https://nn.labml.ai/transformers/mlm/index.html\">\u63a9\u7801\u8bed\u8a00\u6a21\u578b</a>\u4e0a\u8fdb\u884c\u4e86\u5c1d\u8bd5\u3002<a href=\"https://nn.labml.ai/transformers/mlp_mixer/experiment.html\">\u8fd9\u662f\u5b9e\u9a8c\u4ee3\u7801</a>\u3002</p>\n",
 "MLP-Mixer: An all-MLP Architecture for Vision": "MLP \u6df7\u97f3\u5668\uff1a\u9762\u5411\u89c6\u89c9\u7684\u5168 MLP \u67b6\u6784"
}