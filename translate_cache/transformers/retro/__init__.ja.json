{
 "<h1>Retrieval-Enhanced Transformer (Retro)</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2112.04426\">Improving language models by retrieving from trillions of tokens</a>.</p>\n<p>It builds a database of chunks of text. It is a key-value database where the keys are indexed by the BERT embeddings of the chunks. They use a frozen pre-trained BERT model to calculate these embeddings. The values are the corresponding chunks and an equal length of text proceeding that chunk.</p>\n<p>Then the model retrieves text similar (nearest neighbors) to the input to the model from this database. These retrieved texts are used to predict the output.</p>\n<p>Since we use a frozen BERT model for retrieval we can pre-calculate all the nearest neighbors for the training dataset. This speeds up the training process.</p>\n<p>Components:</p>\n<ul><li><a href=\"bert_embeddings.html\">BERT embeddings</a>: Code to get BERT embeddings of chunks of text. </li>\n<li><a href=\"database.html\">Key-value database</a>: Build and retrieve chunks </li>\n<li><a href=\"model.html\">Model</a> </li>\n<li><a href=\"dataset.html\">Dataset</a>: Pre-calculate the nearest neighbors </li>\n<li><a href=\"train.html\">Training code</a></li></ul>\n": "<h1>\u691c\u7d22\u6a5f\u80fd\u4ed8\u304d\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc (\u30ec\u30c8\u30ed)</h1>\n<p>\u3053\u308c\u306f\u3001<a href=\"https://pytorch.org\"><a href=\"https://arxiv.org/abs/2112.04426\">\u6570\u5146\u306e\u30c8\u30fc\u30af\u30f3\u304b\u3089\u53d6\u5f97\u3059\u308b\u3053\u3068\u306b\u3088\u308b\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u6539\u5584\u3068\u3044\u3046\u8ad6\u6587\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a></a>\u3002</p>\n<p>\u30c6\u30ad\u30b9\u30c8\u306e\u584a\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u30c1\u30e3\u30f3\u30af\u306eBERT\u57cb\u3081\u8fbc\u307f\u306b\u3088\u3063\u3066\u30ad\u30fc\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u4ed8\u3051\u3089\u308c\u308b\u30ad\u30fc\u30d0\u30ea\u30e5\u30fc\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3067\u3059\u3002\u3053\u308c\u3089\u306e\u57cb\u3081\u8fbc\u307f\u3092\u8a08\u7b97\u3059\u308b\u306b\u306f\u3001\u4e8b\u524d\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u6e08\u307f\u306e\u30d5\u30ea\u30fc\u30ba\u3057\u305f BERT \u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\u5024\u306f\u3001\u5bfe\u5fdc\u3059\u308b\u30c1\u30e3\u30f3\u30af\u3068\u3001\u305d\u306e\u30c1\u30e3\u30f3\u30af\u306e\u524d\u306b\u7d9a\u304f\u540c\u3058\u9577\u3055\u306e\u30c6\u30ad\u30b9\u30c8\u3067\u3059\u3002</p>\n<p>\u6b21\u306b\u3001\u30e2\u30c7\u30eb\u306f\u3053\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u30e2\u30c7\u30eb\u3078\u306e\u5165\u529b\u306b\u985e\u4f3c\u3057\u305f (\u6700\u3082\u8fd1\u3044\u8fd1\u508d\u306e) \u30c6\u30ad\u30b9\u30c8\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002\u53d6\u5f97\u3057\u305f\u3053\u308c\u3089\u306e\u30c6\u30ad\u30b9\u30c8\u306f\u3001\u51fa\u529b\u306e\u4e88\u6e2c\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059</p>\u3002\n<p>\u691c\u7d22\u306b\u306f\u30d5\u30ea\u30fc\u30ba\u3057\u305f BERT \u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u3059\u3079\u3066\u306e\u6700\u8fd1\u508d\u3092\u4e8b\u524d\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30d7\u30ed\u30bb\u30b9\u304c\u30b9\u30d4\u30fc\u30c9\u30a2\u30c3\u30d7\u3057\u307e\u3059\u3002</p>\n<p>\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8:</p>\n<ul><li><a href=\"bert_embeddings.html\">BERT \u57cb\u3081\u8fbc\u307f:\u30c6\u30ad\u30b9\u30c8\u306e\u30c1\u30e3\u30f3\u30af\u3092</a> BERT \u306b\u57cb\u3081\u8fbc\u3080\u305f\u3081\u306e\u30b3\u30fc\u30c9\u3002</li>\n<li><a href=\"database.html\">\u30ad\u30fc\u30d0\u30ea\u30e5\u30fc\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9:\u30c1\u30e3\u30f3\u30af\u306e\u69cb\u7bc9\u3068\u53d6\u5f97</a></li>\n<li><a href=\"model.html\">\u30e2\u30c7\u30eb</a></li>\n<li><a href=\"dataset.html\">\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8</a>:\u6700\u3082\u8fd1\u3044\u8fd1\u508d\u3092\u4e8b\u524d\u8a08\u7b97</li>\n<li><a href=\"train.html\">\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9</a></li></ul>\n",
 "Retrieval-Enhanced Transformer (Retro)": "\u691c\u7d22\u6a5f\u80fd\u4ed8\u304d\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc (\u30ec\u30c8\u30ed)",
 "This is a PyTorch implementation/tutorial of the paper Improving language models by retrieving from trillions of tokens. It builds a key-value database of chunks of text and retrieves and uses them when making predictions.": "\u3053\u308c\u306f\u3001\u6570\u5146\u306e\u30c8\u30fc\u30af\u30f3\u304b\u3089\u53d6\u5f97\u3059\u308b\u3053\u3068\u306b\u3088\u308b\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u6539\u5584\u3068\u3044\u3046\u8ad6\u6587\u306ePyTorch\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u3059\u3002\u30c6\u30ad\u30b9\u30c8\u306e\u30c1\u30e3\u30f3\u30af\u304b\u3089\u6210\u308b\u30ad\u30fc\u30d0\u30ea\u30e5\u30fc\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u69cb\u7bc9\u3057\u3001\u305d\u308c\u3089\u3092\u53d6\u5f97\u3057\u3066\u4e88\u6e2c\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002"
}