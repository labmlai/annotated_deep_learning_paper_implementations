{
 "<h1>Retrieval-Enhanced Transformer (Retro)</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2112.04426\">Improving language models by retrieving from trillions of tokens</a>.</p>\n<p>It builds a database of chunks of text. It is a key-value database where the keys are indexed by the BERT embeddings of the chunks. They use a frozen pre-trained BERT model to calculate these embeddings. The values are the corresponding chunks and an equal length of text proceeding that chunk.</p>\n<p>Then the model retrieves text similar (nearest neighbors) to the input to the model from this database. These retrieved texts are used to predict the output.</p>\n<p>Since we use a frozen BERT model for retrieval we can pre-calculate all the nearest neighbors for the training dataset. This speeds up the training process.</p>\n<p>Components:</p>\n<ul><li><a href=\"bert_embeddings.html\">BERT embeddings</a>: Code to get BERT embeddings of chunks of text. </li>\n<li><a href=\"database.html\">Key-value database</a>: Build and retrieve chunks </li>\n<li><a href=\"model.html\">Model</a> </li>\n<li><a href=\"dataset.html\">Dataset</a>: Pre-calculate the nearest neighbors </li>\n<li><a href=\"train.html\">Training code</a></li></ul>\n": "<h1>\u68c0\u7d22\u589e\u5f3a\u578b\u53d8\u538b\u5668\uff08\u590d\u53e4\uff09</h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch</a> \u5bf9\u8bba\u6587\u300a<a href=\"https://arxiv.org/abs/2112.04426\">\u901a\u8fc7\u4ece\u6570\u4e07\u4ebf\u4e2a\u4ee3\u5e01\u4e2d\u68c0\u7d22\u6765\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u300b\u7684</a>\u5b9e\u73b0\u3002</p>\n<p>\u5b83\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u5927\u91cf\u6587\u672c\u7684\u6570\u636e\u5e93\u3002\u5b83\u662f\u4e00\u4e2a\u952e\u503c\u6570\u636e\u5e93\uff0c\u5176\u4e2d\u7684\u5bc6\u94a5\u7531\u533a\u5757\u7684 BERT \u5d4c\u5165\u7d22\u5f15\u3002\u4ed6\u4eec\u4f7f\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u7684 BERT \u6a21\u578b\u6765\u8ba1\u7b97\u8fd9\u4e9b\u5d4c\u5165\u3002\u8fd9\u4e9b\u503c\u662f\u76f8\u5e94\u7684\u533a\u5757\u548c\u8be5\u533a\u5757\u7684\u7b49\u957f\u5ea6\u6587\u672c\u3002</p>\n<p>\u7136\u540e\uff0c\u6a21\u578b\u4ece\u8be5\u6570\u636e\u5e93\u68c0\u7d22\u4e0e\u6a21\u578b\u8f93\u5165\u76f8\u4f3c\uff08\u6700\u8fd1\u90bb\u57df\uff09\u7684\u6587\u672c\u3002\u8fd9\u4e9b\u68c0\u7d22\u5230\u7684\u6587\u672c\u7528\u4e8e\u9884\u6d4b\u8f93\u51fa\u3002</p>\n<p>\u7531\u4e8e\u6211\u4eec\u4f7f\u7528\u51bb\u7ed3\u7684 BERT \u6a21\u578b\u8fdb\u884c\u68c0\u7d22\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u9884\u5148\u8ba1\u7b97\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6240\u6709\u6700\u8fd1\u90bb\u57df\u3002\u8fd9\u52a0\u5feb\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002</p>\n<p>\u7ec4\u4ef6\uff1a</p>\n<ul><li><a href=\"bert_embeddings.html\">BERT \u5d4c\u5165</a>\uff1a\u7528\u4e8e\u83b7\u53d6\u5927\u5757\u6587\u672c\u7684 BERT \u5d4c\u5165\u7684\u4ee3\u7801\u3002</li>\n<li><a href=\"database.html\">\u952e\u503c\u6570\u636e\u5e93</a>\uff1a\u751f\u6210\u548c\u68c0\u7d22\u533a\u5757</li>\n<li><a href=\"model.html\">\u6a21\u578b</a></li>\n<li><a href=\"dataset.html\">\u6570\u636e\u96c6</a>\uff1a\u9884\u5148\u8ba1\u7b97\u6700\u8fd1\u7684\u90bb\u5c45</li>\n<li><a href=\"train.html\">\u8bad\u7ec3\u4ee3\u7801</a></li></ul>\n",
 "Retrieval-Enhanced Transformer (Retro)": "\u68c0\u7d22\u589e\u5f3a\u578b\u53d8\u538b\u5668\uff08\u590d\u53e4\uff09",
 "This is a PyTorch implementation/tutorial of the paper Improving language models by retrieving from trillions of tokens. It builds a key-value database of chunks of text and retrieves and uses them when making predictions.": "\u8fd9\u662f\u8bba\u6587\u300a\u901a\u8fc7\u4ece\u6570\u4e07\u4ebf\u4e2a\u4ee4\u724c\u4e2d\u68c0\u7d22\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u300b\u7684 PyTorch \u5b9e\u73b0/\u6559\u7a0b\u3002\u5b83\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u6587\u672c\u5757\u7684\u952e\u503c\u6570\u636e\u5e93\uff0c\u5e76\u5728\u8fdb\u884c\u9884\u6d4b\u65f6\u68c0\u7d22\u548c\u4f7f\u7528\u5b83\u4eec\u3002"
}