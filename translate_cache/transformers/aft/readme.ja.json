{
 "<h1><a href=\"https://nn.labml.ai/transformers/aft/index.html\">An Attention Free Transformer</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2105.14103\">An Attention Free Transformer</a>.</p>\n<p>This paper replaces the <a href=\"https://nn.labml.ai/transformers/mha.html\">self-attention layer</a> with a new efficient operation, that has memory complexity of O(Td), where T is the sequence length and <span translate=no>_^_0_^_</span> is the dimensionality of embeddings.</p>\n<p>The paper introduces AFT along with AFT-local and AFT-conv. Here we have implemented AFT-local which pays attention to closeby tokens in an autoregressive model. </p>\n": "<h1><a href=\"https://nn.labml.ai/transformers/aft/index.html\">\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d5\u30ea\u30fc\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc</a></h1>\n<p>\u3053\u308c\u306f\u3001\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/2105.14103\">\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30fb\u30d5\u30ea\u30fc\u30fb\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u300d<a href=\"https://pytorch.org\">\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a></a>\u3002</p>\n<p>\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001<a href=\"https://nn.labml.ai/transformers/mha.html\">\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u5c64\u3092\u65b0\u3057\u3044\u52b9\u7387\u7684\u306a\u6f14\u7b97\u306b\u7f6e\u304d\u63db\u3048\u307e\u3059</a>\u3002\u30e1\u30e2\u30ea\u8907\u96d1\u5ea6\u306f O (Td) \u3067\u3001T \u306f\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9577\u3055\u3067\u3001<span translate=no>_^_0_^_</span>\u57cb\u3081\u8fbc\u307f\u306e\u6b21\u5143\u3067\u3059\u3002</p>\n<p>\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001AFT\u3068AFT\u30ed\u30fc\u30ab\u30eb\u304a\u3088\u3073AFT-Conv\u306b\u3064\u3044\u3066\u7d39\u4ecb\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u81ea\u5df1\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u8fd1\u304f\u306e\u30c8\u30fc\u30af\u30f3\u306b\u6ce8\u76ee\u3059\u308bAFT-Local\u3092\u5b9f\u88c5\u3057\u307e\u3057\u305f</p>\u3002\n",
 "An Attention Free Transformer": "\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d5\u30ea\u30fc\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc"
}