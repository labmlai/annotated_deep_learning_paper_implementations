{
 "<h1>Pay Attention to MLPs (gMLP)</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2105.08050\">Pay Attention to MLPs</a>.</p>\n<p>This paper introduces a Multilayer Perceptron (MLP) based architecture with gating, which they name <strong>gMLP</strong>. It consists of a stack of <span translate=no>_^_0_^_</span> <em>gMLP</em> blocks.</p>\n<p>Here is <a href=\"experiment.html\">the training code</a> for a gMLP model based autoregressive model.</p>\n": "<h1>MLP (GMLP) \u306b\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044</h1>\n<p>\u3053\u308c\u306f\u3001\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/2105.08050\">MLP\u306b\u6ce8\u610f\u3057\u3066</a>\u300d<a href=\"https://pytorch.org\">\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a>\u3002</p>\n<p><strong>\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001\u30b2\u30fc\u30c6\u30a3\u30f3\u30b0\u3092\u5099\u3048\u305f\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\uff08MLP\uff09\u30d9\u30fc\u30b9\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\uff08GMLP\u3068\u540d\u4ed8\u3051\u3089\u308c\u3066\u3044\u307e\u3059\uff09\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</strong><span translate=no>_^_0_^_</span><em>gMLP</em> \u30d6\u30ed\u30c3\u30af\u306e\u30b9\u30bf\u30c3\u30af\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059</p>\u3002\n<p><a href=\"experiment.html\">gMLP\u30e2\u30c7\u30eb\u30d9\u30fc\u30b9\u306e\u81ea\u5df1\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059</a>\u3002</p>\n",
 "<h2>Spatial Gating Unit</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p>where <span translate=no>_^_1_^_</span> is a linear transformation along the sequence dimension, and <span translate=no>_^_2_^_</span> is element-wise multiplication. <span translate=no>_^_3_^_</span> is split into to parts of equal size <span translate=no>_^_4_^_</span> and <span translate=no>_^_5_^_</span> along the channel dimension (embedding dimension).</p>\n": "<h2>\u7a7a\u9593\u30b2\u30fc\u30c8\u30e6\u30cb\u30c3\u30c8</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p>\u3053\u3053\u3067<span translate=no>_^_1_^_</span>\u3001\u306f\u30b7\u30fc\u30b1\u30f3\u30b9\u6b21\u5143\u306b\u6cbf\u3063\u305f\u7dda\u5f62\u5909\u63db\u3067\u3001<span translate=no>_^_2_^_</span>\u306f\u8981\u7d20\u5358\u4f4d\u306e\u4e57\u7b97\u3067\u3059\u3002<span translate=no>_^_3_^_</span>\u30c1\u30e3\u30cd\u30eb\u5bf8\u6cd5\uff08\u57cb\u3081\u8fbc\u307f\u5bf8\u6cd5\uff09<span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\u306b\u6cbf\u3063\u3066\u540c\u3058\u30b5\u30a4\u30ba\u306e2\u3064\u306e\u90e8\u5206\u306b\u5206\u5272\u3055\u308c\u307e\u3059</p>\u3002\n",
 "<h2>gMLP Block</h2>\n<p>Each block does the following transformations to input embeddings <span translate=no>_^_0_^_</span> where <span translate=no>_^_1_^_</span> is the sequence length and <span translate=no>_^_2_^_</span> is the dimensionality of the embeddings:</p>\n<span translate=no>_^_3_^_</span><p>where <span translate=no>_^_4_^_</span> and <span translate=no>_^_5_^_</span> are learnable projection weights. <span translate=no>_^_6_^_</span> is the Spacial Gating Unit defined below. Output dimensionality of <span translate=no>_^_7_^_</span> will be half of <span translate=no>_^_8_^_</span>. <span translate=no>_^_9_^_</span> is an activation function such as <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.GELU.html\">GeLU</a>.</p>\n": "<h2>GmLP \u30d6\u30ed\u30c3\u30af</h2>\n<p>\u5404\u30d6\u30ed\u30c3\u30af\u306f\u3001\u5165\u529b\u57cb\u3081\u8fbc\u307f\u306b\u5bfe\u3057\u3066\u6b21\u306e\u5909\u63db\u3092\u884c\u3044\u307e\u3059\u3002<span translate=no>_^_0_^_</span>\u3053\u3053\u3067\u3001\u306f\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9577\u3055\u3001<span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\u306f\u57cb\u3081\u8fbc\u307f\u306e\u6b21\u5143\u3067\u3059\u3002</p>\n<span translate=no>_^_3_^_</span><p><span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\u5b66\u7fd2\u53ef\u80fd\u306a\u6295\u5f71\u91cd\u307f\u306e\u4f4d\u7f6e\u3068\u4f4d\u7f6e<span translate=no>_^_6_^_</span>\u306f\u4ee5\u4e0b\u306b\u5b9a\u7fa9\u3059\u308b\u30b9\u30da\u30fc\u30b7\u30e3\u30eb\u30fb\u30b2\u30fc\u30c6\u30a3\u30f3\u30b0\u30fb\u30e6\u30cb\u30c3\u30c8\u3067\u3059\u3002<span translate=no>_^_7_^_</span>\u306e\u51fa\u529b\u6b21\u5143\u306f\u306e\u534a\u5206\u306b\u306a\u308a\u307e\u3059\u3002<span translate=no>_^_8_^_</span><span translate=no>_^_9_^_</span><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.GELU.html\">\u306fGelU\u306e\u3088\u3046\u306a\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u95a2\u6570\u3067\u3059</a></p>\u3002\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> has shape <span translate=no>_^_1_^_</span>. The batch dimension should be of size <span translate=no>_^_2_^_</span> because this implementation supports only same mask for all samples in the batch. </p>\n": "<p><span translate=no>_^_0_^_</span>\u5f62\u304c\u3042\u308a\u307e\u3059<span translate=no>_^_1_^_</span>\u3002<span translate=no>_^_2_^_</span>\u3053\u306e\u5b9f\u88c5\u3067\u306f\u30d0\u30c3\u30c1\u5185\u306e\u3059\u3079\u3066\u306e\u30b5\u30f3\u30d7\u30eb\u306b\u5bfe\u3057\u3066\u540c\u3058\u30de\u30b9\u30af\u3057\u304b\u30b5\u30dd\u30fc\u30c8\u3057\u306a\u3044\u305f\u3081\u3001\u30d0\u30c3\u30c1\u30c7\u30a3\u30e1\u30f3\u30b7\u30e7\u30f3\u306f\u30b5\u30a4\u30ba\u306b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>\n",
 "<p>Activation function <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u6a5f\u80fd <span translate=no>_^_0_^_</span></p>\n",
 "<p>Add the shortcut connection </p>\n": "<p>\u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8\u63a5\u7d9a\u3092\u8ffd\u52a0\u3059\u308b</p>\n",
 "<p>Apply mask to the weights.</p>\n<p>If <span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> then <span translate=no>_^_2_^_</span> will not get any information from token <span translate=no>_^_3_^_</span>. </p>\n": "<p>\u30a6\u30a7\u30a4\u30c8\u306b\u30de\u30b9\u30af\u3092\u304b\u3051\u307e\u3059\u3002</p>\n<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u3082\u3057\u305d\u3046\u306a\u3089<span translate=no>_^_2_^_</span>\u3001\u30c8\u30fc\u30af\u30f3\u304b\u3089\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068\u306f\u306a\u3044<span translate=no>_^_3_^_</span>\u3002</p>\n",
 "<p>Check mask </p>\n": "<p>\u30c1\u30a7\u30c3\u30af\u30de\u30b9\u30af</p>\n",
 "<p>Embedding size (required by <a href=\"../models.html#Encoder\">Encoder</a>. We use the encoder module from transformer architecture and plug <em>gMLP</em> block as a replacement for the <a href=\"../models.html#Encoder\">Transformer Layer</a>. </p>\n": "<p>\u57cb\u3081\u8fbc\u307f\u30b5\u30a4\u30ba (<a href=\"../models.html#Encoder\">\u30a8\u30f3\u30b3\u30fc\u30c0\u3067\u5fc5\u8981</a>)</p><a href=\"../models.html#Encoder\">\u30c8\u30e9\u30f3\u30b9\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u30a8\u30f3\u30b3\u30fc\u30c0\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u7528\u3057\u3001<em>\u30c8\u30e9\u30f3\u30b9\u30ec\u30a4\u30e4\u306e\u4ee3\u308f\u308a\u306bgMLP\u30d6\u30ed\u30c3\u30af\u3092\u30d7\u30e9\u30b0\u3057\u307e\u3059</em>\u3002</a>\n",
 "<p>Final projection <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6700\u7d42\u6295\u5f71 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Get sequence length </p>\n": "<p>\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9577\u3055\u3092\u53d6\u5f97</p>\n",
 "<p>Get the weight matrix; truncate if larger than <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30a6\u30a7\u30a4\u30c8\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u53d6\u5f97\u3002\u3053\u308c\u3088\u308a\u5927\u304d\u3044\u5834\u5408\u306f\u5207\u308a\u6368\u3066\u308b <span translate=no>_^_0_^_</span></p>\n",
 "<p>Here we only support the same mask for all samples </p>\n": "<p>\u3053\u3053\u3067\u306f\u3001\u3059\u3079\u3066\u306e\u30b5\u30f3\u30d7\u30eb\u3067\u540c\u3058\u30de\u30b9\u30af\u306e\u307f\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059</p>\n",
 "<p>Keep a copy for shortcut connection </p>\n": "<p>\u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8\u63a5\u7d9a\u7528\u306b\u30b3\u30d4\u30fc\u3092\u4fdd\u5b58</p>\n",
 "<p>Normalization layer before applying <span translate=no>_^_0_^_</span> </p>\n": "<p>\u9069\u7528\u524d\u306e\u6b63\u898f\u5316\u30ec\u30a4\u30e4\u30fc <span translate=no>_^_0_^_</span></p>\n",
 "<p>Normalization layer fro Pre-Norm </p>\n": "<p>\u30d7\u30ec\u30ce\u30eb\u30e0\u306e\u6b63\u898f\u5316\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Normalize <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30ce\u30fc\u30de\u30e9\u30a4\u30ba <span translate=no>_^_0_^_</span></p>\n",
 "<p>Normalize <span translate=no>_^_0_^_</span> before <span translate=no>_^_1_^_</span> </p>\n": "<p>\u524d\u306b\u30ce\u30fc\u30de\u30e9\u30a4\u30ba <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span></p>\n",
 "<p>Projection and activation <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30d7\u30ed\u30b8\u30a7\u30af\u30b7\u30e7\u30f3\u3068\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Projection layer for <span translate=no>_^_0_^_</span> </p>\n": "<p>\u306e\u6295\u5f71\u30ec\u30a4\u30e4\u30fc <span translate=no>_^_0_^_</span></p>\n",
 "<p>Remove the batch dimension </p>\n": "<p>\u30d0\u30c3\u30c1\u30c7\u30a3\u30e1\u30f3\u30b7\u30e7\u30f3\u3092\u524a\u9664\u3059\u308b</p>\n",
 "<p>Spacial Gating Unit <span translate=no>_^_0_^_</span> </p>\n": "<p>\u7a7a\u9593\u30b2\u30fc\u30c8\u30e6\u30cb\u30c3\u30c8 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Split <span translate=no>_^_0_^_</span> into <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u3068\u306b\u5206\u5272 <span translate=no>_^_2_^_</span></p>\n",
 "<p>Weight <span translate=no>_^_0_^_</span> in <span translate=no>_^_1_^_</span>.</p>\n<p>The paper notes that it&#x27;s important to initialize weights to small values and the bias to <span translate=no>_^_2_^_</span>, so that during the initial training <span translate=no>_^_3_^_</span> is close to identity (apart from the split). </p>\n": "<p>\u91cd\u91cf <span translate=no>_^_0_^_</span> (\u30a4\u30f3<span translate=no>_^_1_^_</span>)</p>\n<p>\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001\u91cd\u307f\u3092\u5c0f\u3055\u3044\u5024\u306b\u521d\u671f\u5316\u3057\u3001\u30d0\u30a4\u30a2\u30b9\u3092\u306b\u521d\u671f\u5316\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3042\u308b\u3068\u8ff0\u3079\u3066\u3044\u307e\u3059\u3002\u305d\u3046\u3059\u308c\u3070<span translate=no>_^_2_^_</span>\u3001\u6700\u521d\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u4e2d\u306b\uff08<span translate=no>_^_3_^_</span>\u5206\u5272\u306f\u5225\u3068\u3057\u3066\uff09\u540c\u4e00\u306b\u8fd1\u3044\u3082\u306e\u306b\u306a\u308a\u307e\u3059\u3002</p>\n",
 "<p>Weight <span translate=no>_^_0_^_</span> in <span translate=no>_^_1_^_</span></p>\n<p>The paper notes that it&#x27;s important to initialize bias to <span translate=no>_^_2_^_</span>. </p>\n": "<p>\u91cd\u91cf (<span translate=no>_^_0_^_</span>\u30a4\u30f3) <span translate=no>_^_1_^_</span></p>\n<p>\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001\u30d0\u30a4\u30a2\u30b9\u3092\u306b\u521d\u671f\u5316\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3060\u3068\u6307\u6458\u3057\u3066\u3044\u307e\u3059\u3002<span translate=no>_^_2_^_</span></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the dimensionality (<span translate=no>_^_1_^_</span>) of <span translate=no>_^_2_^_</span> </li>\n<li><span translate=no>_^_3_^_</span> is the dimensionality of <span translate=no>_^_4_^_</span> </li>\n<li><span translate=no>_^_5_^_</span> is the length of the token sequence (<span translate=no>_^_6_^_</span>)</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u306e\u6b21\u5143 () <span translate=no>_^_1_^_</span> <span translate=no>_^_2_^_</span></li>\n<li><span translate=no>_^_3_^_</span>\u306e\u6b21\u5143\u3067\u3059 <span translate=no>_^_4_^_</span></li>\n<li><span translate=no>_^_5_^_</span>\u306f\u30c8\u30fc\u30af\u30f3\u30fb\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9577\u3055 (<span translate=no>_^_6_^_</span>)</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the dimensionality of <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the sequence length</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306e\u6b21\u5143\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u306f\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9577\u3055\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input <span translate=no>_^_1_^_</span> of shape <span translate=no>_^_2_^_</span> </li>\n<li><span translate=no>_^_3_^_</span> is is a boolean mask of shape <span translate=no>_^_4_^_</span> that controls the visibility of tokens  among each other. The last dimension of size <span translate=no>_^_5_^_</span> is the batch, which we have in other transformer  implementations and was left for compatibility.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u5f62\u72b6\u306e\u5165\u529b\u3067\u3059 <span translate=no>_^_2_^_</span></li>\n<li><span translate=no>_^_3_^_</span>is \u306f\u3001<span translate=no>_^_4_^_</span>\u30c8\u30fc\u30af\u30f3\u540c\u58eb\u306e\u53ef\u8996\u6027\u3092\u5236\u5fa1\u3059\u308b\u30d6\u30fc\u30ea\u30a2\u30f3\u30de\u30b9\u30af\u3067\u3059\u3002<span translate=no>_^_5_^_</span>\u30b5\u30a4\u30ba\u306e\u6700\u5f8c\u306e\u30c7\u30a3\u30e1\u30f3\u30b7\u30e7\u30f3\u306f\u30d0\u30c3\u30c1\u3067\u3059\u3002\u3053\u308c\u306f\u4ed6\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u5b9f\u88c5\u306b\u3082\u3042\u308a\u307e\u3059\u304c\u3001\u4e92\u63db\u6027\u306e\u305f\u3081\u306b\u6b8b\u3055\u308c\u3066\u3044\u307e\u3059</li></ul>\u3002\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input embedding tensor <span translate=no>_^_1_^_</span> of shape <span translate=no>_^_2_^_</span> </li>\n<li><span translate=no>_^_3_^_</span> is a boolean mask of shape <span translate=no>_^_4_^_</span> that controls the visibility of tokens  among each other.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u5f62\u72b6\u306e\u5165\u529b\u57cb\u3081\u8fbc\u307f\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_2_^_</span></li>\n</ul><li><span translate=no>_^_3_^_</span>\u306f\u3001<span translate=no>_^_4_^_</span>\u30c8\u30fc\u30af\u30f3\u540c\u58eb\u306e\u53ef\u8996\u6027\u3092\u5236\u5fa1\u3059\u308b\u30d6\u30fc\u30ea\u30a2\u30f3\u30b7\u30a7\u30a4\u30d7\u30de\u30b9\u30af\u3067\u3059\u3002</li>\n",
 "Pay Attention to MLPs (gMLP)": "MLP (GMLP) \u306b\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044",
 "This is an annotated implementation/tutorial of Pay Attention to MLPs (gMLP) in PyTorch.": "\u3053\u308c\u306f PyTorch \u306e\u300cMLP\uff08GMLP\uff09\u306b\u6ce8\u610f\u300d\u306e\u6ce8\u91c8\u4ed8\u304d\u306e\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u3059\u3002"
}