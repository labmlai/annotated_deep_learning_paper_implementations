{
 "<h1><a href=\"https://nn.labml.ai/transformers/gmlp/index.html\">Pay Attention to MLPs (gMLP)</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2105.08050\">Pay Attention to MLPs</a>.</p>\n<p>This paper introduces a Multilayer Perceptron (MLP) based architecture with gating, which they name <strong>gMLP</strong>. It consists of a stack of <span translate=no>_^_0_^_</span> <em>gMLP</em> blocks.</p>\n<p>Here is <a href=\"https://nn.labml.ai/transformers/gmlp/experiment.html\">the training code</a> for a gMLP model based autoregressive model. </p>\n": "<h1><a href=\"https://nn.labml.ai/transformers/gmlp/index.html\">MLP (GMLP) \u306b\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044</a></h1>\n<p>\u3053\u308c\u306f\u3001\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/2105.08050\">MLP\u306b\u6ce8\u610f\u3057\u3066</a>\u300d<a href=\"https://pytorch.org\">\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a>\u3002</p>\n<p><strong>\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001\u30b2\u30fc\u30c6\u30a3\u30f3\u30b0\u3092\u5099\u3048\u305f\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\uff08MLP\uff09\u30d9\u30fc\u30b9\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\uff08GMLP\u3068\u540d\u4ed8\u3051\u3089\u308c\u3066\u3044\u307e\u3059\uff09\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</strong><span translate=no>_^_0_^_</span><em>gMLP</em> \u30d6\u30ed\u30c3\u30af\u306e\u30b9\u30bf\u30c3\u30af\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059</p>\u3002\n<p><a href=\"https://nn.labml.ai/transformers/gmlp/experiment.html\">gMLP\u30e2\u30c7\u30eb\u30d9\u30fc\u30b9\u306e\u81ea\u5df1\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059</a>\u3002</p>\n",
 "Pay Attention to MLPs (gMLP)": "MLP (GMLP) \u306b\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044"
}