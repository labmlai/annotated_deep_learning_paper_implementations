{
 "<h1><a href=\"https://nn.labml.ai/transformers/gmlp/index.html\">Pay Attention to MLPs (gMLP)</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2105.08050\">Pay Attention to MLPs</a>.</p>\n<p>This paper introduces a Multilayer Perceptron (MLP) based architecture with gating, which they name <strong>gMLP</strong>. It consists of a stack of <span translate=no>_^_0_^_</span> <em>gMLP</em> blocks.</p>\n<p>Here is <a href=\"https://nn.labml.ai/transformers/gmlp/experiment.html\">the training code</a> for a gMLP model based autoregressive model. </p>\n": "<h1><a href=\"https://nn.labml.ai/transformers/gmlp/index.html\">\u6ce8\u610f MLP (GmLP)</a></h1>\n<p>\u8fd9\u662f P <a href=\"https://pytorch.org\">yTorch</a> \u5bf9\u300a<a href=\"https://arxiv.org/abs/2105.08050\">\u6ce8\u610f MLP\u300b\u4e00\u6587\u7684</a>\u5b9e\u73b0\u3002</p>\n<p>\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u7684\u5e26\u6709\u95e8\u63a7\u7684\u67b6\u6784\uff0c\u4ed6\u4eec\u5c06\u5176\u547d\u540d\u4e3a <strong>gmLP</strong>\u3002\u5b83\u7531\u4e00\u5806<span translate=no>_^_0_^_</span> <em>gmLP</em> \u5757\u7ec4\u6210\u3002</p>\n<p>\u8fd9\u662f\u57fa<a href=\"https://nn.labml.ai/transformers/gmlp/experiment.html\">\u4e8e GmLP \u6a21\u578b\u7684\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8bad\u7ec3\u4ee3\u7801</a>\u3002</p>\n",
 "Pay Attention to MLPs (gMLP)": "\u6ce8\u610f MLP (gMLP)"
}