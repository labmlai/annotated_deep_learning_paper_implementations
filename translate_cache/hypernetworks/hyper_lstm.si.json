{
 "<h1>HyperLSTM module</h1>\n": "<h1>\u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0d91\u0dbd\u0dca\u0d91\u0dc3\u0dca\u0da7\u0dd3\u0d91\u0db8\u0dca\u0db8\u0ddc\u0da9\u0dd2\u0dba\u0dd4\u0dbd\u0dba</h1>\n",
 "<h1>HyperNetworks - HyperLSTM</h1>\n<p>We have implemented HyperLSTM introduced in paper <a href=\"https://arxiv.org/abs/1609.09106\">HyperNetworks</a>, with annotations using <a href=\"https://pytorch.org\">PyTorch</a>. <a href=\"https://blog.otoro.net/2016/09/28/hyper-networks/\">This blog post</a> by David Ha gives a good explanation of HyperNetworks.</p>\n<p>We have an experiment that trains a HyperLSTM to predict text on Shakespeare dataset. Here&#x27;s the link to code: <a href=\"experiment.html\"><span translate=no>_^_0_^_</span></a></p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/hypernetworks/experiment.ipynb\"><span translate=no>_^_1_^_</span></a> <a href=\"https://app.labml.ai/run/9e7f39e047e811ebbaff2b26e3148b3d\"><span translate=no>_^_2_^_</span></a></p>\n<p>HyperNetworks use a smaller network to generate weights of a larger network. There are two variants: static hyper-networks and dynamic hyper-networks. Static HyperNetworks have smaller networks that generate weights (kernels) of a convolutional network. Dynamic HyperNetworks generate parameters of a recurrent neural network for each step. This is an implementation of the latter.</p>\n<h2>Dynamic HyperNetworks</h2>\n<p>In a RNN the parameters stay constant for each step. Dynamic HyperNetworks generate different parameters for each step. HyperLSTM has the structure of a LSTM but the parameters of each step are changed by a smaller LSTM network.</p>\n<p>In the basic form, a Dynamic HyperNetwork has a smaller recurrent network that generates a feature vector corresponding to each parameter tensor of the larger recurrent network. Let&#x27;s say the larger network has some parameter <span translate=no>_^_3_^_</span> the smaller network generates a feature vector <span translate=no>_^_4_^_</span> and we dynamically compute <span translate=no>_^_5_^_</span> as a linear transformation of <span translate=no>_^_6_^_</span>. For instance <span translate=no>_^_7_^_</span> where <span translate=no>_^_8_^_</span> is a 3-d tensor parameter and <span translate=no>_^_9_^_</span> is a tensor-vector multiplication. <span translate=no>_^_10_^_</span> is usually a linear transformation of the output of the smaller recurrent network.</p>\n<h3>Weight scaling instead of computing</h3>\n<p>Large recurrent networks have large dynamically computed parameters. These are calculated using linear transformation of feature vector <span translate=no>_^_11_^_</span>. And this transformation requires an even larger weight tensor. That is, when <span translate=no>_^_12_^_</span> has shape <span translate=no>_^_13_^_</span>, <span translate=no>_^_14_^_</span> will be <span translate=no>_^_15_^_</span>.</p>\n<p>To overcome this, we compute the weight parameters of the recurrent network by dynamically scaling each row of a matrix of same size.</p>\n<span translate=no>_^_16_^_</span><p>where <span translate=no>_^_17_^_</span> is a <span translate=no>_^_18_^_</span> parameter matrix.</p>\n<p>We can further optimize this when we compute <span translate=no>_^_19_^_</span>, as <span translate=no>_^_20_^_</span> where <span translate=no>_^_21_^_</span> stands for element-wise multiplication.</p>\n": "<h1>\u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0db1\u0dd9\u0da7\u0dca\u0dc0\u0dbb\u0dca\u0d9a\u0dca\u0dc3\u0dca- \u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0d91\u0dbd\u0dca\u0d91\u0dc3\u0dca\u0d91\u0db8\u0dca</h1>\n<p>\u0d85\u0db4\u0dd2 <a href=\"https://pytorch.org\">PyTorch</a>\u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0dc0\u0dd2\u0dc0\u0dbb\u0dab \u0dc3\u0db8\u0d9c, \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 <a href=\"https://arxiv.org/abs/1609.09106\">HyperNetworks</a>\u0dc4\u0db3\u0dd4\u0db1\u0dca\u0dc0\u0dcf HyperlSTM \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb \u0d87\u0dad. <a href=\"https://blog.otoro.net/2016/09/28/hyper-networks/\">\u0da9\u0dda\u0dc0\u0dd2\u0da9\u0dca \u0dc4\u0dcf\u0d9c\u0dda \u0db8\u0dd9\u0db8 \u0db6\u0dca\u0dbd\u0ddc\u0d9c\u0dca \u0dc3\u0da7\u0dc4\u0db1</a> \u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0db1\u0dd9\u0da7\u0dca\u0dc0\u0dbb\u0dca\u0d9a\u0dca\u0dc3\u0dca \u0db4\u0dd2\u0dc5\u0dd2\u0db6\u0db3 \u0dc4\u0ddc\u0db3 \u0db4\u0dd0\u0dc4\u0dd0\u0daf\u0dd2\u0dbd\u0dd2 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0d9a\u0dca \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0dba\u0dd2. </p>\n<p>\u0dc2\u0dda\u0d9a\u0dca\u0dc3\u0dca\u0db4\u0dd2\u0dba\u0dbb\u0dca\u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba \u0db4\u0dd2\u0dc5\u0dd2\u0db6\u0db3 \u0db4\u0dd9\u0dc5 \u0db4\u0dd4\u0dbb\u0ddd\u0d9a\u0dae\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0d91\u0dbd\u0dca\u0d91\u0dc3\u0dca\u0da7\u0dd3\u0d91\u0db8\u0dca \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dca\u0dc4\u0daf\u0dcf \u0db6\u0dd0\u0dbd\u0dd3\u0db8\u0d9a\u0dca \u0d85\u0db4\u0da7 \u0dad\u0dd2\u0db6\u0dda. \u0db8\u0dd9\u0db1\u0dca\u0db1 \u0d9a\u0dda\u0dad\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0db6\u0dd0\u0db3\u0dd2\u0dba: <a href=\"experiment.html\"><span translate=no>_^_0_^_</span></a></p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/hypernetworks/experiment.ipynb\"><span translate=no>_^_1_^_</span></a> <a href=\"https://app.labml.ai/run/9e7f39e047e811ebbaff2b26e3148b3d\"> <span translate=no>_^_2_^_</span></a></p>\n<p>\u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0db1\u0dd9\u0da7\u0dca\u0dc0\u0dbb\u0dca\u0d9a\u0dca\u0dc3\u0dca\u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0da2\u0dcf\u0dbd\u0dba\u0d9a \u0db6\u0dbb \u0d8b\u0dad\u0dca\u0db4\u0dcf\u0daf\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d9a\u0dd4\u0da9\u0dcf \u0da2\u0dcf\u0dbd\u0dba\u0d9a\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. \u0db4\u0dca\u0dbb\u0db7\u0dda\u0daf \u0daf\u0dd9\u0d9a\u0d9a\u0dca \u0dad\u0dd2\u0db6\u0dda: \u0dc3\u0dca\u0dae\u0dd2\u0dad\u0dd2\u0d9a \u0d85\u0db0\u0dd2-\u0da2\u0dcf\u0dbd \u0dc3\u0dc4 \u0d9c\u0dad\u0dd2\u0d9a \u0d85\u0db0\u0dd2-\u0da2\u0dcf\u0dbd. \u0dc3\u0dca\u0dae\u0dd2\u0dad\u0dd2\u0d9a \u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0db1\u0dd9\u0da7\u0dca\u0dc0\u0dbb\u0dca\u0d9a\u0dca\u0dc3\u0dca \u0dba\u0db1\u0dd4 \u0dc3\u0d82\u0dc0\u0dc4\u0db1 \u0da2\u0dcf\u0dbd\u0dba\u0d9a \u0db6\u0dbb (\u0d9a\u0dbb\u0dca\u0db1\u0dbd\u0dca) \u0da2\u0db1\u0db1\u0dba \u0d9a\u0dbb\u0db1 \u0d9a\u0dd4\u0da9\u0dcf \u0da2\u0dcf\u0dbd \u0d87\u0dad. \u0da9\u0dba\u0dd2\u0db1\u0db8\u0dd2\u0d9a\u0dca \u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0db1\u0dd9\u0da7\u0dca\u0dc0\u0dbb\u0dca\u0d9a\u0dca\u0dc3\u0dca \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db4\u0dd2\u0dba\u0dc0\u0dbb \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dd4\u0db1\u0dbb\u0dcf\u0dc0\u0dbb\u0dca\u0dad\u0db1 \u0dc3\u0dca\u0db1\u0dcf\u0dba\u0dd4\u0d9a \u0da2\u0dcf\u0dbd\u0dba\u0d9a \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0da2\u0db1\u0db1\u0dba \u0d9a\u0dbb\u0dba\u0dd2. \u0db8\u0dd9\u0dba \u0d85\u0dc0\u0dc3\u0dcf\u0db1 \u0dc0\u0dc1\u0dba\u0dd9\u0db1\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dba\u0dd2. </p>\n<h2>\u0d9c\u0dad\u0dd2\u0d9a\u0d85\u0db0\u0dd2-\u0da2\u0dcf\u0dbd</h2>\n<p>RNN\u0dc4\u0dd2 \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db4\u0dd2\u0dba\u0dc0\u0dbb \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0db1\u0dd2\u0dba\u0dad\u0dc0 \u0db4\u0dc0\u0dad\u0dd3. \u0d9c\u0dad\u0dd2\u0d9a \u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0db1\u0dd9\u0da7\u0dca\u0dc0\u0dbb\u0dca\u0d9a\u0dca\u0dc3\u0dca \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db4\u0dd2\u0dba\u0dc0\u0dbb \u0dc3\u0db3\u0dc4\u0dcf \u0dc0\u0dd2\u0dc0\u0dd2\u0db0 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0da2\u0db1\u0db1\u0dba \u0d9a\u0dbb\u0dba\u0dd2. \u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0d91\u0dbd\u0dca\u0d91\u0dc3\u0dca\u0da7\u0dd3\u0d91\u0db8\u0dca \u0dc4\u0dd2 LSTM \u0dc4\u0dd2 \u0dc0\u0dca\u0dba\u0dd4\u0dc4\u0dba \u0d87\u0dad\u0dd2 \u0db1\u0db8\u0dd4\u0dad\u0dca \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db4\u0dd2\u0dba\u0dc0\u0dbb\u0dda \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0d9a\u0dd4\u0da9\u0dcf LSTM \u0da2\u0dcf\u0dbd\u0dba\u0d9a\u0dd2\u0db1\u0dca \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0dc0\u0dda. </p>\n<p>\u0db8\u0dd6\u0dbd\u0dd2\u0d9a\u0dc3\u0dca\u0dc0\u0dbb\u0dd6\u0db4\u0dba\u0dd9\u0db1\u0dca, \u0da9\u0dba\u0dd2\u0db1\u0db8\u0dd2\u0d9a\u0dca \u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0db1\u0dd9\u0da7\u0dca\u0dc0\u0dbb\u0dca\u0d9a\u0dca \u0d9a\u0dd4\u0da9\u0dcf \u0db4\u0dd4\u0db1\u0dbb\u0dcf\u0dc0\u0dbb\u0dca\u0dad\u0db1 \u0da2\u0dcf\u0dbd\u0dba\u0d9a\u0dca \u0d87\u0dad\u0dd2 \u0d85\u0dad\u0dbb \u0d91\u0dba \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0db4\u0dd4\u0db1\u0dbb\u0dcf\u0dc0\u0dbb\u0dca\u0dad\u0db1 \u0da2\u0dcf\u0dbd\u0dba\u0dda \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2 \u0da7\u0dd9\u0db1\u0dca\u0dc3\u0dbb\u0dba\u0da7 \u0d85\u0db1\u0dd4\u0dbb\u0dd6\u0db4 \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0daf\u0ddb\u0dc1\u0dd2\u0d9a\u0dba\u0d9a\u0dca \u0da2\u0db1\u0db1\u0dba \u0d9a\u0dbb\u0dba\u0dd2. \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0da2\u0dcf\u0dbd\u0dba\u0da7 \u0dba\u0db8\u0dca \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba\u0d9a\u0dca <span translate=no>_^_3_^_</span> \u0d87\u0dad\u0dd0\u0dba\u0dd2 \u0d9a\u0dd2\u0dba\u0db8\u0dd4 \u0d9a\u0dd4\u0da9\u0dcf \u0da2\u0dcf\u0dbd\u0dba \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0daf\u0ddb\u0dc1\u0dd2\u0d9a\u0dba\u0d9a\u0dca \u0da2\u0db1\u0db1\u0dba \u0d9a\u0dbb\u0db1 <span translate=no>_^_4_^_</span> \u0d85\u0dad\u0dbb \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0d9a\u0dca <span translate=no>_^_5_^_</span> \u0dbd\u0dd9\u0dc3 \u0d9c\u0dad\u0dd2\u0d9a\u0dc0 \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db8\u0dd4 <span translate=no>_^_6_^_</span>. \u0d8b\u0daf\u0dcf\u0dc4\u0dbb\u0dab\u0dba\u0d9a\u0dca <span translate=no>_^_7_^_</span> \u0dbd\u0dd9\u0dc3 3-d \u0da7\u0dd9\u0db1\u0dca\u0dc3\u0dbb\u0dca \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba\u0d9a\u0dca <span translate=no>_^_9_^_</span> \u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0d91\u0dba \u0d86\u0dad\u0dad\u0dd2\u0dba-\u0daf\u0ddb\u0dc1\u0dd2\u0d9a \u0d9c\u0dd4\u0dab \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc0\u0dda. <span translate=no>_^_8_^_</span> <span translate=no>_^_10_^_</span> \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0dba\u0dd9\u0db1\u0dca \u0d9a\u0dd4\u0da9\u0dcf \u0db4\u0dd4\u0db1\u0dbb\u0dcf\u0dc0\u0dbb\u0dca\u0dad\u0db1 \u0da2\u0dcf\u0dbd\u0dba\u0dda \u0db1\u0dd2\u0db8\u0dd0\u0dc0\u0dd4\u0db8\u0dda \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0d9a\u0dd2. </p>\n<h3>\u0db4\u0dbb\u0dd2\u0d9c\u0dab\u0d9a\u0d9a\u0dbb\u0dab\u0dba\u0dc0\u0dd9\u0db1\u0dd4\u0dc0\u0da7 \u0db6\u0dbb \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba</h3>\n<p>\u0dc0\u0dd2\u0dc1\u0dcf\u0dbd\u0db4\u0dd4\u0db1\u0dbb\u0dcf\u0dc0\u0dbb\u0dca\u0dad\u0db1 \u0da2\u0dcf\u0dbd\u0dba\u0db1\u0dca \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0d9c\u0dad\u0dd2\u0d9a\u0dc0 \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0d87\u0dad. \u0db8\u0dd9\u0db8 \u0dbd\u0d9a\u0dca\u0dc2\u0dab\u0dba \u0daf\u0ddb\u0dc1\u0dd2\u0d9a \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb <span translate=no>_^_11_^_</span>\u0d87\u0dad. \u0db8\u0dd9\u0db8 \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0da7 \u0d8a\u0da7\u0dad\u0dca \u0dc0\u0da9\u0dcf \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0db6\u0dbb \u0d86\u0dad\u0dad\u0dba\u0d9a\u0dca \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0dc0\u0dda. \u0d91\u0db1\u0db8\u0dca, \u0dc4\u0dd0\u0da9\u0dba <span translate=no>_^_12_^_</span> \u0d87\u0dad\u0dd2 \u0dc0\u0dd2\u0da7 <span translate=no>_^_13_^_</span>, <span translate=no>_^_14_^_</span> \u0dc0\u0db1\u0dd4 \u0d87\u0dad <span translate=no>_^_15_^_</span>. </p>\n<p>\u0db8\u0dd9\u0dba\u0da2\u0dba \u0d9c\u0dd0\u0db1\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf, \u0d91\u0d9a\u0db8 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dda \u0d85\u0db1\u0dd4\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db4\u0dda\u0dc5\u0dd2\u0dba \u0d9c\u0dad\u0dd2\u0d9a\u0dc0 \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0db4\u0dd4\u0db1\u0dbb\u0dcf\u0dc0\u0dbb\u0dca\u0dad\u0db1 \u0da2\u0dcf\u0dbd\u0dba\u0dda \u0db6\u0dbb \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db8\u0dd4. </p>\n<span translate=no>_^_16_^_</span><p><span translate=no>_^_18_^_</span> \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2 \u0d85\u0db1\u0dd4\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0d9a\u0ddc\u0dc4\u0dd9\u0daf <span translate=no>_^_17_^_</span> . </p>\n<p>\u0db8\u0dd6\u0dbd\u0daf\u0dca\u0dbb\u0dc0\u0dca\u0dba-wise\u0dcf\u0db1\u0dc0\u0db1\u0dca\u0dad \u0d9c\u0dd4\u0dab \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0db4 \u0d9c\u0dab\u0db1\u0dba <span translate=no>_^_19_^_</span><span translate=no>_^_20_^_</span> \u0d9a\u0dbb\u0db1 <span translate=no>_^_21_^_</span> \u0dc0\u0dd2\u0da7 \u0d85\u0db4\u0da7 \u0db8\u0dd9\u0dba \u0dad\u0dc0\u0daf\u0dd4\u0dbb\u0da7\u0dad\u0dca \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2\u0dba. </p>\n",
 "<h2>HyperLSTM Cell</h2>\n<p>For HyperLSTM the smaller network and the larger network both have the LSTM structure. This is defined in Appendix A.2.2 in the paper.</p>\n": "<h2>\u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0d91\u0dbd\u0dca\u0d91\u0dc3\u0dca\u0da7\u0dd3\u0d91\u0db8\u0dca\u0dc3\u0ddb\u0dbd\u0dba</h2>\n<p>HyperLSTM\u0dc3\u0db3\u0dc4\u0dcf \u0d9a\u0dd4\u0da9\u0dcf \u0da2\u0dcf\u0dbd\u0dba \u0dc3\u0dc4 \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0da2\u0dcf\u0dbd\u0dba \u0daf\u0dd9\u0d9a\u0db8 LSTM \u0dc0\u0dca\u0dba\u0dd4\u0dc4\u0dba \u0d87\u0dad. \u0db8\u0dd9\u0dba \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dc0\u0dbd \u0d8b\u0db4\u0d9c\u0dca\u0dbb\u0db1\u0dca\u0dae\u0dba A.2.2 \u0dc4\u0dd2 \u0d85\u0dbb\u0dca\u0dae \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0d87\u0dad. </p>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p> <span translate=no>_^_0_^_</span> is the size of the input <span translate=no>_^_1_^_</span>, <span translate=no>_^_2_^_</span> is the size of the LSTM, and <span translate=no>_^_3_^_</span> is the size of the smaller LSTM that alters the weights of the larger outer LSTM. <span translate=no>_^_4_^_</span> is the size of the feature vectors used to alter the LSTM weights.</p>\n<p>We use the output of the smaller LSTM to compute <span translate=no>_^_5_^_</span>, <span translate=no>_^_6_^_</span> and <span translate=no>_^_7_^_</span> using linear transformations. We calculate <span translate=no>_^_8_^_</span>, <span translate=no>_^_9_^_</span>, and <span translate=no>_^_10_^_</span> from these, using linear transformations again. These are then used to scale the rows of weight and bias tensors of the main LSTM.</p>\n<p>\ud83d\udcdd Since the computation of <span translate=no>_^_11_^_</span> and <span translate=no>_^_12_^_</span> are two sequential linear transformations these can be combined into a single linear transformation. However we&#x27;ve implemented this separately so that it matches with the description in the paper.</p>\n": "<p> <span translate=no>_^_0_^_</span> \u0dba\u0db1\u0dd4 \u0d86\u0daf\u0dcf\u0db1\u0dba\u0dda \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba <span translate=no>_^_1_^_</span>, <span translate=no>_^_2_^_</span> LSTM \u0dc4\u0dd2 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba <span translate=no>_^_3_^_</span> \u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0d9a\u0dd4\u0da9\u0dcf LSTM \u0dc4\u0dd2 \u0db6\u0dbb \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0d9a\u0dbb\u0db1 \u0d9a\u0dd4\u0da9\u0dcf LSTM \u0dc0\u0dbd \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2 \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0db4\u0dd2\u0da7\u0dad LSTM. <span translate=no>_^_4_^_</span> \u0dba\u0db1\u0dd4 LSTM \u0db6\u0dbb \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1 \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0daf\u0ddb\u0dc1\u0dd2\u0d9a\u0dc0\u0dbd \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2. </p>\n<p>\u0d9c\u0dab\u0db1\u0dba <span translate=no>_^_5_^_</span>\u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0db4\u0dd2 \u0d9a\u0dd4\u0da9\u0dcf LSTM \u0dc4\u0dd2 \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba <span translate=no>_^_7_^_</span> \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1 <span translate=no>_^_6_^_</span> \u0d85\u0dad\u0dbb \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0db1\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd4. \u0d85\u0db4\u0dd2 \u0db1\u0dd0\u0dc0\u0dad \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0db1\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd2\u0db1\u0dca <span translate=no>_^_8_^_</span><span translate=no>_^_9_^_</span>, \u0dc3\u0dc4 <span translate=no>_^_10_^_</span> \u0db8\u0dda\u0dc0\u0dcf\u0dba\u0dd2\u0db1\u0dca \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db8\u0dd4. \u0db8\u0dda\u0dc0\u0dcf \u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1 LSTM \u0dc4\u0dd2 \u0db6\u0dbb \u0dc3\u0dc4 \u0db4\u0d9a\u0dca\u0dc2\u0d9c\u0dca\u0dbb\u0dcf\u0dc4\u0dd3 \u0d86\u0dad\u0dad\u0dd3\u0db1\u0dca \u0db4\u0dda\u0dc5\u0dd2 \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<p>\ud83d\udcdd\u0dc0\u0db1 \u0d9c\u0dab\u0db1\u0dba <span translate=no>_^_11_^_</span> \u0dc4\u0dcf \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dd2\u0d9a \u0dbb\u0dda\u0d9b\u0dd3\u0dba <span translate=no>_^_12_^_</span> \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0db1\u0dca \u0daf\u0dd9\u0d9a\u0d9a\u0dca \u0db8\u0dd9\u0db8 \u0dad\u0db1\u0dd2 \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0db6\u0dc0\u0da7 \u0d92\u0d9a\u0dcf\u0db6\u0daf\u0dca\u0db0 \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2 \u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca. \u0d9a\u0dd9\u0dc3\u0dda \u0dc0\u0dd9\u0dad\u0dad\u0dca \u0d85\u0db4\u0dd2 \u0db8\u0dd9\u0dba \u0dc0\u0dd9\u0db1 \u0dc0\u0dd9\u0db1\u0db8 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb \u0d87\u0dad\u0dd2 \u0d85\u0dad\u0dbb \u0d91\u0db8\u0d9f\u0dd2\u0db1\u0dca \u0d91\u0dba \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dc0\u0dbd \u0dc0\u0dd2\u0dc3\u0dca\u0dad\u0dbb\u0dba \u0dc3\u0db8\u0d9f \u0d9c\u0dd0\u0dbd\u0db4\u0dda. </p>\n",
 "<p> Create a network of <span translate=no>_^_0_^_</span> of HyperLSTM.</p>\n": "<p> HyperlSTM\u0da2\u0dcf\u0dbd\u0dba\u0d9a\u0dca \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1. <span translate=no>_^_0_^_</span> </p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span> </p>\n",
 "<p><span translate=no>_^_0_^_</span> \ud83e\udd14 In the paper it was specified as <span translate=no>_^_1_^_</span> I feel that it&#x27;s a typo. </p>\n": "<p><span translate=no>_^_0_^_</span> \ud83e\udd14 \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dad\u0dd4\u0dc5 \u0d91\u0dba \u0da7\u0dba\u0dd2\u0db4\u0ddd \u0db6\u0dc0 <span translate=no>_^_1_^_</span> \u0db8\u0da7 \u0dc4\u0dd0\u0d9f\u0dd9\u0db1 \u0db4\u0dbb\u0dd2\u0daf\u0dd2 \u0db1\u0dd2\u0dba\u0db8 \u0d9a\u0dbb \u0d87\u0dad. </p>\n",
 "<p>Collect the output <span translate=no>_^_0_^_</span> of the final layer </p>\n": "<p>\u0d85\u0dc0\u0dc3\u0dcf\u0db1 <span translate=no>_^_0_^_</span> \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Collect the outputs of the final layer at each step </p>\n": "<p>\u0d91\u0d9a\u0dca\u0d91\u0d9a\u0dca \u0db4\u0dd2\u0dba\u0dc0\u0dbb\u0dda\u0daf\u0dd3 \u0d85\u0dc0\u0dc3\u0dcf\u0db1 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba\u0db1\u0dca \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Create cells for each layer. Note that only the first layer gets the input directly. Rest of the layers get the input from the layer below </p>\n": "<p>\u0d91\u0d9a\u0dca\u0d91\u0d9a\u0dca \u0dc3\u0dca\u0dae\u0dbb\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0ddb\u0dbd \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1. \u0db4\u0dc5\u0db8\u0dd4 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0da7 \u0db4\u0db8\u0dab\u0d9a\u0dca \u0d86\u0daf\u0dcf\u0db1\u0dba \u0d9a\u0dd9\u0dbd\u0dd2\u0db1\u0dca\u0db8 \u0dbd\u0dd0\u0db6\u0dd9\u0db1 \u0db6\u0dc0 \u0dc3\u0dbd\u0d9a\u0db1\u0dca\u0db1. \u0dc3\u0dd9\u0dc3\u0dd4 \u0dc3\u0dca\u0dae\u0dbb \u0db4\u0dc4\u0dad \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dd9\u0db1\u0dca \u0d86\u0daf\u0dcf\u0db1\u0dba \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dd3 </p>\n",
 "<p>Get the state of the layer </p>\n": "<p>\u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda\u0dad\u0dad\u0dca\u0dc0\u0dba \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Initialize the state with zeros if <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0db1\u0db8\u0dca\u0dc1\u0dd4\u0db1\u0dca\u0dba \u0dc3\u0db8\u0d9f \u0dbb\u0dcf\u0da2\u0dca\u0dba\u0dba \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Input to the first layer is the input itself </p>\n": "<p>\u0db4\u0dc5\u0db8\u0dd4\u0dc3\u0dca\u0dae\u0dbb\u0dba\u0da7 \u0d86\u0daf\u0dcf\u0db1\u0dba \u0dba\u0db1\u0dd4 \u0d86\u0daf\u0dcf\u0db1\u0dba \u0db8 \u0dc0\u0dda </p>\n",
 "<p>Input to the next layer is the state of this layer </p>\n": "<p>\u0d8a\u0dc5\u0d9f\u0dc3\u0dca\u0dae\u0dbb\u0dba\u0da7 \u0d86\u0daf\u0dcf\u0db1\u0dba \u0db8\u0dd9\u0db8 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda \u0dad\u0dad\u0dca\u0dc0\u0dba\u0dba\u0dd2 </p>\n",
 "<p>Layer normalization </p>\n": "<p>\u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba </p>\n",
 "<p>Loop through the layers </p>\n": "<p>\u0dc3\u0dca\u0dae\u0dbb\u0dc4\u0dbb\u0dc4\u0dcf \u0dbd\u0dd6\u0db4\u0dca </p>\n",
 "<p>Reverse stack the tensors to get the states of each layer</p>\n<p>\ud83d\udcdd You can just work with the tensor itself but this is easier to debug </p>\n": "<p>\u0d91\u0d9a\u0dca\u0d91\u0d9a\u0dca \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda \u0dad\u0dad\u0dca\u0dc0\u0dba\u0db1\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0dd0\u0db1\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d86\u0dad\u0dad\u0dd3\u0db1\u0dca \u0d86\u0db4\u0dc3\u0dd4 \u0dc4\u0dbb\u0dc0\u0db1\u0dca\u0db1</p>\n<p>\ud83d\udcdd\u0d94\u0db6\u0da7 \u0da7\u0dd9\u0db1\u0dca\u0dc3\u0dbb\u0dca \u0dc3\u0db8\u0d9f \u0dc0\u0dd0\u0da9 \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2 \u0db1\u0db8\u0dd4\u0dad\u0dca \u0db8\u0dd9\u0dba \u0db1\u0dd2\u0daf\u0ddc\u0dc3\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0db4\u0dc4\u0dc3\u0dd4\u0dba </p>\n",
 "<p>Stack the outputs and states </p>\n": "<p>\u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba\u0db1\u0dca\u0dc3\u0dc4 \u0db4\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dad \u0d9c\u0ddc\u0da9\u0d9c\u0dc3\u0db1\u0dca\u0db1 </p>\n",
 "<p>Store sizes to initialize state </p>\n": "<p>\u0dad\u0dad\u0dca\u0dc0\u0dba\u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab \u0d9c\u0db6\u0da9\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>The input to the hyperLSTM is <span translate=no>_^_0_^_</span> where <span translate=no>_^_1_^_</span> is the input and <span translate=no>_^_2_^_</span> is the output of the outer LSTM at previous step. So the input size is <span translate=no>_^_3_^_</span>.</p>\n<p>The output of hyperLSTM is <span translate=no>_^_4_^_</span> and <span translate=no>_^_5_^_</span>. </p>\n": "<p>\u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0d91\u0dbd\u0dca\u0d91\u0dc3\u0dca\u0da7\u0dd3\u0d91\u0db8\u0dca\u0dc0\u0dd9\u0dad \u0d86\u0daf\u0dcf\u0db1\u0dba \u0dba\u0db1\u0dd4 \u0d86\u0daf\u0dcf\u0db1\u0dba <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> \u0d9a\u0ddc\u0dad\u0dd0\u0db1\u0daf \u0dba\u0db1\u0dca\u0db1 \u0dc3\u0dc4 <span translate=no>_^_2_^_</span> \u0db4\u0dd9\u0dbb \u0db4\u0dd2\u0dba\u0dc0\u0dbb\u0dda\u0daf\u0dd3 \u0db4\u0dd2\u0da7\u0dad LSTM \u0dc4\u0dd2 \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba \u0dc0\u0dda. \u0d91\u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca \u0d86\u0daf\u0dcf\u0db1 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba \u0dc0\u0dda <span translate=no>_^_3_^_</span>. </p>\n<p>HyperlSTM\u0dc4\u0dd2 \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba <span translate=no>_^_4_^_</span> \u0dc3\u0dc4 <span translate=no>_^_5_^_</span>. </p>\n",
 "<p>The weight matrices <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0db6\u0dbb\u0db8\u0dd0\u0da7\u0dca\u0da7\u0dca\u0dbb\u0dd2\u0dc3\u0dca <span translate=no>_^_0_^_</span> </p>\n",
 "<p>We calculate <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span>, <span translate=no>_^_2_^_</span> and <span translate=no>_^_3_^_</span> in a loop </p>\n": "<p>\u0d85\u0db4\u0dd2\u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db8\u0dd4 <span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>, <span translate=no>_^_2_^_</span> \u0dc3\u0dc4 \u0dbd\u0dd6\u0db4\u0dba\u0d9a\u0dca <span translate=no>_^_3_^_</span> \u0dad\u0dd4\u0dc5 </p>\n",
 "<span translate=no>_^_0_^_</span><p> </p>\n": "<span translate=no>_^_0_^_</span><p> </p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> has shape <span translate=no>_^_1_^_</span> and </li>\n<li><span translate=no>_^_2_^_</span> is a tuple of <span translate=no>_^_3_^_</span>.  <span translate=no>_^_4_^_</span> have shape <span translate=no>_^_5_^_</span> and  <span translate=no>_^_6_^_</span> have shape <span translate=no>_^_7_^_</span>.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0dc4\u0dd0\u0da9\u0dba <span translate=no>_^_1_^_</span> \u0dc3\u0dc4 </li>\n<li><span translate=no>_^_2_^_</span> \u0d9a tuple \u0dc0\u0dda <span translate=no>_^_3_^_</span>. <span translate=no>_^_4_^_</span> \u0dc4\u0dd0\u0da9\u0dba <span translate=no>_^_5_^_</span> \u0dc3\u0dc4 \u0dc4\u0dd0\u0da9\u0dba <span translate=no>_^_6_^_</span> \u0d87\u0dad\u0dd2 <span translate=no>_^_7_^_</span>. </li></ul>\n",
 "A PyTorch implementation/tutorial of HyperLSTM introduced in paper HyperNetworks.": "\u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0db1\u0dd9\u0da7\u0dca\u0dc0\u0dbb\u0dca\u0d9a\u0dca\u0dc3\u0dca \u0dc4\u0dd2 \u0dc4\u0db3\u0dd4\u0db1\u0dca\u0dc0\u0dcf \u0daf\u0dd3 \u0d87\u0dad\u0dd2 \u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0d91\u0dbd\u0dca\u0d91\u0dc3\u0dca\u0da7\u0dd3\u0d91\u0db8\u0dca \u0dc4\u0dd2 \u0db4\u0dba\u0dd2\u0da7\u0ddd\u0da0\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8/\u0db1\u0dd2\u0db6\u0db1\u0dca\u0db0\u0db1\u0dba.",
 "HyperNetworks - HyperLSTM": "\u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0db1\u0dd9\u0da7\u0dca\u0dc0\u0dbb\u0dca\u0d9a\u0dca\u0dc3\u0dca - \u0dc4\u0dba\u0dd2\u0db4\u0dbb\u0dca\u0d91\u0dbd\u0dca\u0d91\u0dc3\u0dca\u0d91\u0db8\u0dca"
}