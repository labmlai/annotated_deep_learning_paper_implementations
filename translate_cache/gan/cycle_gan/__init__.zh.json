{
 "<h1>Cycle GAN</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation/tutorial of the paper <a href=\"https://arxiv.org/abs/1703.10593\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a>.</p>\n<p>I&#x27;ve taken pieces of code from <a href=\"https://github.com/eriklindernoren/PyTorch-GAN\">eriklindernoren/PyTorch-GAN</a>. It is a very good resource if you want to checkout other GAN variations too.</p>\n<p>Cycle GAN does image-to-image translation. It trains a model to translate an image from given distribution to another, say, images of class A and B. Images of a certain distribution could be things like images of a certain style, or nature. The models do not need paired images between A and B. Just a set of images of each class is enough. This works very well on changing between image styles, lighting changes, pattern changes, etc. For example, changing summer to winter, painting style to photos, and horses to zebras.</p>\n<p>Cycle GAN trains two generator models and two discriminator models. One generator translates images from A to B and the other from B to A. The discriminators test whether the generated images look real.</p>\n<p>This file contains the model code as well as the training code. We also have a Google Colab notebook.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/gan/cycle_gan/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n": "<h1>\u5faa\u73af GAN</h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch \u7684 PyTorch</a> \u5b9e\u73b0/\u6559\u7a0b\uff0c\u8be5\u8bba\u6587<a href=\"https://arxiv.org/abs/1703.10593\">\u4f7f\u7528\u5468\u671f\u4e00\u81f4\u6027\u5bf9\u6297\u7f51\u7edc\u8fdb\u884c\u56fe\u50cf\u95f4\u7684\u975e\u914d\u5bf9\u8f6c\u6362</a>\u3002</p>\n<p>\u6211\u4ece <a href=\"https://github.com/eriklindernoren/PyTorch-GAN\">eriklindernoren/pytorch-Gan</a> \u90a3\u91cc\u62ff\u4e86\u4e00\u4e9b\u4ee3\u7801\u3002\u5982\u679c\u4f60\u4e5f\u60f3\u67e5\u770b\u5176\u4ed6 GAN \u53d8\u4f53\uff0c\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u8d44\u6e90\u3002</p>\nCyc@@ <p>le GAN \u8fdb\u884c\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u8f6c\u6362\u3002\u5b83\u8bad\u7ec3\u6a21\u578b\u5c06\u56fe\u50cf\u4ece\u7ed9\u5b9a\u5206\u5e03\u8f6c\u6362\u5230\u53e6\u4e00\u4e2a\u5206\u5e03\uff0c\u6bd4\u5982A\u7c7b\u548cB\u7c7b\u7684\u56fe\u50cf\uff0c\u67d0\u4e2a\u5206\u5e03\u7684\u56fe\u50cf\u53ef\u4ee5\u662f\u67d0\u79cd\u98ce\u683c\u6216\u81ea\u7136\u7684\u56fe\u50cf\u3002\u6a21\u578b\u4e0d\u9700\u8981 A \u548c B \u4e4b\u95f4\u7684\u914d\u5bf9\u56fe\u50cf\uff0c\u6bcf\u4e2a\u7c7b\u522b\u7684\u4e00\u7ec4\u56fe\u50cf\u5c31\u8db3\u591f\u4e86\u3002\u8fd9\u975e\u5e38\u9002\u5408\u5728\u56fe\u50cf\u98ce\u683c\u3001\u5149\u7167\u53d8\u5316\u3001\u56fe\u6848\u53d8\u5316\u7b49\u4e4b\u95f4\u8fdb\u884c\u5207\u6362\u3002\u4f8b\u5982\uff0c\u5c06\u590f\u5929\u6539\u4e3a\u51ac\u5929\uff0c\u5c06\u7ed8\u753b\u98ce\u683c\u6539\u4e3a\u7167\u7247\uff0c\u5c06\u9a6c\u6539\u4e3a\u6591\u9a6c\u3002</p>\n<p>Cycle GAN \u53ef\u8bad\u7ec3\u4e24\u4e2a\u53d1\u7535\u673a\u6a21\u578b\u548c\u4e24\u4e2a\u9274\u522b\u5668\u6a21\u578b\u3002\u4e00\u4e2a\u751f\u6210\u5668\u5c06\u56fe\u50cf\u4ece A \u8f6c\u6362\u5230 B\uff0c\u53e6\u4e00\u4e2a\u4ece B \u8f6c\u6362\u5230 A\u3002\u5224\u522b\u5668\u6d4b\u8bd5\u751f\u6210\u7684\u56fe\u50cf\u662f\u5426\u771f\u5b9e\u3002</p>\n<p>\u6b64\u6587\u4ef6\u5305\u542b\u6a21\u578b\u4ee3\u7801\u548c\u8bad\u7ec3\u4ee3\u7801\u3002\u6211\u4eec\u8fd8\u6709\u4e00\u53f0\u8c37\u6b4c Colab \u7b14\u8bb0\u672c\u7535\u8111\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/gan/cycle_gan/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n",
 "<h2>Configurations</h2>\n": "<h2>\u914d\u7f6e</h2>\n",
 "<h2>Evaluate trained Cycle GAN</h2>\n": "<h2>\u8bc4\u4f30\u8bad\u7ec3\u8fc7\u7684\u5faa\u73af GAN</h2>\n",
 "<h2>Initialize models and data loaders</h2>\n": "<h2>\u521d\u59cb\u5316\u6a21\u578b\u548c\u6570\u636e\u52a0\u8f7d\u5668</h2>\n",
 "<h2>Train Cycle GAN</h2>\n": "<h2>\u706b\u8f66\u5468\u671f GAN</h2>\n",
 "<h2>Training</h2>\n<p>We aim to solve: <span translate=no>_^_0_^_</span></p>\n<p>where, <span translate=no>_^_1_^_</span> translates images from <span translate=no>_^_2_^_</span>, <span translate=no>_^_3_^_</span> translates images from <span translate=no>_^_4_^_</span>, <span translate=no>_^_5_^_</span> tests if images are from <span translate=no>_^_6_^_</span> space, <span translate=no>_^_7_^_</span> tests if images are from <span translate=no>_^_8_^_</span> space, and</p>\n<span translate=no>_^_9_^_</span><p><span translate=no>_^_10_^_</span> is the generative adversarial loss from the original GAN paper.</p>\n<p><span translate=no>_^_11_^_</span> is the cyclic loss, where we try to get <span translate=no>_^_12_^_</span> to be similar to <span translate=no>_^_13_^_</span>, and <span translate=no>_^_14_^_</span> to be similar to <span translate=no>_^_15_^_</span>. Basically if the two generators (transformations) are applied in series it should give back the original image. This is the main contribution of this paper. It trains the generators to generate an image of the other distribution that is similar to the original image. Without this loss <span translate=no>_^_16_^_</span> could generate anything that&#x27;s from the distribution of <span translate=no>_^_17_^_</span>. Now it needs to generate something from the distribution of <span translate=no>_^_18_^_</span> but still has properties of <span translate=no>_^_19_^_</span>, so that <span translate=no>_^_20_^_</span> can re-generate something like <span translate=no>_^_21_^_</span>.</p>\n<p><span translate=no>_^_22_^_</span> is the identity loss. This was used to encourage the mapping to preserve color composition between the input and the output.</p>\n<p>To solve <span translate=no>_^_23_^_</span>, discriminators <span translate=no>_^_24_^_</span> and <span translate=no>_^_25_^_</span> should <strong>ascend</strong> on the gradient,</p>\n<span translate=no>_^_26_^_</span><p>That is descend on <em>negative</em> log-likelihood loss.</p>\n<p>In order to stabilize the training the negative log- likelihood objective was replaced by a least-squared loss - the least-squared error of discriminator, labelling real images with 1, and generated images with 0. So we want to descend on the gradient,</p>\n<span translate=no>_^_27_^_</span><p>We use least-squares for generators also. The generators should <em>descend</em> on the gradient,</p>\n<span translate=no>_^_28_^_</span><p>We use <span translate=no>_^_29_^_</span> for <span translate=no>_^_30_^_</span> and <span translate=no>_^_31_^_</span> for <span translate=no>_^_32_^_</span>. We use <span translate=no>_^_33_^_</span> for <span translate=no>_^_34_^_</span> and <span translate=no>_^_35_^_</span> for <span translate=no>_^_36_^_</span>.</p>\n": "<h2>\u8bad\u7ec3</h2>\n<p>\u6211\u4eec\u7684\u76ee\u6807\u662f\u89e3\u51b3\uff1a<span translate=no>_^_0_^_</span></p>\n<p>\u5176\u4e2d\uff0c<span translate=no>_^_1_^_</span>\u7ffb\u8bd1\u56fe\u50cf<span translate=no>_^_2_^_</span>\uff0c<span translate=no>_^_3_^_</span>\u7ffb\u8bd1\u6765\u81ea\u7684\u56fe\u50cf<span translate=no>_^_4_^_</span>\uff0c<span translate=no>_^_5_^_</span>\u6d4b\u8bd5\u56fe\u50cf\u662f\u5426\u6765\u81ea<span translate=no>_^_6_^_</span>\u592a\u7a7a\uff0c<span translate=no>_^_7_^_</span>\u6d4b\u8bd5\u56fe\u50cf\u662f\u5426\u6765\u81ea<span translate=no>_^_8_^_</span>\u592a\u7a7a\uff0c\u4ee5\u53ca</p>\n<span translate=no>_^_9_^_</span><p><span translate=no>_^_10_^_</span>\u662f\u539f\u59cb GAN \u8bba\u6587\u4ea7\u751f\u7684\u5bf9\u6297\u635f\u5931\u3002</p>\n<p><span translate=no>_^_11_^_</span>\u662f\u5faa\u73af\u635f\u5931\uff0c\u6211\u4eec<span translate=no>_^_12_^_</span>\u8bd5\u56fe<span translate=no>_^_13_^_</span>\u4e0e\u4e4b\u76f8\u4f3c\u548c<span translate=no>_^_14_^_</span>\u76f8\u4f3c<span translate=no>_^_15_^_</span>\u3002\u57fa\u672c\u4e0a\uff0c\u5982\u679c\u4e24\u4e2a\u751f\u6210\u5668\uff08\u53d8\u6362\uff09\u662f\u4e32\u8054\u5e94\u7528\u7684\uff0c\u5b83\u5e94\u8be5\u8fd4\u56de\u539f\u59cb\u56fe\u50cf\u3002\u8fd9\u662f\u672c\u6587\u7684\u4e3b\u8981\u8d21\u732e\u3002\u5b83\u8bad\u7ec3\u751f\u6210\u5668\u4ee5\u751f\u6210\u4e0e\u539f\u59cb\u56fe\u50cf\u76f8\u4f3c\u7684\u5176\u4ed6\u5206\u5e03\u7684\u56fe\u50cf\u3002\u5982\u679c\u6ca1\u6709\u8fd9\u79cd\u635f\u5931\uff0c<span translate=no>_^_16_^_</span>\u53ef\u80fd\u4f1a\u4ea7\u751f\u4efb\u4f55\u6765\u81ea\u5206\u53d1\u7684\u635f\u5931<span translate=no>_^_17_^_</span>\u3002\u73b0\u5728\u5b83\u9700\u8981\u4ece\u7684\u5206\u5e03\u4e2d\u751f\u6210\u4e00\u4e9b\u4e1c\u897f\uff0c<span translate=no>_^_18_^_</span>\u4f46\u4ecd\u7136\u5177\u6709\u7684\u5c5e\u6027<span translate=no>_^_19_^_</span>\uff0c\u8fd9\u6837<span translate=no>_^_20_^_</span>\u624d\u80fd\u91cd\u65b0\u751f\u6210\u7c7b\u4f3c\u7684\u4e1c\u897f<span translate=no>_^_21_^_</span>\u3002</p>\n<p><span translate=no>_^_22_^_</span>\u662f\u8eab\u4efd\u4e22\u5931\u3002\u8fd9\u88ab\u7528\u6765\u9f13\u52b1\u6620\u5c04\u4ee5\u4fdd\u7559\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u989c\u8272\u6784\u6210\u3002</p>\n<p>\u4e3a\u4e86\u6c42\u89e3<span translate=no>_^_23_^_</span>\uff0c\u9274\u522b<span translate=no>_^_24_^_</span>\u5668\u548c<span translate=no>_^_25_^_</span>\u5e94\u8be5\u5728\u68af\u5ea6\u4e0a<strong>\u5347</strong>\uff0c</p>\n<span translate=no>_^_26_^_</span><p>\u8fd9\u53d6\u51b3\u4e8e<em>\u8d1f</em>\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u3002</p>\n<p>\u4e3a\u4e86\u7a33\u5b9a\u8bad\u7ec3\uff0c\u8d1f\u5bf9\u6570\u4f3c\u7136\u76ee\u6807\u88ab\u6700\u5c0f\u4e8c\u4e58\u635f\u5931\u6240\u53d6\u4ee3\uff0c\u5373\u9274\u522b\u5668\u7684\u6700\u5c0f\u4e8c\u4e58\u8bef\u5dee\uff0c\u75281\u6807\u8bb0\u771f\u5b9e\u56fe\u50cf\uff0c\u5c06\u751f\u6210\u7684\u56fe\u50cf\u6807\u8bb0\u4e3a0\u3002\u6240\u4ee5\u6211\u4eec\u60f3\u5728\u6e10\u53d8\u4e0a\u4e0b\u964d\uff0c</p>\n<span translate=no>_^_27_^_</span><p>\u6211\u4eec\u4e5f\u4f7f\u7528\u6700\u5c0f\u4e8c\u4e58\u4f5c\u4e3a\u751f\u6210\u5668\u3002\u53d1\u7535\u673a\u5e94\u8be5<em>\u4e0b\u964d\u5230</em>\u68af\u5ea6\u4e0a\uff0c</p>\n<span translate=no>_^_28_^_</span><p>\u6211\u4eec\u4f7f\u7528 f<span translate=no>_^_29_^_</span> or<span translate=no>_^_30_^_</span> \u548c f<span translate=no>_^_31_^_</span> or<span translate=no>_^_32_^_</span>\u3002\u6211\u4eec\u4f7f\u7528 f<span translate=no>_^_33_^_</span> or<span translate=no>_^_34_^_</span> \u548c f<span translate=no>_^_35_^_</span> or<span translate=no>_^_36_^_</span>\u3002</p>\n",
 "<h3>Dataset to load images</h3>\n": "<h3>\u7528\u4e8e\u52a0\u8f7d\u56fe\u50cf\u7684\u6570\u636e\u96c6</h3>\n",
 "<h3>Optimize the discriminators with gan loss.</h3>\n": "<h3>\u5229\u7528 gan \u635f\u8017\u4f18\u5316\u9274\u522b\u5668\u3002</h3>\n",
 "<h3>Optimize the generators with identity, gan and cycle losses.</h3>\n": "\u5229\u7528@@ <h3>\u6807\u8bc6\u3001\u589e\u76ca\u548c\u5faa\u73af\u635f\u8017\u6765\u4f18\u5316\u53d1\u7535\u673a\u3002</h3>\n",
 "<h3>Plot an image with matplotlib</h3>\n": "<h3>\u4f7f\u7528 matplotlib \u7ed8\u5236\u56fe\u50cf</h3>\n",
 "<h3>Replay Buffer</h3>\n<p>Replay buffer is used to train the discriminator. Generated images are added to the replay buffer and sampled from it.</p>\n<p>The replay buffer returns the newly added image with a probability of <span translate=no>_^_0_^_</span>. Otherwise, it sends an older generated image and replaces the older image with the newly generated image.</p>\n<p>This is done to reduce model oscillation.</p>\n": "<h3>\u91cd\u64ad\u7f13\u51b2\u533a</h3>\n<p>\u91cd\u64ad\u7f13\u51b2\u533a\u7528\u4e8e\u8bad\u7ec3\u9274\u522b\u5668\u3002\u751f\u6210\u7684\u56fe\u50cf\u88ab\u6dfb\u52a0\u5230\u91cd\u653e\u7f13\u51b2\u533a\u5e76\u4ece\u4e2d\u53d6\u6837\u3002</p>\n<p>\u91cd\u653e\u7f13\u51b2\u533a\u8fd4\u56de\u65b0\u6dfb\u52a0\u7684\u56fe\u50cf\uff0c\u6982\u7387\u4e3a<span translate=no>_^_0_^_</span>\u3002\u5426\u5219\uff0c\u5b83\u4f1a\u53d1\u9001\u4e00\u4e2a\u8f83\u65e7\u7684\u751f\u6210\u7684\u56fe\u50cf\uff0c\u5e76\u7528\u65b0\u751f\u6210\u7684\u56fe\u50cf\u66ff\u6362\u65e7\u7684\u56fe\u50cf\u3002</p>\n<p>\u8fd9\u6837\u505a\u662f\u4e3a\u4e86\u51cf\u5c11\u6a21\u578b\u632f\u8361\u3002</p>\n",
 "<h4>Download dataset and extract data</h4>\n": "<h4>\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u63d0\u53d6\u6570\u636e</h4>\n",
 "<h4>Initialize the dataset</h4>\n<ul><li><span translate=no>_^_0_^_</span> is the name of the dataset </li>\n<li><span translate=no>_^_1_^_</span> is the set of image transforms </li>\n<li><span translate=no>_^_2_^_</span> is either <span translate=no>_^_3_^_</span> or <span translate=no>_^_4_^_</span></li></ul>\n": "<h4>\u521d\u59cb\u5316\u6570\u636e\u96c6</h4>\n<ul><li><span translate=no>_^_0_^_</span>\u662f\u6570\u636e\u96c6\u7684\u540d\u79f0</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u56fe\u50cf\u53d8\u6362\u7684\u96c6\u5408</li>\n<li><span translate=no>_^_2_^_</span>\u662f<span translate=no>_^_3_^_</span>\u6216<span translate=no>_^_4_^_</span></li></ul>\n",
 "<p> Change to training mode </p>\n": "<p>\u6539\u4e3a\u8bad\u7ec3\u6a21\u5f0f</p>\n",
 "<p> Initialize convolution layer weights to <span translate=no>_^_0_^_</span></p>\n": "<p>\u5c06\u5377\u79ef\u5c42\u6743\u91cd\u521d\u59cb\u5316\u4e3a<span translate=no>_^_0_^_</span></p>\n",
 "<p> Load an image and change to RGB if in grey-scale.</p>\n": "<p>\u52a0\u8f7d\u56fe\u50cf\u5e76\u66f4\u6539\u4e3a RGB\uff08\u5982\u679c\u4e3a\u7070\u5ea6\uff09\u3002</p>\n",
 "<p> The generator is a residual network.</p>\n": "<p>\u53d1\u7535\u673a\u662f\u4e00\u4e2a\u6b8b\u4f59\u7f51\u7edc\u3002</p>\n",
 "<p> This is the discriminator block module. It does a convolution, an optional normalization, and a leaky ReLU.</p>\n<p>It shrinks the height and width of the input feature map by half.</p>\n": "<p>\u8fd9\u662f\u9274\u522b\u5668\u5757\u6a21\u5757\u3002\u5b83\u6267\u884c\u5377\u79ef\u3001\u53ef\u9009\u5f52\u4e00\u5316\u548c\u6cc4\u6f0f\u7684 RelU\u3002</p>\n<p>\u5b83\u5c06\u8f93\u5165\u8981\u7d20\u5730\u56fe\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7f29\u5c0f\u4e00\u534a\u3002</p>\n",
 "<p> This is the discriminator.</p>\n": "<p>\u8fd9\u662f\u9274\u522b\u5668\u3002</p>\n",
 "<p> This is the residual block, with two convolution layers.</p>\n": "<p>\u8fd9\u662f\u6b8b\u5dee\u5757\uff0c\u6709\u4e24\u4e2a\u5377\u79ef\u5c42\u3002</p>\n",
 "<p> Train discriminators </p>\n": "<p>\u8bad\u7ec3\u9274\u522b\u5668</p>\n",
 "<p><span translate=no>_^_0_^_</span> will pick a GPU if available </p>\n": "<p><span translate=no>_^_0_^_</span>\u5982\u679c\u6709\u7684\u8bdd\uff0c\u4f1a\u9009\u62e9\u4e00\u4e2a GPU</p>\n",
 "<p>Add batch dimension and move to the device we use </p>\n": "<p>\u6dfb\u52a0\u6279\u91cf\u7ef4\u5ea6\u5e76\u79fb\u52a8\u5230\u6211\u4eec\u4f7f\u7528\u7684\u8bbe\u5907</p>\n",
 "<p>Add/retrieve an image </p>\n": "<p>\u6dfb\u52a0/\u68c0\u7d22\u56fe\u50cf</p>\n",
 "<p>Arrange images along x-axis </p>\n": "<p>\u6cbf x \u8f74\u6392\u5217\u56fe\u50cf</p>\n",
 "<p>Arrange images along y-axis </p>\n": "<p>\u6cbf y \u8f74\u6392\u5217\u56fe\u50cf</p>\n",
 "<p>Calculate configurations. It will calculate <span translate=no>_^_0_^_</span> and all other configs required by it. </p>\n": "<p>\u8ba1\u7b97\u914d\u7f6e\u3002\u5b83\u5c06\u8ba1\u7b97<span translate=no>_^_0_^_</span>\u548c\u5b83\u6240\u9700\u7684\u6240\u6709\u5176\u4ed6\u914d\u7f6e\u3002</p>\n",
 "<p>Calculate configurations. We specify the generators <span translate=no>_^_0_^_</span> so that it only loads those and their dependencies. Configs like <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> will be calculated, since these are required by <span translate=no>_^_3_^_</span> and <span translate=no>_^_4_^_</span>.</p>\n<p>If you want other parameters like <span translate=no>_^_5_^_</span> you should specify them here. If you specify nothing, all the configurations will be calculated, including data loaders. Calculation of configurations and their dependencies will happen when you call <span translate=no>_^_6_^_</span> </p>\n": "<p>\u8ba1\u7b97\u914d\u7f6e\u3002\u6211\u4eec\u6307\u5b9a\u751f\u6210\u5668\uff0c<span translate=no>_^_0_^_</span>\u4ee5\u4fbf\u5b83\u53ea\u52a0\u8f7d\u8fd9\u4e9b\u751f\u6210\u5668\u53ca\u5176\u4f9d\u8d56\u9879\u3002<span translate=no>_^_2_^_</span>\u5c06\u8ba1\u7b97<span translate=no>_^_1_^_</span>\u548c\u4e4b\u7c7b\u7684\u914d\u7f6e\uff0c\u56e0\u4e3a<span translate=no>_^_3_^_</span>\u548c\u9700\u8981\u8fd9\u4e9b\u914d\u7f6e<span translate=no>_^_4_^_</span>\u3002</p>\n<p>\u5982\u679c\u4f60\u60f3\u8981\u5176\u4ed6\u53c2\u6570\uff0c<span translate=no>_^_5_^_</span>\u4f60\u5e94\u8be5\u5728\u8fd9\u91cc\u6307\u5b9a\u5b83\u4eec\u3002\u5982\u679c\u672a\u6307\u5b9a\u4efb\u4f55\u5185\u5bb9\uff0c\u5219\u5c06\u8ba1\u7b97\u6240\u6709\u914d\u7f6e\uff0c\u5305\u62ec\u6570\u636e\u52a0\u8f7d\u5668\u3002\u8c03\u7528\u65f6\u5c06\u8ba1\u7b97\u914d\u7f6e\u53ca\u5176\u4f9d\u8d56\u5173\u7cfb<span translate=no>_^_6_^_</span></p>\n",
 "<p>Create a sequential module with the layers </p>\n": "<p>\u4f7f\u7528\u5c42\u521b\u5efa\u987a\u5e8f\u6a21\u5757</p>\n",
 "<p>Create an experiment </p>\n": "<p>\u521b\u5efa\u5b9e\u9a8c</p>\n",
 "<p>Create configs object </p>\n": "<p>\u521b\u5efa\u914d\u7f6e\u5bf9\u8c61</p>\n",
 "<p>Create configurations </p>\n": "<p>\u521b\u5efa\u914d\u7f6e</p>\n",
 "<p>Create experiment </p>\n": "<p>\u521b\u5efa\u5b9e\u9a8c</p>\n",
 "<p>Create the learning rate schedules. The learning rate stars flat until <span translate=no>_^_0_^_</span> epochs, and then linearly reduce to <span translate=no>_^_1_^_</span> at end of training. </p>\n": "<p>\u521b\u5efa\u5b66\u4e60\u901f\u7387\u8868\u3002\u5b66\u4e60\u7387\u4e00\u76f4\u4fdd\u6301\u4e0d\u53d8\uff0c\u76f4\u5230<span translate=no>_^_0_^_</span>\u5404\u4e2a\u65f6\u4ee3\uff0c\u7136\u540e\u5728\u8bad\u7ec3\u7ed3\u675f<span translate=no>_^_1_^_</span>\u65f6\u7ebf\u6027\u964d\u4f4e\u3002</p>\n",
 "<p>Create the models </p>\n": "<p>\u521b\u5efa\u6a21\u578b</p>\n",
 "<p>Create the optmizers </p>\n": "<p>\u521b\u5efa\u4f18\u5316\u5668</p>\n",
 "<p>Cycle loss <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5468\u671f\u635f\u5931<span translate=no>_^_0_^_</span></p>\n",
 "<p>Data loaders </p>\n": "<p>\u6570\u636e\u52a0\u8f7d\u5668</p>\n",
 "<p>Dataset path </p>\n": "<p>\u6570\u636e\u96c6\u8def\u5f84</p>\n",
 "<p>Display </p>\n": "<p>\u663e\u793a</p>\n",
 "<p>Display the generated image. </p>\n": "<p>\u663e\u793a\u751f\u6210\u7684\u56fe\u50cf\u3002</p>\n",
 "<p>Display the image </p>\n": "<p>\u663e\u793a\u56fe\u50cf</p>\n",
 "<p>Download destination </p>\n": "<p>\u4e0b\u8f7d\u76ee\u7684\u5730</p>\n",
 "<p>Download file (generally ~100MB) </p>\n": "<p>\u4e0b\u8f7d\u6587\u4ef6\uff08\u4e00\u822c\u7ea6\u4e3a 100MB\uff09</p>\n",
 "<p>Download folder </p>\n": "<p>\u4e0b\u8f7d\u6587\u4ef6\u5939</p>\n",
 "<p>Download if missing </p>\n": "<p>\u5982\u679c\u7f3a\u5c11\u5219\u4e0b\u8f7d</p>\n",
 "<p>Each of these blocks will shrink the height and width by a factor of 2 </p>\n": "<p>\u8fd9\u4e9b\u65b9\u5757\u4e2d\u7684\u6bcf\u4e00\u4e2a\u90fd\u4f1a\u5c06\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7f29\u5c0f 2 \u500d</p>\n",
 "<p>Evaluation mode </p>\n": "<p>\u8bc4\u4f30\u6a21\u5f0f</p>\n",
 "<p>Extract the archive </p>\n": "<p>\u89e3\u538b\u6863\u6848</p>\n",
 "<p>Finally we map the feature map to an RGB image </p>\n": "<p>\u6700\u540e\uff0c\u6211\u4eec\u5c06\u7279\u5f81\u56fe\u6620\u5c04\u5230 RGB \u56fe\u50cf</p>\n",
 "<p>GAN Loss</p>\n<span translate=no>_^_0_^_</span><p> </p>\n": "<p>GAN \u635f\u5931</p>\n<span translate=no>_^_0_^_</span><p></p>\n",
 "<p>GAN loss <span translate=no>_^_0_^_</span> </p>\n": "<p>GAN \u635f\u5931<span translate=no>_^_0_^_</span></p>\n",
 "<p>Generate images <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> </p>\n": "<p>\u751f\u6210\u56fe\u50cf<span translate=no>_^_0_^_</span>\u548c<span translate=no>_^_1_^_</span></p>\n",
 "<p>Generate samples from test set and save them </p>\n": "<p>\u4ece\u6d4b\u8bd5\u96c6\u751f\u6210\u6837\u672c\u5e76\u4fdd\u5b58</p>\n",
 "<p>Get an image from dataset </p>\n": "<p>\u4ece\u6570\u636e\u96c6\u4e2d\u83b7\u53d6\u56fe\u50cf</p>\n",
 "<p>Get image paths </p>\n": "<p>\u83b7\u53d6\u56fe\u50cf\u8def\u5f84</p>\n",
 "<p>Get min and max values of the image for normalization </p>\n": "<p>\u83b7\u53d6\u56fe\u50cf\u7684\u6700\u5c0f\u503c\u548c\u6700\u5927\u503c\u4ee5\u8fdb\u884c\u5f52\u4e00\u5316</p>\n",
 "<p>Hyper-parameters </p>\n": "<p>\u8d85\u53c2\u6570</p>\n",
 "<p>Identity loss <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8eab\u4efd\u4e22\u5931<span translate=no>_^_0_^_</span></p>\n",
 "<p>Image dimensions </p>\n": "<p>\u56fe\u50cf\u5c3a\u5bf8</p>\n",
 "<p>Image transformations </p>\n": "<p>\u56fe\u50cf\u53d8\u6362</p>\n",
 "<p>Image transforms </p>\n": "<p>\u56fe\u50cf\u53d8\u6362</p>\n",
 "<p>Initialize weights to <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5c06\u6743\u91cd\u521d\u59cb\u5316\u4e3a<span translate=no>_^_0_^_</span></p>\n",
 "<p>L1 loss is used for cycle loss and identity loss </p>\n": "<p>L1 \u635f\u5931\u7528\u4e8e\u5468\u671f\u635f\u5931\u548c\u8eab\u4efd\u4e22\u5931</p>\n",
 "<p>Learning rate schedules </p>\n": "<p>\u5b66\u4e60\u901f\u7387\u8868</p>\n",
 "<p>Load hyper parameters set for training </p>\n": "<p>\u52a0\u8f7d\u4e3a\u8bad\u7ec3\u8bbe\u7f6e\u7684\u8d85\u7ea7\u53c2\u6570</p>\n",
 "<p>Load your own data. Here we try the test set. I was trying with Yosemite photos, they look awesome. You can use <span translate=no>_^_0_^_</span>, if you specified <span translate=no>_^_1_^_</span> as something you wanted to be calculated in the call to <span translate=no>_^_2_^_</span> </p>\n": "<p>\u52a0\u8f7d\u60a8\u81ea\u5df1\u7684\u6570\u636e\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5c1d\u8bd5\u6d4b\u8bd5\u96c6\u3002\u6211\u5728\u5c1d\u8bd5\u4f18\u80dc\u7f8e\u5730\u7684\u7167\u7247\uff0c\u5b83\u4eec\u770b\u8d77\u6765\u5f88\u68d2\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528<span translate=no>_^_0_^_</span>\uff0c\u5982\u679c\u4f60\u6307\u5b9a<span translate=no>_^_1_^_</span>\u4e3a\u4f60\u60f3\u8981\u5728\u8c03\u7528\u4e2d\u8ba1\u7b97\u7684\u4e1c\u897f<span translate=no>_^_2_^_</span></p>\n",
 "<p>Log losses </p>\n": "<p>\u5bf9\u6570\u635f\u5931</p>\n",
 "<p>Loop through epochs </p>\n": "<p>\u5faa\u73af\u7a7f\u8d8a\u65f6\u4ee3</p>\n",
 "<p>Loop through the dataset </p>\n": "<p>\u5faa\u73af\u6d4f\u89c8\u6570\u636e\u96c6</p>\n",
 "<p>Loss coefficients </p>\n": "<p>\u635f\u5931\u7cfb\u6570</p>\n",
 "<p>Models </p>\n": "<p>\u6a21\u7279</p>\n",
 "<p>Move images to the device </p>\n": "<p>\u5c06\u56fe\u50cf\u79fb\u52a8\u5230\u8bbe\u5907</p>\n",
 "<p>Move tensor to CPU </p>\n": "<p>\u5c06\u5f20\u91cf\u79fb\u5230 CPU</p>\n",
 "<p>New line </p>\n": "<p>\u65b0\u4ea7\u54c1\u7ebf</p>\n",
 "<p>Number of images in the dataset </p>\n": "<p>\u6570\u636e\u96c6\u4e2d\u7684\u5f71\u50cf\u6570\u91cf</p>\n",
 "<p>Number of residual blocks in the generator </p>\n": "<p>\u751f\u6210\u5668\u4e2d\u7684\u6b8b\u4f59\u5757\u6570</p>\n",
 "<p>Optimizers </p>\n": "<p>\u4f18\u5316\u5668</p>\n",
 "<p>Output of the discriminator is also a map of probabilities, whether each region of the image is real or generated </p>\n": "<p>\u5224\u522b\u5668\u7684\u8f93\u51fa\u4e5f\u662f\u6982\u7387\u56fe\uff0c\u65e0\u8bba\u56fe\u50cf\u7684\u6bcf\u4e2a\u533a\u57df\u662f\u771f\u5b9e\u7684\u8fd8\u662f\u751f\u6210\u7684</p>\n",
 "<p>Register models for saving and loading. <span translate=no>_^_0_^_</span> gives a dictionary of <span translate=no>_^_1_^_</span> in <span translate=no>_^_2_^_</span>. You can also specify a custom dictionary of models. </p>\n": "<p>\u6ce8\u518c\u6a21\u578b\u4ee5\u8fdb\u884c\u4fdd\u5b58\u548c\u52a0\u8f7d\u3002<span translate=no>_^_0_^_</span>\u7ed9\u51fa\u4e86<span translate=no>_^_1_^_</span> in \u7684\u5b57\u5178<span translate=no>_^_2_^_</span>\u3002\u60a8\u8fd8\u53ef\u4ee5\u6307\u5b9a\u6a21\u578b\u7684\u81ea\u5b9a\u4e49\u5b57\u5178\u3002</p>\n",
 "<p>Replay buffers to keep generated samples </p>\n": "<p>\u91cd\u64ad\u7f13\u51b2\u533a\u4ee5\u4fdd\u7559\u751f\u6210\u7684\u6837\u672c</p>\n",
 "<p>Return a pair of images. These pairs get batched together, and they do not act like pairs in training. So it is kind of ok that we always keep giving the same pair. </p>\n": "<p>\u8fd4\u56de\u4e00\u5bf9\u56fe\u50cf\u3002\u8fd9\u4e9b\u5bf9\u88ab\u5206\u6210\u4e00\u7ec4\uff0c\u5728\u8bad\u7ec3\u4e2d\u5b83\u4eec\u4e0d\u50cf\u6210\u5bf9\u90a3\u6837\u8d77\u4f5c\u7528\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u603b\u662f\u7ee7\u7eed\u7ed9\u540c\u6837\u7684\u8d27\u5e01\u5bf9\u662f\u53ef\u4ee5\u7684\u3002</p>\n",
 "<p>Return generated images </p>\n": "<p>\u8fd4\u56de\u751f\u6210\u7684\u56fe\u50cf</p>\n",
 "<p>Run the training </p>\n": "<p>\u8fd0\u884c\u8bad\u7ec3</p>\n",
 "<p>Sample images </p>\n": "<p>\u6837\u672c\u56fe\u7247</p>\n",
 "<p>Save images at intervals </p>\n": "<p>\u6bcf\u9694\u4e00\u6bb5\u65f6\u95f4\u4fdd\u5b58\u56fe\u50cf</p>\n",
 "<p>Save models when sampling images </p>\n": "<p>\u91c7\u6837\u56fe\u50cf\u65f6\u4fdd\u5b58\u6a21\u578b</p>\n",
 "<p>Save training statistics and increment the global step counter </p>\n": "<p>\u4fdd\u5b58\u8bad\u7ec3\u7edf\u8ba1\u6570\u636e\u5e76\u589e\u52a0\u5168\u5c40\u6b65\u6570\u8ba1\u6570\u5668</p>\n",
 "<p>Scale image values to be <a href=\"0...1\">0...1</a> </p>\n": "<p>\u5c06\u56fe\u50cf\u503c\u7f29\u653e\u4e3a <a href=\"0...1\">0... 1</a></p>\n",
 "<p>Set the run UUID from the training run </p>\n": "<p>\u8bbe\u7f6e\u8bad\u7ec3\u8dd1\u7684\u8dd1\u6b65 UUID</p>\n",
 "<p>Show Image </p>\n": "<p>\u663e\u793a\u56fe\u7247</p>\n",
 "<p>Show samples </p>\n": "<p>\u663e\u793a\u6837\u672c</p>\n",
 "<p>Specify which run to load from. Loading will actually happen when you call <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6307\u5b9a\u8981\u4ece\u54ea\u4e2a\u8fd0\u884c\u4e2d\u52a0\u8f7d\u3002\u5f53\u4f60\u6253\u7535\u8bdd\u65f6\uff0c\u52a0\u8f7d\u5b9e\u9645\u4e0a\u4f1a\u53d1\u751f<span translate=no>_^_0_^_</span></p>\n",
 "<p>Start and watch the experiment </p>\n": "<p>\u5f00\u59cb\u89c2\u770b\u5b9e\u9a8c</p>\n",
 "<p>Start the experiment </p>\n": "<p>\u5f00\u59cb\u5b9e\u9a8c</p>\n",
 "<p>Take a step in the optimizer </p>\n": "<p>\u5728\u4f18\u5316\u5668\u4e2d\u8fc8\u51fa\u4e00\u6b65</p>\n",
 "<p>The paper suggests using a least-squares loss instead of negative log-likelihood, at it is found to be more stable. </p>\n": "<p>\u8be5\u8bba\u6587\u5efa\u8bae\u4f7f\u7528\u6700\u5c0f\u4e8c\u4e58\u635f\u5931\u800c\u4e0d\u662f\u8d1f\u5bf9\u6570\u4f3c\u7136\uff0c\u56e0\u4e3a\u4eba\u4eec\u53d1\u73b0\u5b83\u66f4\u7a33\u5b9a\u3002</p>\n",
 "<p>Then the resulting feature map is up-sampled to match the original image height and width. </p>\n": "<p>\u7136\u540e\u5bf9\u751f\u6210\u7684\u8981\u7d20\u5730\u56fe\u8fdb\u884c\u4e0a\u91c7\u6837\uff0c\u4ee5\u5339\u914d\u539f\u59cb\u56fe\u50cf\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002</p>\n",
 "<p>This first block runs a <span translate=no>_^_0_^_</span> convolution and maps the image to a feature map. The output feature map has the same height and width because we have a padding of <span translate=no>_^_1_^_</span>. Reflection padding is used because it gives better image quality at edges.</p>\n<p><span translate=no>_^_2_^_</span> in <span translate=no>_^_3_^_</span> saves a little bit of memory. </p>\n": "<p>\u7b2c\u4e00\u4e2a\u5757\u8fd0\u884c<span translate=no>_^_0_^_</span>\u5377\u79ef\u5e76\u5c06\u56fe\u50cf\u6620\u5c04\u5230\u8981\u7d20\u5730\u56fe\u3002\u8f93\u51fa\u8981\u7d20\u5730\u56fe\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u76f8\u540c\uff0c\u56e0\u4e3a\u6211\u4eec\u7684\u5185\u8fb9\u8ddd\u4e3a<span translate=no>_^_1_^_</span>\u3002\u4f7f\u7528\u53cd\u5c04\u586b\u5145\u662f\u56e0\u4e3a\u5b83\u53ef\u4ee5\u5728\u8fb9\u7f18\u5904\u63d0\u4f9b\u66f4\u597d\u7684\u56fe\u50cf\u8d28\u91cf\u3002</p>\n<p><span translate=no>_^_2_^_</span>in<span translate=no>_^_3_^_</span> \u53ef\u4ee5\u8282\u7701\u4e00\u70b9\u5185\u5b58\u3002</p>\n",
 "<p>Total loss </p>\n": "<p>\u603b\u4e8f\u635f</p>\n",
 "<p>Train the generators. This returns the generated images. </p>\n": "<p>\u8bad\u7ec3\u53d1\u7535\u673a\u3002\u8fd9\u5c06\u8fd4\u56de\u751f\u6210\u7684\u56fe\u50cf\u3002</p>\n",
 "<p>Training data loader </p>\n": "<p>\u8bad\u7ec3\u6570\u636e\u52a0\u8f7d\u5668</p>\n",
 "<p>URL </p>\n": "<p>\u7f51\u5740</p>\n",
 "<p>Update learning rates </p>\n": "<p>\u66f4\u65b0\u5b66\u4e60\u7387</p>\n",
 "<p>Validation data loader </p>\n": "<p>\u9a8c\u8bc1\u6570\u636e\u52a0\u8f7d\u5668</p>\n",
 "<p>We don&#x27;t need axes </p>\n": "<p>\u6211\u4eec\u4e0d\u9700\u8981\u65a7\u5934</p>\n",
 "<p>We don&#x27;t need gradients </p>\n": "<p>\u6211\u4eec\u4e0d\u9700\u8981\u6e10\u53d8</p>\n",
 "<p>We down-sample with two <span translate=no>_^_0_^_</span> convolutions with stride of 2 </p>\n": "<p>\u6211\u4eec\u4f7f\u7528\u6b65\u5e45\u4e3a 2 \u7684\u4e24\u4e2a<span translate=no>_^_0_^_</span>\u5377\u79ef\u8fdb\u884c\u5411\u4e0b\u91c7\u6837</p>\n",
 "<p>We have to change the order of dimensions to HWC. </p>\n": "<p>\u6211\u4eec\u5fc5\u987b\u5c06\u5c3a\u5bf8\u987a\u5e8f\u66f4\u6539\u4e3a HWC\u3002</p>\n",
 "<p>We take this through <span translate=no>_^_0_^_</span>. This module is defined below. </p>\n": "<p>\u6211\u4eec\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898<span translate=no>_^_0_^_</span>\u3002\u6b64\u6a21\u5757\u5b9a\u4e49\u5982\u4e0b\u3002</p>\n",
 "<p>Zero pad on top and left to keep the output height and width same with the <span translate=no>_^_0_^_</span> kernel </p>\n": "<p>\u9876\u90e8\u548c\u5de6\u4fa7\u7684\u96f6\u586b\u5145\u4ee5\u4fdd\u6301\u8f93\u51fa\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u4e0e<span translate=no>_^_0_^_</span>\u5185\u6838\u76f8\u540c</p>\n",
 "<p>evaluate() </p>\n": "<p>\u8bc4\u4f30 ()</p>\n",
 "<p>false labels equal to <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5047\u6807\u7b7e\u7b49\u4e8e<span translate=no>_^_0_^_</span></p>\n",
 "<p>true labels equal to <span translate=no>_^_0_^_</span> </p>\n": "<p>\u771f\u5b9e\u6807\u7b7e\u7b49\u4e8e<span translate=no>_^_0_^_</span></p>\n",
 "A simple PyTorch implementation/tutorial of Cycle GAN introduced in paper Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.": "\u4e00\u7bc7\u7b80\u5355\u7684 PyTorch Cycle GAN \u5b9e\u73b0/\u6559\u7a0b\u5728\u8bba\u6587\u4e2d\u4ecb\u7ecd\u4e86 Cycle GAN \u4f7f\u7528\u5468\u671f\u4e00\u81f4\u7684\u5bf9\u6297\u7f51\u7edc\u8fdb\u884c\u672a\u914d\u5bf9\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u3002",
 "Cycle GAN": "\u5faa\u73af\u589e\u76ca"
}