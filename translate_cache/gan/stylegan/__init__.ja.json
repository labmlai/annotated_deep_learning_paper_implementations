{
 "<h1>StyleGAN 2</h1>\n": "<h1>\u30b9\u30bf\u30a4\u30eb\u30ac\u30f3 2</h1>\n",
 "<h2>Generative Adversarial Networks</h2>\n": "<h2>\u30b8\u30a7\u30cd\u30ec\u30fc\u30c6\u30a3\u30d6\u30fb\u30a2\u30c9\u30d0\u30b5\u30ea\u30a2\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af</h2>\n",
 "<h2>Progressive GAN</h2>\n": "<h2>\u30d7\u30ed\u30c3\u30b7\u30d6 GAN</h2>\n",
 "<h2>StyleGAN 2</h2>\n": "<h2>\u30b9\u30bf\u30a4\u30eb\u30ac\u30f3 2</h2>\n",
 "<h2>StyleGAN</h2>\n": "<h2>\u30b9\u30bf\u30a4\u30eb\u30ac\u30f3</h2>\n",
 "<h3>Convolution with Weight Modulation and Demodulation</h3>\n<p>This layer scales the convolution weights by the style vector and demodulates by normalizing it.</p>\n": "<h3>\u91cd\u307f\u5909\u8abf\u3068\u5fa9\u8abf\u306b\u3088\u308b\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3</h3>\n<p>\u3053\u306e\u5c64\u306f\u3001\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u306e\u91cd\u307f\u3092\u30b9\u30bf\u30a4\u30eb\u30d9\u30af\u30c8\u30eb\u3067\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3057\u3001\u6b63\u898f\u5316\u3057\u3066\u5fa9\u8abf\u3057\u307e\u3059\u3002</p>\n",
 "<h4>AdaIN</h4>\n": "<h4>\u30a2\u30c0\u30f3</h4>\n",
 "<h4>Bilinear Up and Down Sampling</h4>\n": "<h4>\u30d0\u30a4\u30ea\u30cb\u30a2\u30a2\u30c3\u30d7/\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0</h4>\n",
 "<h4>Mapping Network</h4>\n": "<h4>\u30de\u30c3\u30d4\u30f3\u30b0\u30cd\u30c3\u30c8\u30ef\u30fc\u30af</h4>\n",
 "<h4>No Progressive Growing</h4>\n": "<h4>\u30d7\u30ed\u30b0\u30ec\u30c3\u30b7\u30d6\u6210\u9577\u306a\u3057</h4>\n",
 "<h4>Path Length Regularization</h4>\n": "<h4>\u7d4c\u8def\u9577\u306e\u6b63\u5247\u5316</h4>\n",
 "<h4>Stochastic Variation</h4>\n": "<h4>\u78ba\u7387\u7684\u5909\u52d5</h4>\n",
 "<h4>Style Mixing</h4>\n": "<h4>\u30b9\u30bf\u30a4\u30eb\u30df\u30ad\u30b7\u30f3\u30b0</h4>\n",
 "<h4>Weight Modulation and Demodulation</h4>\n": "<h4>\u91cd\u307f\u5909\u8abf\u3068\u5fa9\u8abf</h4>\n",
 "<p> <a id=\"discriminator\"></a></p>\n<h2>StyleGAN 2 Discriminator</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p>Discriminator first transforms the image to a feature map of the same resolution and then runs it through a series of blocks with residual connections. The resolution is down-sampled by <span translate=no>_^_1_^_</span> at each block while doubling the number of features.</p>\n": "<p><a id=\"discriminator\"></a></p>\n<h2>\u30b9\u30bf\u30a4\u30eb\u30ac\u30f3 2 \u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p>\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u306f\u3001\u307e\u305a\u753b\u50cf\u3092\u540c\u3058\u89e3\u50cf\u5ea6\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306b\u5909\u63db\u3057\u3066\u304b\u3089\u3001\u6b8b\u7559\u63a5\u7d9a\u306e\u3042\u308b\u4e00\u9023\u306e\u30d6\u30ed\u30c3\u30af\u3092\u51e6\u7406\u3057\u307e\u3059\u3002\u89e3\u50cf\u5ea6\u306f\u30d6\u30ed\u30c3\u30af\u3054\u3068\u306b\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3055\u308c\u3001<span translate=no>_^_1_^_</span>\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u306f 2 \u500d\u306b\u306a\u308a\u307e\u3059\u3002</p>\n",
 "<p> <a id=\"discriminator_black\"></a></p>\n<h3>Discriminator Block</h3>\n<p><span translate=no>_^_0_^_</span></p>\n<p>Discriminator block consists of two <span translate=no>_^_1_^_</span> convolutions with a residual connection.</p>\n": "<p><a id=\"discriminator_black\"></a></p>\n<h3>\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u30d6\u30ed\u30c3\u30af</h3>\n<p><span translate=no>_^_0_^_</span></p>\n<p>\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u30d6\u30ed\u30c3\u30af\u306f\u3001\u6b8b\u5dee\u7d50\u5408\u3092\u3082\u3064 2 <span translate=no>_^_1_^_</span> \u3064\u306e\u7573\u307f\u8fbc\u307f\u3067\u69cb\u6210\u3055\u308c\u307e\u3059\u3002</p>\n",
 "<p> <a id=\"down_sample\"></a></p>\n<h3>Down-sample</h3>\n<p>The down-sample operation <a href=\"#smooth\">smoothens</a> each feature channel and  scale <span translate=no>_^_0_^_</span> using bilinear interpolation. This is based on the paper  <a href=\"https://arxiv.org/abs/1904.11486\">Making Convolutional Networks Shift-Invariant Again</a>.</p>\n": "<p><a id=\"down_sample\"></a></p>\n<h3>\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30eb</h3>\n<p>\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30eb\u64cd\u4f5c\u3067\u306f\u3001<a href=\"#smooth\"><span translate=no>_^_0_^_</span>\u30d0\u30a4\u30ea\u30cb\u30a2\u88dc\u9593\u3092\u4f7f\u7528\u3057\u3066\u5404\u30d5\u30a3\u30fc\u30c1\u30e3\u30c1\u30e3\u30cd\u30eb\u3068\u30b9\u30b1\u30fc\u30eb\u304c\u6ed1\u3089\u304b\u306b\u306a\u308a\u307e\u3059</a>\u3002\u3053\u308c\u306f\u3001\u300c<a href=\"https://arxiv.org/abs/1904.11486\">\u7573\u307f\u8fbc\u307f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u518d\u3073\u30b7\u30d5\u30c8\u4e0d\u5909\u306b\u3059\u308b</a>\u300d\u3068\u3044\u3046\u8ad6\u6587\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059</p>\u3002\n",
 "<p> <a id=\"equalized_conv2d\"></a></p>\n<h2>Learning-rate Equalized 2D Convolution Layer</h2>\n<p>This uses <a href=\"#equalized_weights\">learning-rate equalized weights</a> for a convolution layer.</p>\n": "<p><a id=\"equalized_conv2d\"></a></p>\n<h2>\u5b66\u7fd2\u7387\u5747\u7b49\u53162D\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc</h2>\n<p>\u3053\u308c\u306f\u3001<a href=\"#equalized_weights\">\u7573\u307f\u8fbc\u307f\u5c64\u306b\u5b66\u7fd2\u7387\u304c\u5747\u7b49\u5316\u3055\u308c\u305f\u91cd\u307f\u3092\u4f7f\u7528\u3057\u307e\u3059</a>\u3002</p>\n",
 "<p> <a id=\"equalized_linear\"></a></p>\n<h2>Learning-rate Equalized Linear Layer</h2>\n<p>This uses <a href=\"#equalized_weights\">learning-rate equalized weights</a> for a linear layer.</p>\n": "<p><a id=\"equalized_linear\"></a></p>\n<h2>\u5b66\u7fd2\u7387\u5747\u7b49\u5316\u7dda\u5f62\u5c64</h2>\n<p>\u3053\u308c\u306f\u3001<a href=\"#equalized_weights\">\u7dda\u5f62\u30ec\u30a4\u30e4\u30fc\u306e\u5b66\u7fd2\u7387\u304c\u5747\u7b49\u5316\u3055\u308c\u305f\u91cd\u307f\u3092\u4f7f\u7528\u3057\u307e\u3059</a>\u3002</p>\n",
 "<p> <a id=\"equalized_weight\"></a></p>\n<h2>Learning-rate Equalized Weights Parameter</h2>\n<p>This is based on equalized learning rate introduced in the Progressive GAN paper. Instead of initializing weights at <span translate=no>_^_0_^_</span> they initialize weights to <span translate=no>_^_1_^_</span> and then multiply them by <span translate=no>_^_2_^_</span> when using it. <span translate=no>_^_3_^_</span></p>\n<p>The gradients on stored parameters <span translate=no>_^_4_^_</span> get multiplied by <span translate=no>_^_5_^_</span> but this doesn&#x27;t have an affect since optimizers such as Adam normalize them by a running mean of the squared gradients.</p>\n<p>The optimizer updates on <span translate=no>_^_6_^_</span> are proportionate to the learning rate <span translate=no>_^_7_^_</span>. But the effective weights <span translate=no>_^_8_^_</span> get updated proportionately to <span translate=no>_^_9_^_</span>. Without equalized learning rate, the effective weights will get updated proportionately to just <span translate=no>_^_10_^_</span>.</p>\n<p>So we are effectively scaling the learning rate by <span translate=no>_^_11_^_</span> for these weight parameters.</p>\n": "<p><a id=\"equalized_weight\"></a></p>\n<h2>\u5b66\u7fd2\u7387\u5747\u7b49\u5316\u91cd\u307f\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc</h2>\n<p>\u3053\u308c\u306f\u3001\u30d7\u30ed\u30b0\u30ec\u30c3\u30b7\u30d6GAN\u306e\u8ad6\u6587\u3067\u7d39\u4ecb\u3055\u308c\u305f\u5b66\u7fd2\u7387\u306e\u5747\u7b49\u5316\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002\u30a6\u30a7\u30a4\u30c8\u3092\u3067\u521d\u671f\u5316\u3059\u308b\u4ee3\u308f\u308a\u306b\u3001\u30a6\u30a7\u30a4\u30c8\u3092\u306b\u521d\u671f\u5316\u3057\u3001<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u4f7f\u7528\u6642\u306b\u305d\u306e\u30a6\u30a7\u30a4\u30c8\u3092\u4e57\u7b97\u3057\u307e\u3059\u3002<span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span></p>\n<p><span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\u4fdd\u5b58\u3055\u308c\u3066\u3044\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u52fe\u914d\u306f\u4e57\u7b97\u3055\u308c\u307e\u3059\u304c\u3001Adam \u306a\u3069\u306e\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306f\u52fe\u914d\u306e 2 \u4e57\u5e73\u5747\u3067\u6b63\u898f\u5316\u3059\u308b\u305f\u3081\u3001\u5f71\u97ff\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p>\n<p><span translate=no>_^_6_^_</span>\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306e\u66f4\u65b0\u306f\u5b66\u7fd2\u7387\u306b\u6bd4\u4f8b\u3057\u307e\u3059\u3002<span translate=no>_^_7_^_</span>\u305f\u3060\u3057\u3001<span translate=no>_^_8_^_</span>\u6709\u52b9\u91cd\u307f\u306f\u305d\u308c\u306b\u6bd4\u4f8b\u3057\u3066\u66f4\u65b0\u3055\u308c\u307e\u3059\u3002<span translate=no>_^_9_^_</span>\u5b66\u7fd2\u7387\u304c\u5747\u7b49\u5316\u3055\u308c\u3066\u3044\u306a\u3044\u3068\u3001\u6709\u52b9\u91cd\u307f\u306f\u6b63\u306b\u6bd4\u4f8b\u3057\u3066\u66f4\u65b0\u3055\u308c\u307e\u3059</p>\u3002<span translate=no>_^_10_^_</span>\n<p>\u305d\u3053\u3067\u3001<span translate=no>_^_11_^_</span>\u3053\u308c\u3089\u306e\u91cd\u307f\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3088\u3063\u3066\u5b66\u7fd2\u7387\u3092\u52b9\u679c\u7684\u306b\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3057\u3066\u3044\u307e\u3059\u3002</p>\n",
 "<p> <a id=\"generator\"></a></p>\n<h2>StyleGAN2 Generator</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p><small><em><span translate=no>_^_1_^_</span> denotes a linear layer. <span translate=no>_^_2_^_</span> denotes a broadcast and scaling operation (noise is a single channel). <a href=\"#to_rgb\"><span translate=no>_^_3_^_</span></a> also has a style modulation which is not shown in the diagram to keep it simple.</em></small></p>\n<p>The generator starts with a learned constant. Then it has a series of blocks. The feature map resolution is doubled at each block Each block outputs an RGB image and they are scaled up and summed to get the final RGB image.</p>\n": "<p><a id=\"generator\"></a></p>\n<h2>\u30b9\u30bf\u30a4\u30eb GAN2 \u30b8\u30a7\u30cd\u30ec\u30fc\u30bf</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p><small><em><span translate=no>_^_1_^_</span>\u7dda\u5f62\u30ec\u30a4\u30e4\u30fc\u3092\u793a\u3057\u307e\u3059\u3002<span translate=no>_^_2_^_</span>\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8\u3068\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u64cd\u4f5c\u3092\u8868\u3057\u307e\u3059\uff08\u30ce\u30a4\u30ba\u306f\u5358\u4e00\u30c1\u30e3\u30cd\u30eb\uff09\u3002<a href=\"#to_rgb\"><span translate=no>_^_3_^_</span></a>\u307e\u305f\u3001\u56f3\u306b\u306f\u793a\u3055\u308c\u3066\u3044\u306a\u3044\u30b9\u30bf\u30a4\u30eb\u30e2\u30b8\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3082\u4ed8\u3044\u3066\u304a\u308a\u3001\u30b7\u30f3\u30d7\u30eb\u3055\u3092\u4fdd\u3063\u3066\u3044\u307e\u3059</em></small></p>\u3002\n<p>\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u306f\u5b66\u7fd2\u3057\u305f\u5b9a\u6570\u304b\u3089\u59cb\u307e\u308a\u307e\u3059\u3002\u6b21\u306b\u3001\u4e00\u9023\u306e\u30d6\u30ed\u30c3\u30af\u304c\u3042\u308a\u307e\u3059\u3002\u7279\u5fb4\u30de\u30c3\u30d7\u306e\u89e3\u50cf\u5ea6\u306f\u5404\u30d6\u30ed\u30c3\u30af\u3067 2 \u500d\u306b\u306a\u308a\u307e\u3059\u3002\u5404\u30d6\u30ed\u30c3\u30af\u306f RGB \u753b\u50cf\u3092\u51fa\u529b\u3057\u3001\u305d\u308c\u3089\u3092\u62e1\u5927\u3057\u3066\u5408\u8a08\u3057\u3066\u6700\u7d42\u7684\u306a RGB \u753b\u50cf\u306b\u306a\u308a\u307e\u3059</p>\u3002\n",
 "<p> <a id=\"generator_block\"></a></p>\n<h3>Generator Block</h3>\n<p><span translate=no>_^_0_^_</span></p>\n<p><small><em><span translate=no>_^_1_^_</span> denotes a linear layer. <span translate=no>_^_2_^_</span> denotes a broadcast and scaling operation (noise is a single channel). <a href=\"#to_rgb\"><span translate=no>_^_3_^_</span></a> also has a style modulation which is not shown in the diagram to keep it simple.</em></small></p>\n<p>The generator block consists of two <a href=\"#style_block\">style blocks</a> (<span translate=no>_^_4_^_</span> convolutions with style modulation) and an RGB output.</p>\n": "<p><a id=\"generator_block\"></a></p>\n<h3>\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30d6\u30ed\u30c3\u30af</h3>\n<p><span translate=no>_^_0_^_</span></p>\n<p><small><em><span translate=no>_^_1_^_</span>\u7dda\u5f62\u30ec\u30a4\u30e4\u30fc\u3092\u793a\u3057\u307e\u3059\u3002<span translate=no>_^_2_^_</span>\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8\u3068\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u64cd\u4f5c\u3092\u8868\u3057\u307e\u3059\uff08\u30ce\u30a4\u30ba\u306f\u5358\u4e00\u30c1\u30e3\u30cd\u30eb\uff09\u3002<a href=\"#to_rgb\"><span translate=no>_^_3_^_</span></a>\u307e\u305f\u3001\u56f3\u306b\u306f\u793a\u3055\u308c\u3066\u3044\u306a\u3044\u30b9\u30bf\u30a4\u30eb\u30e2\u30b8\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3082\u4ed8\u3044\u3066\u304a\u308a\u3001\u30b7\u30f3\u30d7\u30eb\u3055\u3092\u4fdd\u3063\u3066\u3044\u307e\u3059</em></small></p>\u3002\n<p>\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u30d6\u30ed\u30c3\u30af\u306f\u30012 <a href=\"#style_block\">\u3064\u306e\u30b9\u30bf\u30a4\u30eb\u30d6\u30ed\u30c3\u30af (<span translate=no>_^_4_^_</span>\u30b9\u30bf\u30a4\u30eb\u5909\u8abf\u306b\u3088\u308b\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3</a>) \u3068 1 \u3064\u306e RGB \u51fa\u529b\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>\n",
 "<p> <a id=\"gradient_penalty\"></a></p>\n<h2>Gradient Penalty</h2>\n<p>This is the <span translate=no>_^_0_^_</span> regularization penality from the paper <a href=\"https://arxiv.org/abs/1801.04406\">Which Training Methods for GANs do actually Converge?</a>.</p>\n<p><span translate=no>_^_1_^_</span></p>\n<p>That is we try to reduce the L2 norm of gradients of the discriminator with respect to images, for real images (<span translate=no>_^_2_^_</span>).</p>\n": "<p><a id=\"gradient_penalty\"></a></p>\n<h2>\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u30da\u30ca\u30eb\u30c6\u30a3</h2>\n<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://arxiv.org/abs/1801.04406\">GAN\u306e\u3069\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u65b9\u6cd5\u304c\u5b9f\u969b\u306b\u53ce\u675f\u3059\u308b\u306e\u304b\u300d<span translate=no>_^_0_^_</span> \u3068\u3044\u3046\u8ad6\u6587\u306e\u6b63\u5247\u5316\u306e\u30da\u30ca\u30eb\u30c6\u30a3\u3067\u3059</a>\u3002</p>\u3002\n<p><span translate=no>_^_1_^_</span></p>\n<p>\u3064\u307e\u308a\u3001\u5b9f\u969b\u306e\u753b\u50cf () \u306b\u3064\u3044\u3066\u3001\u753b\u50cf\u306b\u5bfe\u3059\u308b\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u306e\u52fe\u914d\u306eL2\u30ce\u30eb\u30e0\u3092\u5c0f\u3055\u304f\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u307e\u3059\u3002<span translate=no>_^_2_^_</span></p>\n",
 "<p> <a id=\"mapping_network\"></a></p>\n<h2>Mapping Network</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p>This is an MLP with 8 linear layers. The mapping network maps the latent vector <span translate=no>_^_1_^_</span> to an intermediate latent space <span translate=no>_^_2_^_</span>. <span translate=no>_^_3_^_</span> space will be disentangled from the image space where the factors of variation become more linear.</p>\n": "<p><a id=\"mapping_network\"></a></p>\n<h2>\u30de\u30c3\u30d4\u30f3\u30b0\u30cd\u30c3\u30c8\u30ef\u30fc\u30af</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p>\u3053\u308c\u306f8\u3064\u306e\u7dda\u5f62\u30ec\u30a4\u30e4\u30fc\u3092\u5099\u3048\u305fMLP\u3067\u3059\u3002\u30de\u30c3\u30d4\u30f3\u30b0\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u3001<span translate=no>_^_1_^_</span>\u6f5c\u5728\u30d9\u30af\u30c8\u30eb\u3092\u4e2d\u9593\u6f5c\u5728\u7a7a\u9593\u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u307e\u3059\u3002<span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span>\u7a7a\u9593\u306f\u753b\u50cf\u7a7a\u9593\u304b\u3089\u5207\u308a\u96e2\u3055\u308c\u3001\u5909\u5316\u306e\u8981\u56e0\u304c\u3088\u308a\u76f4\u7dda\u7684\u306b\u306a\u308a\u307e\u3059</p>\u3002\n",
 "<p> <a id=\"mini_batch_std_dev\"></a></p>\n<h3>Mini-batch Standard Deviation</h3>\n<p>Mini-batch standard deviation calculates the standard deviation across a mini-batch (or a subgroups within the mini-batch) for each feature in the feature map. Then it takes the mean of all the standard deviations and appends it to the feature map as one extra feature.</p>\n": "<p><a id=\"mini_batch_std_dev\"></a></p>\n<h3>\u30df\u30cb\u30d0\u30c3\u30c1\u6a19\u6e96\u504f\u5dee</h3>\n<p>\u30df\u30cb\u30d0\u30c3\u30c1\u6a19\u6e96\u504f\u5dee\u306f\u3001\u7279\u5fb4\u30de\u30c3\u30d7\u5185\u306e\u5404\u30d5\u30a3\u30fc\u30c1\u30e3\u306b\u3064\u3044\u3066\u3001\u30df\u30cb\u30d0\u30c3\u30c1 (\u307e\u305f\u306f\u30df\u30cb\u30d0\u30c3\u30c1\u5185\u306e\u30b5\u30d6\u30b0\u30eb\u30fc\u30d7) \u5168\u4f53\u306e\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u6b21\u306b\u3001\u3059\u3079\u3066\u306e\u6a19\u6e96\u504f\u5dee\u306e\u5e73\u5747\u3092\u53d6\u5f97\u3057\u3001\u305d\u308c\u3092 1 \u3064\u306e\u7279\u5fb4\u3068\u3057\u3066\u7279\u5fb4\u30de\u30c3\u30d7\u306b\u8ffd\u52a0\u3057\u307e\u3059\u3002</p>\n",
 "<p> <a id=\"path_length_penalty\"></a></p>\n<h2>Path Length Penalty</h2>\n<p>This regularization encourages a fixed-size step in <span translate=no>_^_0_^_</span> to result in a fixed-magnitude change in the image.</p>\n<p><span translate=no>_^_1_^_</span></p>\n<p>where <span translate=no>_^_2_^_</span> is the Jacobian <span translate=no>_^_3_^_</span>, <span translate=no>_^_4_^_</span> are sampled from <span translate=no>_^_5_^_</span> from the mapping network, and <span translate=no>_^_6_^_</span> are images with noise <span translate=no>_^_7_^_</span>.</p>\n<p><span translate=no>_^_8_^_</span> is the exponential moving average of <span translate=no>_^_9_^_</span> as the training progresses.</p>\n<p><span translate=no>_^_10_^_</span> is calculated without explicitly calculating the Jacobian using <span translate=no>_^_11_^_</span></p>\n": "<p><a id=\"path_length_penalty\"></a></p>\n<h2>\u7d4c\u8def\u9577\u30da\u30ca\u30eb\u30c6\u30a3</h2>\n<p>\u3053\u306e\u6b63\u5247\u5316\u306b\u3088\u308a\u3001<span translate=no>_^_0_^_</span>\u56fa\u5b9a\u30b5\u30a4\u30ba\u306e\u30b9\u30c6\u30c3\u30d7\u30a4\u30f3\u304c\u4fc3\u9032\u3055\u308c\u3001\u753b\u50cf\u306e\u5927\u304d\u3055\u304c\u56fa\u5b9a\u3055\u308c\u305f\u5909\u5316\u304c\u751f\u3058\u307e\u3059\u3002</p>\n<p><span translate=no>_^_1_^_</span></p>\n<p>\u3053\u3053\u3067\u3001<span translate=no>_^_2_^_</span><span translate=no>_^_5_^_</span>\u306f\u30de\u30c3\u30d4\u30f3\u30b0\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3055\u308c\u305f\u30e4\u30b3\u30d3\u30a2\u30f3\u3067\u3001<span translate=no>_^_3_^_</span><span translate=no>_^_6_^_</span>\u30ce\u30a4\u30ba\u306e\u5165\u3063\u305f\u753b\u50cf\u3067\u3059\u3002<span translate=no>_^_4_^_</span> <span translate=no>_^_7_^_</span></p>\n<p><span translate=no>_^_8_^_</span>\u306f\u3001<span translate=no>_^_9_^_</span>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u9032\u884c\u306b\u4f34\u3046\u6307\u6570\u79fb\u52d5\u5e73\u5747\u3067\u3059\u3002</p>\n<p><span translate=no>_^_10_^_</span>\u3092\u4f7f\u7528\u3057\u3066\u30e4\u30b3\u30d3\u30a2\u30f3\u3092\u660e\u793a\u7684\u306b\u8a08\u7b97\u305b\u305a\u306b\u8a08\u7b97\u3055\u308c\u307e\u3059 <span translate=no>_^_11_^_</span></p>\n",
 "<p> <a id=\"smooth\"></a></p>\n<h3>Smoothing Layer</h3>\n<p>This layer blurs each channel</p>\n": "<p><a id=\"smooth\"></a></p>\n<h3>\u30b9\u30e0\u30fc\u30b8\u30f3\u30b0\u30ec\u30a4\u30e4\u30fc</h3>\n<p>\u3053\u306e\u30ec\u30a4\u30e4\u30fc\u306f\u5404\u30c1\u30e3\u30f3\u30cd\u30eb\u3092\u307c\u304b\u3057\u307e\u3059</p>\n",
 "<p> <a id=\"style_block\"></a></p>\n<h3>Style Block</h3>\n<p><span translate=no>_^_0_^_</span></p>\n<p><small><em><span translate=no>_^_1_^_</span> denotes a linear layer. <span translate=no>_^_2_^_</span> denotes a broadcast and scaling operation (noise is single channel).</em></small></p>\n<p>Style block has a weight modulation convolution layer.</p>\n": "<p><a id=\"style_block\"></a></p>\n<h3>\u30b9\u30bf\u30a4\u30eb\u30d6\u30ed\u30c3\u30af</h3>\n<p><span translate=no>_^_0_^_</span></p>\n<p><small><em><span translate=no>_^_1_^_</span>\u7dda\u5f62\u30ec\u30a4\u30e4\u30fc\u3092\u793a\u3057\u307e\u3059\u3002<span translate=no>_^_2_^_</span>\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8\u3068\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u64cd\u4f5c\u3092\u8868\u3057\u307e\u3059\uff08\u30ce\u30a4\u30ba\u306f\u30b7\u30f3\u30b0\u30eb\u30c1\u30e3\u30cd\u30eb</em></small></p>\uff09\u3002\n<p>\u30b9\u30bf\u30a4\u30eb\u30d6\u30ed\u30c3\u30af\u306b\u306f\u30a6\u30a7\u30a4\u30c8\u30e2\u30b8\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc\u304c\u3042\u308a\u307e\u3059\u3002</p>\n",
 "<p> <a id=\"to_rgb\"></a></p>\n<h3>To RGB</h3>\n<p><span translate=no>_^_0_^_</span></p>\n<p><small><em><span translate=no>_^_1_^_</span> denotes a linear layer.</em></small></p>\n<p>Generates an RGB image from a feature map using <span translate=no>_^_2_^_</span> convolution.</p>\n": "<p><a id=\"to_rgb\"></a></p>\n<h3>RGB \u3078</h3>\n<p><span translate=no>_^_0_^_</span></p>\n<p><small><em><span translate=no>_^_1_^_</span>\u7dda\u5f62\u30ec\u30a4\u30e4\u30fc\u3092\u793a\u3057\u307e\u3059\u3002</em></small></p>\n<p><span translate=no>_^_2_^_</span>\u7573\u307f\u8fbc\u307f\u3092\u4f7f\u7528\u3057\u3066\u3001\u7279\u5fb4\u30de\u30c3\u30d7\u304b\u3089 RGB \u753b\u50cf\u3092\u751f\u6210\u3057\u307e\u3059\u3002</p>\n",
 "<p> <a id=\"up_sample\"></a></p>\n<h3>Up-sample</h3>\n<p>The up-sample operation scales the image up by <span translate=no>_^_0_^_</span> and <a href=\"#smooth\">smoothens</a> each feature channel. This is based on the paper  <a href=\"https://arxiv.org/abs/1904.11486\">Making Convolutional Networks Shift-Invariant Again</a>.</p>\n": "<p><a id=\"up_sample\"></a></p>\n<h3>\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30eb</h3>\n<p>\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30eb\u64cd\u4f5c\u3067\u306f\u3001<span translate=no>_^_0_^_</span>\u753b\u50cf\u304c\u5404\u30d5\u30a3\u30fc\u30c1\u30e3\u30c1\u30e3\u30cd\u30eb\u3054\u3068\u306b\u62e1\u5927\u3055\u308c\u3001<a href=\"#smooth\">\u6ed1\u3089\u304b\u306b\u306a\u308a\u307e\u3059</a>\u3002\u3053\u308c\u306f\u3001\u300c<a href=\"https://arxiv.org/abs/1904.11486\">\u7573\u307f\u8fbc\u307f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u518d\u3073\u30b7\u30d5\u30c8\u4e0d\u5909\u306b\u3059\u308b</a>\u300d\u3068\u3044\u3046\u8ad6\u6587\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059</p>\u3002\n",
 "<p><a href=\"#equalized_linear\">Equalized learning-rate linear layers</a> </p>\n": "<p><a href=\"#equalized_linear\">\u5b66\u7fd2\u7387\u306e\u5747\u7b49\u5316\u30ea\u30cb\u30a2\u30ec\u30a4\u30e4\u30fc</a></p>\n",
 "<p><a href=\"#equalized_weight\">Weights parameter with equalized learning rate</a> </p>\n": "<p><a href=\"#equalized_weight\">\u5b66\u7fd2\u7387\u304c\u5747\u7b49\u5316\u3055\u308c\u305f\u91cd\u307f\u30d1\u30e9\u30e1\u30fc\u30bf</a></p>\n",
 "<p><a href=\"#equalized_weights\">Learning-rate equalized weights</a> </p>\n": "<p><a href=\"#equalized_weights\">\u5b66\u7fd2\u7387\u5747\u7b49\u5316\u30a6\u30a7\u30a4\u30c8</a></p>\n",
 "<p><a href=\"#mini_batch_std_dev\">Mini-batch Standard Deviation</a> </p>\n": "<p><a href=\"#mini_batch_std_dev\">\u30df\u30cb\u30d0\u30c3\u30c1\u6a19\u6e96\u504f\u5dee</a></p>\n",
 "<p><em>Our implementation is a minimalistic StyleGAN 2 model training code. Only single GPU training is supported to keep the implementation simple. We managed to shrink it to keep it at less than 500 lines of code, including the training loop.</em></p>\n": "<p><em>\u79c1\u305f\u3061\u306e\u5b9f\u88c5\u306f\u3001\u6700\u5c0f\u9650\u306eStyleGAN 2\u30e2\u30c7\u30eb\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u3067\u3059\u3002\u5b9f\u88c5\u3092\u30b7\u30f3\u30d7\u30eb\u306b\u4fdd\u3064\u305f\u3081\u3001\u5358\u4e00\u306e GPU \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u307f\u304c\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u306a\u3093\u3068\u304b\u7e2e\u5c0f\u3057\u3066\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30eb\u30fc\u30d7\u3092\u542b\u3081\u3066 500 \u884c\u672a\u6e80\u306e\u30b3\u30fc\u30c9\u306b\u6291\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f</em></p>\u3002\n",
 "<p><em>toRGB</em> layer </p>\n": "<p><em>TorGB \u30ec\u30a4\u30e4\u30fc</em></p>\n",
 "<p><small><em><span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> denote feature map resolution scaling and scaling. <span translate=no>_^_2_^_</span>, <span translate=no>_^_3_^_</span>, ... denote feature map resolution at the generator or discriminator block. Each discriminator and generator block consists of 2 convolution layers with leaky ReLU activations.</em></small></p>\n": "<p><small><em><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u306e\u89e3\u50cf\u5ea6\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3068\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3092\u793a\u3057\u307e\u3059\u3002<span translate=no>_^_2_^_</span>,<span translate=no>_^_3_^_</span>,... \u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30d6\u30ed\u30c3\u30af\u307e\u305f\u306f\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30d6\u30ed\u30c3\u30af\u3067\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u89e3\u50cf\u5ea6\u3092\u793a\u3057\u307e\u3059\u3002\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u3068\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u306e\u5404\u30d6\u30ed\u30c3\u30af\u306f\u3001\u30ea\u30fc\u30af\u3057\u3084\u3059\u3044ReLU\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u3092\u5099\u3048\u305f2\u3064\u306e\u7573\u307f\u8fbc\u307f\u5c64\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059</em></small></p>\u3002\n",
 "<p><small><em><span translate=no>_^_0_^_</span> denotes a linear layer. <span translate=no>_^_1_^_</span> denotes a broadcast and scaling operation (noise is a single channel). StyleGAN also uses progressive growing like Progressive GAN.</em></small></p>\n": "<p><small><em><span translate=no>_^_0_^_</span>\u7dda\u5f62\u30ec\u30a4\u30e4\u30fc\u3092\u793a\u3057\u307e\u3059\u3002<span translate=no>_^_1_^_</span>\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8\u3068\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u64cd\u4f5c\u3092\u8868\u3057\u307e\u3059\uff08\u30ce\u30a4\u30ba\u306f\u5358\u4e00\u30c1\u30e3\u30cd\u30eb\uff09\u3002StyleGan\u306f\u3001\u30d7\u30ed\u30b0\u30ec\u30c3\u30b7\u30d6GAN\u306e\u3088\u3046\u306a\u30d7\u30ed\u30b0\u30ec\u30c3\u30b7\u30d6\u683d\u57f9\u3082\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059</em></small></p>\u3002\n",
 "<p><small><em>These are <span translate=no>_^_0_^_</span> images generated after training for about 80K steps.</em></small></p>\n": "<p><small><em>\u3053\u308c\u3089\u306f\u3001\u7d04 80K <span translate=no>_^_0_^_</span> \u30b9\u30c6\u30c3\u30d7\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u5f8c\u306b\u751f\u6210\u3055\u308c\u305f\u753b\u50cf\u3067\u3059\u3002</em></small></p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> Then it&#x27;s demodulated by normalizing, <span translate=no>_^_1_^_</span> where <span translate=no>_^_2_^_</span> is the input channel, <span translate=no>_^_3_^_</span> is the output channel, and <span translate=no>_^_4_^_</span> is the kernel index.</p>\n": "<p><span translate=no>_^_0_^_</span>\u6b21\u306b\u3001\u306f\u5165\u529b\u30c1\u30e3\u30f3\u30cd\u30eb\u3001\u306f\u51fa\u529b\u30c1\u30e3\u30f3\u30cd\u30eb\u3001<span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span><span translate=no>_^_4_^_</span>\u306f\u30ab\u30fc\u30cd\u30eb\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u6b63\u898f\u5316\u3057\u3066\u5fa9\u8abf\u3055\u308c\u307e\u3059\u3002</p>\n",
 "<p><span translate=no>_^_0_^_</span> convolution </p>\n": "<p><span translate=no>_^_0_^_</span>\u7573\u307f\u8fbc\u307f</p>\n",
 "<p><span translate=no>_^_0_^_</span> up sampling layer. The feature space is up sampled at each block </p>\n": "<p><span translate=no>_^_0_^_</span>\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ec\u30a4\u30e4\u30fc\u3002\u30d5\u30a3\u30fc\u30c1\u30e3\u30fc\u30b9\u30da\u30fc\u30b9\u306f\u5404\u30d6\u30ed\u30c3\u30af\u3067\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3055\u308c\u307e\u3059</p>\u3002\n",
 "<p><span translate=no>_^_0_^_</span> where <span translate=no>_^_1_^_</span> is the input channel, <span translate=no>_^_2_^_</span> is the output channel, and <span translate=no>_^_3_^_</span> is the kernel index.</p>\n<p>The result has shape <span translate=no>_^_4_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u3053\u3053\u3067\u3001<span translate=no>_^_1_^_</span>\u306f\u5165\u529b\u30c1\u30e3\u30f3\u30cd\u30eb\u3001<span translate=no>_^_2_^_</span>\u306f\u51fa\u529b\u30c1\u30e3\u30f3\u30cd\u30eb\u3001<span translate=no>_^_3_^_</span>\u306f\u30ab\u30fc\u30cd\u30eb\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3002</p>\n<p>\u7d50\u679c\u306b\u306f\u5f62\u304c\u3042\u308a\u307e\u3059 <span translate=no>_^_4_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p><strong>\ud83c\udfc3 Here&#x27;s the training code: <a href=\"experiment.html\"><span translate=no>_^_0_^_</span></a>.</strong></p>\n": "<p><strong>\ud83c\udfc3 \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059:<a href=\"experiment.html\"><span translate=no>_^_0_^_</span></a>.</strong></p>\n",
 "<p>Activation function </p>\n": "<p>\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u6a5f\u80fd</p>\n",
 "<p>Add bias and evaluate activation function </p>\n": "<p>\u30d0\u30a4\u30a2\u30b9\u3092\u52a0\u3048\u3066\u6d3b\u6027\u5316\u95a2\u6570\u3092\u8a55\u4fa1\u3059\u308b</p>\n",
 "<p>Add padding </p>\n": "<p>\u30d1\u30c7\u30a3\u30f3\u30b0\u3092\u8ffd\u52a0</p>\n",
 "<p>Add the residual and scale </p>\n": "<p>\u6b8b\u5dee\u3092\u8ffd\u52a0\u3057\u3066\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3057\u307e\u3059</p>\n",
 "<p>All the up and down-sampling operations are accompanied by bilinear smoothing.</p>\n": "<p>\u3059\u3079\u3066\u306e\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u64cd\u4f5c\u3068\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u64cd\u4f5c\u306b\u306f\u3001\u30d0\u30a4\u30ea\u30cb\u30a2\u30b9\u30e0\u30fc\u30b8\u30f3\u30b0\u304c\u4f34\u3044\u307e\u3059\u3002</p>\n",
 "<p>Append (concatenate) the standard deviations to the feature map </p>\n": "<p>\u6a19\u6e96\u504f\u5dee\u3092\u6a5f\u80fd\u30de\u30c3\u30d7\u306b\u8ffd\u52a0 (\u9023\u7d50) \u3057\u307e\u3059</p>\n",
 "<p>At each resolution, the generator network produces an image in latent space which is converted into RGB, with a <span translate=no>_^_0_^_</span> convolution. When we progress from a lower resolution to a higher resolution  (say from <span translate=no>_^_1_^_</span> to <span translate=no>_^_2_^_</span> ) we scale the latent image by <span translate=no>_^_3_^_</span>  and add a new block (two <span translate=no>_^_4_^_</span> convolution layers)  and a new <span translate=no>_^_5_^_</span> layer to get RGB. The transition is done smoothly by adding a residual connection to  the <span translate=no>_^_6_^_</span> scaled <span translate=no>_^_7_^_</span> RGB image. The weight of this residual connection is slowly reduced, to let the new block take over.</p>\n": "<p>\u305d\u308c\u305e\u308c\u306e\u89e3\u50cf\u5ea6\u3067\u3001\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u6f5c\u5728\u7a7a\u9593\u306b\u753b\u50cf\u3092\u751f\u6210\u3057\u3001<span translate=no>_^_0_^_</span>\u305d\u308c\u3092\u7573\u307f\u8fbc\u307f\u3067RGB\u306b\u5909\u63db\u3057\u307e\u3059\u3002\u4f4e\u3044\u89e3\u50cf\u5ea6\u304b\u3089\u9ad8\u3044\u89e3\u50cf\u5ea6\u3078\uff08\u305f\u3068\u3048\u3070\u304b\u3089 <span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\uff09\u9032\u3080\u3068\u304d\u3001<span translate=no>_^_3_^_</span>\u6f5c\u5728\u753b\u50cf\u3092\u62e1\u5927\u7e2e\u5c0f\u3057\u3001\u65b0\u3057\u3044\u30d6\u30ed\u30c3\u30af\uff08<span translate=no>_^_4_^_</span>2\u3064\u306e\u7573\u307f\u8fbc\u307f\u5c64\uff09<span translate=no>_^_5_^_</span>\u3068\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u3092\u8ffd\u52a0\u3057\u3066RGB\u306b\u3057\u307e\u3059\u3002<span translate=no>_^_6_^_</span>\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3055\u308c\u305f <span translate=no>_^_7_^_</span> RGB \u753b\u50cf\u306b\u6b8b\u4f59\u63a5\u7d9a\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u30c8\u30e9\u30f3\u30b8\u30b7\u30e7\u30f3\u304c\u30b9\u30e0\u30fc\u30ba\u306b\u884c\u308f\u308c\u307e\u3059\u3002\u3053\u306e\u6b8b\u3063\u305f\u63a5\u7d9a\u90e8\u306e\u91cd\u91cf\u306f\u5f90\u3005\u306b\u6e1b\u3089\u3055\u308c\u3066\u3044\u304d\u3001\u65b0\u3057\u3044\u30d6\u30ed\u30c3\u30af\u306b\u5f15\u304d\u7d99\u304c\u308c\u307e\u3059</p>\u3002\n",
 "<p>Bias </p>\n": "<p>\u30d0\u30a4\u30a2\u30b9</p>\n",
 "<p>Blurring kernel </p>\n": "<p>\u30d6\u30e9\u30fc\u30ea\u30f3\u30b0\u30ab\u30fc\u30cd\u30eb</p>\n",
 "<p>Calculate <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8a08\u7b97 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> </p>\n": "<p>\u8a08\u7b97 <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span></p>\n",
 "<p>Calculate <span translate=no>_^_0_^_</span> and normalize by the square root of image size. This is scaling is not mentioned in the paper but was present in <a href=\"https://github.com/NVlabs/stylegan2/blob/master/training/loss.py#L167\">their implementation</a>. </p>\n": "<p><span translate=no>_^_0_^_</span>\u753b\u50cf\u30b5\u30a4\u30ba\u306e\u5e73\u65b9\u6839\u3067\u8a08\u7b97\u3057\u3066\u6b63\u898f\u5316\u3057\u307e\u3059\u3002\u3053\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u306b\u3064\u3044\u3066\u306f\u8ad6\u6587\u3067\u306f\u89e6\u308c\u3089\u308c\u3066\u3044\u307e\u305b\u3093\u304c\u3001<a href=\"https://github.com/NVlabs/stylegan2/blob/master/training/loss.py#L167\">\u5b9f\u88c5\u306b\u306f\u5b58\u5728\u3057\u3066\u3044\u307e\u3057\u305f</a>\u3002</p>\n",
 "<p>Calculate L2-norm of <span translate=no>_^_0_^_</span> </p>\n": "<p>\u306e L2 \u30ce\u30eb\u30e0\u306e\u8a08\u7b97 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate and append <a href=\"#mini_batch_std_dev\">mini-batch standard deviation</a> </p>\n": "<p><a href=\"#mini_batch_std_dev\">\u30df\u30cb\u30d0\u30c3\u30c1\u6a19\u6e96\u504f\u5dee\u306e\u8a08\u7b97\u3068\u8ffd\u52a0</a></p>\n",
 "<p>Calculate gradients of <span translate=no>_^_0_^_</span> with respect to <span translate=no>_^_1_^_</span>. <span translate=no>_^_2_^_</span> is set to <span translate=no>_^_3_^_</span> since we want the gradients of <span translate=no>_^_4_^_</span>, and we need to create and retain graph since we have to compute gradients with respect to weight on this loss. </p>\n": "<p><span translate=no>_^_0_^_</span>\u3092\u57fa\u6e96\u3068\u3057\u305f\u52fe\u914d\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002<span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span>\u306e\u52fe\u914d\u3092\u6c42\u3081\u3066\u3044\u308b\u306e\u3067\u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u307e\u305f<span translate=no>_^_4_^_</span>\u3001\u3053\u306e\u640d\u5931\u306b\u3088\u308b\u91cd\u307f\u306b\u5bfe\u3059\u308b\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3057\u3066\u4fdd\u6301\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</p>\u3002\n",
 "<p>Calculate gradients to get <span translate=no>_^_0_^_</span> </p>\n": "<p>\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3066\u53d6\u5f97 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate the mean of <span translate=no>_^_0_^_</span> </p>\n": "<p>\u306e\u5e73\u5747\u3092\u8a08\u7b97 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate the norm <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30ce\u30eb\u30e0\u306e\u8a08\u7b97 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate the number of features for each block.</p>\n<p>Something like <span translate=no>_^_0_^_</span>. </p>\n": "<p>\u5404\u30d6\u30ed\u30c3\u30af\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u6570\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p>\n<p>\u306e\u3088\u3046\u306a\u3082\u306e<span translate=no>_^_0_^_</span>\u3002</p>\n",
 "<p>Calculate the number of features for each block</p>\n<p>Something like <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5404\u30d6\u30ed\u30c3\u30af\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u6570\u3092\u8a08\u7b97\u3057\u307e\u3059</p>\n<p>\u306e\u3088\u3046\u306a\u3082\u306e <span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate the penalty <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30da\u30ca\u30eb\u30c6\u30a3\u306e\u8a08\u7b97 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate the standard deviation for each feature among <span translate=no>_^_0_^_</span> samples</p>\n<span translate=no>_^_1_^_</span><p> </p>\n": "<p><span translate=no>_^_0_^_</span>\u30b5\u30f3\u30d7\u30eb\u9593\u306e\u5404\u7279\u5fb4\u306e\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u3057\u307e\u3059</p>\n<span translate=no>_^_1_^_</span><p></p>\n",
 "<p>Check if the batch size is divisible by the group size </p>\n": "<p>\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u304c\u30b0\u30eb\u30fc\u30d7\u30b5\u30a4\u30ba\u3067\u5272\u308a\u5207\u308c\u308b\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Convert from RGB </p>\n": "<p>RGB \u304b\u3089\u5909\u63db</p>\n",
 "<p>Convert the kernel to a PyTorch tensor </p>\n": "<p>\u30ab\u30fc\u30cd\u30eb\u3092 PyTorch \u30c6\u30f3\u30bd\u30eb\u306b\u5909\u63db\u3057\u307e\u3059</p>\n",
 "<p>Convolution </p>\n": "<p>\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3</p>\n",
 "<p>Convolutions </p>\n": "<p>\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3</p>\n",
 "<p>Create the MLP </p>\n": "<p>MLP \u3092\u4f5c\u6210\u3057\u3066\u4e0b\u3055\u3044</p>\n",
 "<p>Demodulate </p>\n": "<p>\u5fa9\u8abf\u3059\u308b</p>\n",
 "<p>Discriminator blocks </p>\n": "<p>\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u30d6\u30ed\u30c3\u30af</p>\n",
 "<p>Down-sample </p>\n": "<p>\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30eb</p>\n",
 "<p>Down-sampling and <span translate=no>_^_0_^_</span> convolution layer for the residual connection </p>\n": "<p><span translate=no>_^_0_^_</span>\u6b8b\u5dee\u63a5\u7d9a\u7528\u306e\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u5c64\u3068\u7573\u307f\u8fbc\u307f\u5c64</p>\n",
 "<p>Down-sampling layer </p>\n": "<p>\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Evaluate rest of the blocks </p>\n": "<p>\u6b8b\u308a\u306e\u30d6\u30ed\u30c3\u30af\u3092\u8a55\u4fa1</p>\n",
 "<p>Expand the learned constant to match batch size </p>\n": "<p>\u5b66\u7fd2\u3057\u305f\u5b9a\u6570\u3092\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306b\u5408\u308f\u305b\u3066\u62e1\u5f35\u3059\u308b</p>\n",
 "<p>Expand the standard deviation to append to the feature map </p>\n": "<p>\u6a19\u6e96\u504f\u5dee\u3092\u62e1\u5f35\u3057\u3066\u7279\u5fb4\u30de\u30c3\u30d7\u306b\u8ffd\u52a0</p>\n",
 "<p>Exponential sum of <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> where <span translate=no>_^_2_^_</span> is the value of it at <span translate=no>_^_3_^_</span>-th step of training </p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e <span translate=no>_^_2_^_</span> <span translate=no>_^_3_^_</span>-\u756a\u76ee\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u306e\u5024\u306e\u6307\u6570\u548c\u3092\u6c42\u3081\u308b</p>\n",
 "<p>Final <span translate=no>_^_0_^_</span> convolution layer </p>\n": "<p><span translate=no>_^_0_^_</span>\u6700\u7d42\u7573\u307f\u8fbc\u307f\u5c64</p>\n",
 "<p>Final linear layer to get the classification </p>\n": "<p>\u5206\u985e\u3092\u884c\u3046\u305f\u3081\u306e\u6700\u5f8c\u306e\u7dda\u5f62\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>First <a href=\"#style_block\">style block</a> changes the feature map size to <span translate=no>_^_0_^_</span> </p>\n": "<p><a href=\"#style_block\">\u6700\u521d\u306e\u30b9\u30bf\u30a4\u30eb\u30d6\u30ed\u30c3\u30af\u306f</a>\u3001\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u306e\u30b5\u30a4\u30ba\u3092\u6b21\u306e\u3088\u3046\u306b\u5909\u66f4\u3057\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>First style block for <span translate=no>_^_0_^_</span> resolution and layer to get RGB </p>\n": "<p><span translate=no>_^_0_^_</span>\u89e3\u50cf\u5ea6\u3068\u30ec\u30a4\u30e4\u30fc\u3092RGB\u306b\u3059\u308b\u6700\u521d\u306e\u30b9\u30bf\u30a4\u30eb\u30d6\u30ed\u30c3\u30af</p>\n",
 "<p>First style block with first noise tensor. The output is of shape <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6700\u521d\u306e\u30ce\u30a4\u30ba\u30c6\u30f3\u30bd\u30eb\u3092\u6301\u3064\u6700\u521d\u306e\u30b9\u30bf\u30a4\u30eb\u30d6\u30ed\u30c3\u30af\u3002\u51fa\u529b\u306f\u6574\u5f62\u3057\u3066\u3044\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Flatten </p>\n": "<p>\u5e73\u5766\u5316</p>\n",
 "<p>Generative adversarial networks have two components; the generator and the discriminator. The generator network takes a random latent vector (<span translate=no>_^_0_^_</span>)  and tries to generate a realistic image. The discriminator network tries to differentiate the real images from generated images. When we train the two networks together the generator starts generating images indistinguishable from real images.</p>\n": "<p>\u30b8\u30a7\u30cd\u30ec\u30fc\u30c6\u30a3\u30d6\u30fb\u30a2\u30c9\u30d0\u30fc\u30b5\u30ea\u30a2\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u306f\u3001\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u3068\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u3068\u3044\u30462\u3064\u306e\u8981\u7d20\u304c\u3042\u308a\u307e\u3059\u3002\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u6f5c\u5728\u30d9\u30af\u30c8\u30eb (<span translate=no>_^_0_^_</span>) \u3092\u53d7\u3051\u53d6\u308a\u3001\u30ea\u30a2\u30eb\u306a\u753b\u50cf\u3092\u751f\u6210\u3057\u3088\u3046\u3068\u3057\u307e\u3059\u3002\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u3001\u5b9f\u969b\u306e\u753b\u50cf\u3068\u751f\u6210\u3055\u308c\u305f\u753b\u50cf\u3092\u533a\u5225\u3057\u3088\u3046\u3068\u3057\u307e\u3059\u30022 \u3064\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4e00\u7dd2\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3068\u3001\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u306f\u5b9f\u969b\u306e\u753b\u50cf\u3068\u533a\u5225\u304c\u3064\u304b\u306a\u3044\u753b\u50cf\u3092\u751f\u6210\u3057\u59cb\u3081\u307e\u3059</p>\u3002\n",
 "<p>Generator blocks </p>\n": "<p>\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30d6\u30ed\u30c3\u30af</p>\n",
 "<p>Get <a href=\"#equalized_weight\">learning rate equalized weights</a> </p>\n": "<p><a href=\"#equalized_weight\">\u5b66\u7fd2\u7387\u3092\u5747\u7b49\u306b\u3057\u305f\u91cd\u307f\u4ed8\u3051\u3092\u5b9f\u884c</a></p>\n",
 "<p>Get RGB image </p>\n": "<p>RGB \u30a4\u30e1\u30fc\u30b8\u3092\u53d6\u5f97</p>\n",
 "<p>Get batch size </p>\n": "<p>\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3092\u53d6\u5f97</p>\n",
 "<p>Get batch size, height and width </p>\n": "<p>\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3001\u9ad8\u3055\u3001\u5e45\u3092\u53d6\u5f97</p>\n",
 "<p>Get first rgb image </p>\n": "<p>\u6700\u521d\u306e RGB \u30a4\u30e1\u30fc\u30b8\u3092\u53d6\u5f97</p>\n",
 "<p>Get number of pixels </p>\n": "<p>\u30d4\u30af\u30bb\u30eb\u6570\u3092\u53d6\u5f97</p>\n",
 "<p>Get shape of the input feature map </p>\n": "<p>\u5165\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u306e\u5f62\u72b6\u3092\u53d6\u5f97</p>\n",
 "<p>Get style vector <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30b9\u30bf\u30a4\u30eb\u30d9\u30af\u30c8\u30eb\u3092\u53d6\u5f97 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Get style vector from <span translate=no>_^_0_^_</span> (denoted by <span translate=no>_^_1_^_</span> in the diagram) with an <a href=\"#equalized_linear\">equalized learning-rate linear layer</a> </p>\n": "<p><a href=\"#equalized_linear\">\u5b66\u7fd2\u7387\u304c\u5747\u7b49\u5316\u3055\u308c\u305f\u7dda\u5f62\u5c64\u3067 <span translate=no>_^_0_^_</span></a> (<span translate=no>_^_1_^_</span>\u56f3\u3067\u793a\u3055\u308c\u3066\u3044\u308b) \u304b\u3089\u30b9\u30bf\u30a4\u30eb\u30d9\u30af\u30c8\u30eb\u3092\u53d6\u5f97</p>\n",
 "<p>Get the device </p>\n": "<p>\u30c7\u30d0\u30a4\u30b9\u3092\u5165\u624b</p>\n",
 "<p>Get the mean standard deviation </p>\n": "<p>\u5e73\u5747\u6a19\u6e96\u504f\u5dee\u3092\u53d6\u5f97</p>\n",
 "<p>Get the residual connection </p>\n": "<p>\u6b8b\u4f59\u63a5\u7d9a\u3092\u53d6\u5f97</p>\n",
 "<p>He initialization constant </p>\n": "<p>HE \u521d\u671f\u5316\u5b9a\u6570</p>\n",
 "<p>Increment <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30a4\u30f3\u30af\u30ea\u30e1\u30f3\u30c8 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Initialize the weights with <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30a6\u30a7\u30a4\u30c8\u3092\u6b21\u306e\u3088\u3046\u306b\u521d\u671f\u5316\u3057\u307e\u3059\u3002<span translate=no>_^_0_^_</span></p>\n",
 "<p>It maps the random latent vector (<span translate=no>_^_0_^_</span>)  into a different latent space (<span translate=no>_^_1_^_</span>),  with an 8-layer neural network. This gives an intermediate latent space <span translate=no>_^_2_^_</span> where the factors of variations are more linear (disentangled).</p>\n": "<p>\u30e9\u30f3\u30c0\u30e0\u306a\u6f5c\u5728\u30d9\u30af\u30c8\u30eb\uff08<span translate=no>_^_0_^_</span>\uff09\u3092\u30018\u5c64\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u7528\u3057\u3066\u5225\u306e\u6f5c\u5728\u7a7a\u9593\uff08<span translate=no>_^_1_^_</span>\uff09\u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u5909\u52d5\u306e\u8981\u56e0\u304c\u3088\u308a\u76f4\u7dda\u7684\u306a\uff08<span translate=no>_^_2_^_</span>\u3082\u3064\u308c\u304c\u89e3\u304b\u308c\u305f\uff09\u4e2d\u9593\u7684\u306a\u6f5c\u5728\u7a7a\u9593\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p>\n",
 "<p>Layer to convert RGB image to a feature map with <span translate=no>_^_0_^_</span> number of features. </p>\n": "<p>RGB <span translate=no>_^_0_^_</span> \u753b\u50cf\u3092\u591a\u6570\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u3092\u542b\u3080\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u306b\u5909\u63db\u3059\u308b\u30ec\u30a4\u30e4\u30fc\u3002</p>\n",
 "<p>Leaky Relu </p>\n": "<p>\u30ea\u30fc\u30ad\u30fc\u30ea\u30ec\u30fc</p>\n",
 "<p>Linear transformation </p>\n": "<p>\u7dda\u5f62\u5909\u63db</p>\n",
 "<p>Map <span translate=no>_^_0_^_</span> to <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u306b\u30de\u30c3\u30d4\u30f3\u30b0 <span translate=no>_^_1_^_</span></p>\n",
 "<p>Multiply the weights by <span translate=no>_^_0_^_</span> and return </p>\n": "<p>\u91cd\u307f\u5024\u3092\u639b\u3051\u3066\u8fd4\u3057\u307e\u3059\u3002<span translate=no>_^_0_^_</span></p>\n",
 "<p>Noise is made available to each block which helps the generator create more realistic images. Noise is scaled per channel by a learned weight.</p>\n": "<p>\u5404\u30d6\u30ed\u30c3\u30af\u306b\u30ce\u30a4\u30ba\u304c\u5165\u308b\u305f\u3081\u3001\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u306f\u3088\u308a\u30ea\u30a2\u30eb\u306a\u753b\u50cf\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\u30ce\u30a4\u30ba\u306f\u3001\u5b66\u7fd2\u3057\u305f\u91cd\u307f\u306b\u3088\u3063\u3066\u30c1\u30e3\u30f3\u30cd\u30eb\u3054\u3068\u306b\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3055\u308c\u307e\u3059</p>\u3002\n",
 "<p>Noise scale </p>\n": "<p>\u30ce\u30a4\u30ba\u30b9\u30b1\u30fc\u30eb</p>\n",
 "<p>Normalize <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30ce\u30fc\u30de\u30e9\u30a4\u30ba <span translate=no>_^_0_^_</span></p>\n",
 "<p>Normalize the kernel </p>\n": "<p>\u30ab\u30fc\u30cd\u30eb\u3092\u6b63\u898f\u5316</p>\n",
 "<p>Number of <a href=\"#discriminator_block\">discirminator blocks</a> </p>\n": "<p><a href=\"#discriminator_block\">\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u30d6\u30ed\u30c3\u30af\u306e\u6570</a></p>\n",
 "<p>Number of features after adding the standard deviations map </p>\n": "<p>\u6a19\u6e96\u504f\u5dee\u30de\u30c3\u30d7\u3092\u8ffd\u52a0\u3057\u305f\u5f8c\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u6570</p>\n",
 "<p>Number of generator blocks </p>\n": "<p>\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30d6\u30ed\u30c3\u30af\u6570</p>\n",
 "<p>Number of output features </p>\n": "<p>\u51fa\u529b\u6a5f\u80fd\u306e\u6570</p>\n",
 "<p>Number of steps calculated <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8a08\u7b97\u3055\u308c\u305f\u30b9\u30c6\u30c3\u30d7\u6570 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Padding layer </p>\n": "<p>\u30d1\u30c7\u30a3\u30f3\u30b0\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Padding size </p>\n": "<p>\u30d1\u30c7\u30a3\u30f3\u30b0\u30b5\u30a4\u30ba</p>\n",
 "<p>Path length regularization encourages a fixed-size step in <span translate=no>_^_0_^_</span> to result in a non-zero,  fixed-magnitude change in the generated image.</p>\n": "<p>\u30d1\u30b9\u9577\u306e\u6b63\u5247\u5316\u306b\u3088\u308a\u3001\u56fa\u5b9a\u30b5\u30a4\u30ba\u306e\u30b9\u30c6\u30c3\u30d7\u30a4\u30f3\u304c\u4fc3\u9032\u3055\u308c\u3001<span translate=no>_^_0_^_</span>\u751f\u6210\u3055\u308c\u308b\u30a4\u30e1\u30fc\u30b8\u306b\u30bc\u30ed\u4ee5\u5916\u306e\u56fa\u5b9a\u30de\u30b0\u30cb\u30c1\u30e5\u30fc\u30c9\u5909\u5316\u304c\u751f\u3058\u307e\u3059\u3002</p>\n",
 "<p>Progressive GAN generates high-resolution images (<span translate=no>_^_0_^_</span>) of size. It does so by <em>progressively</em> increasing the image size. First, it trains a network that produces a <span translate=no>_^_1_^_</span> image, then <span translate=no>_^_2_^_</span> ,  then an <span translate=no>_^_3_^_</span> image, and so on up to the desired image resolution.</p>\n": "<p>\u30d7\u30ed\u30b0\u30ec\u30c3\u30b7\u30d6 GAN \u306f\u3001\u30b5\u30a4\u30ba\u306e\u9ad8\u89e3\u50cf\u5ea6\u753b\u50cf (<span translate=no>_^_0_^_</span>) \u3092\u751f\u6210\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001<em>\u753b\u50cf\u30b5\u30a4\u30ba\u3092\u5f90\u3005\u306b\u5927\u304d\u304f\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u884c\u308f\u308c\u307e\u3059</em>\u3002\u307e\u305a\u3001<span translate=no>_^_1_^_</span>\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5b66\u7fd2\u3055\u305b\u3066\u753b\u50cf\u3092\u751f\u6210\u3057<span translate=no>_^_2_^_</span>\u3001<span translate=no>_^_3_^_</span>\u6b21\u306b\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\u3068\u3044\u3063\u305f\u5177\u5408\u306b\u3001\u76ee\u7684\u306e\u753b\u50cf\u89e3\u50cf\u5ea6\u307e\u3067\u5b66\u7fd2\u3055\u305b\u307e\u3059\u3002</p>\n",
 "<p>Regularize after first step </p>\n": "<p>\u6700\u521d\u306e\u4e00\u6b69\u3092\u8e0f\u307f\u51fa\u3057\u305f\u3089\u6b63\u5247\u5316</p>\n",
 "<p>Reshape <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5f62\u3092\u5909\u3048\u308b <span translate=no>_^_0_^_</span></p>\n",
 "<p>Reshape <span translate=no>_^_0_^_</span> to <span translate=no>_^_1_^_</span> and return </p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u5f62\u72b6\u3092\u5909\u66f4\u3057\u3066\u623b\u308b</p>\n",
 "<p>Reshape and return </p>\n": "<p>\u5f62\u3092\u5909\u3048\u3066\u623b\u308b</p>\n",
 "<p>Reshape for smoothening </p>\n": "<p>\u5f62\u3092\u5909\u3048\u3066\u306a\u3081\u3089\u304b\u306b</p>\n",
 "<p>Reshape gradients to calculate the norm </p>\n": "<p>\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u5f62\u3092\u5909\u3048\u3066\u30ce\u30eb\u30e0\u3092\u8a08\u7b97\u3057\u3088\u3046</p>\n",
 "<p>Reshape the scales </p>\n": "<p>\u4f53\u91cd\u8a08\u306e\u5f62\u3092\u5909\u3048\u3066</p>\n",
 "<p>Reshape weights </p>\n": "<p>\u30a6\u30a7\u30a4\u30c8\u306e\u5f62\u3092\u5909\u3048\u308b</p>\n",
 "<p>Return a dummy loss if we can&#x27;t calculate <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8a08\u7b97\u3067\u304d\u306a\u3044\u5834\u5408\u306f\u30c0\u30df\u30fc\u30ed\u30b9\u3092\u8fd4\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Return feature map and rgb image </p>\n": "<p>\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u3068 RGB \u30a4\u30e1\u30fc\u30b8\u3092\u8fd4\u3059</p>\n",
 "<p>Return the classification score </p>\n": "<p>\u5206\u985e\u30b9\u30b3\u30a2\u3092\u8fd4\u3059</p>\n",
 "<p>Return the final RGB image </p>\n": "<p>\u6700\u7d42\u7684\u306a RGB \u30a4\u30e1\u30fc\u30b8\u3092\u8fd4\u3059</p>\n",
 "<p>Return the loss <span translate=no>_^_0_^_</span> </p>\n": "<p>\u640d\u5931\u3092\u8fd4\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Return the penalty </p>\n": "<p>\u30da\u30ca\u30eb\u30c6\u30a3\u3092\u8fd4\u305b</p>\n",
 "<p>Run it through the <a href=\"#generator_block\">generator block</a> </p>\n": "<p><a href=\"#generator_block\">\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30d6\u30ed\u30c3\u30af\u306b\u901a\u3057\u3066\u304f\u3060\u3055\u3044</a></p>\n",
 "<p>Run through the <a href=\"#discriminator_block\">discriminator blocks</a> </p>\n": "<p><a href=\"#discriminator_block\">\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u30fb\u30d6\u30ed\u30c3\u30af\u3092\u304f\u3050\u308a\u629c\u3051\u308d</a></p>\n",
 "<p>Save kernel as a fixed parameter (no gradient updates) </p>\n": "<p>\u30ab\u30fc\u30cd\u30eb\u3092\u56fa\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066\u4fdd\u5b58 (\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u66f4\u65b0\u306a\u3057)</p>\n",
 "<p>Scale and add noise </p>\n": "<p>\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3068\u30ce\u30a4\u30ba\u306e\u8ffd\u52a0</p>\n",
 "<p>Scaled down </p>\n": "<p>\u30b9\u30b1\u30fc\u30eb\u30c0\u30a6\u30f3</p>\n",
 "<p>Scaling factor <span translate=no>_^_0_^_</span> after adding the residual </p>\n": "<p><span translate=no>_^_0_^_</span>\u6b8b\u5dee\u3092\u52a0\u3048\u305f\u5f8c\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u4fc2\u6570</p>\n",
 "<p>Second <a href=\"#style_block\">style block</a> </p>\n": "<p><a href=\"#style_block\">\u30bb\u30ab\u30f3\u30c9\u30b9\u30bf\u30a4\u30eb\u30d6\u30ed\u30c3\u30af</a></p>\n",
 "<p>Second style block with second noise tensor. The output is of shape <span translate=no>_^_0_^_</span> </p>\n": "<p>2 \u756a\u76ee\u306e\u30ce\u30a4\u30ba\u30c6\u30f3\u30bd\u30eb\u3092\u5099\u3048\u305f 2 \u756a\u76ee\u306e\u30b9\u30bf\u30a4\u30eb\u30d6\u30ed\u30c3\u30af\u3002\u51fa\u529b\u306f\u6574\u5f62\u3057\u3066\u3044\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Smoothen (blur) with the kernel </p>\n": "<p>\u30ab\u30fc\u30cd\u30eb\u306b\u3088\u308b\u30b9\u30e0\u30fc\u30b8\u30f3\u30b0 (\u307c\u304b\u3057)</p>\n",
 "<p>Smoothing layer </p>\n": "<p>\u30b9\u30e0\u30fc\u30b8\u30f3\u30b0\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Smoothing or blurring </p>\n": "<p>\u30b9\u30e0\u30fc\u30b8\u30f3\u30b0\u307e\u305f\u306f\u307c\u304b\u3057</p>\n",
 "<p>Split the samples into groups of <span translate=no>_^_0_^_</span>, we flatten the feature map to a single dimension since we want to calculate the standard deviation for each feature. </p>\n": "<p>\u5404\u7279\u5fb4\u306e\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u3057\u305f\u3044\u306e\u3067<span translate=no>_^_0_^_</span>\u3001\u30b5\u30f3\u30d7\u30eb\u3092\u30b0\u30eb\u30fc\u30d7\u306b\u5206\u3051\u3001\u7279\u5fb4\u30de\u30c3\u30d7\u3092 1 \u3064\u306e\u6b21\u5143\u306b\u5e73\u5766\u5316\u3057\u307e\u3059\u3002</p>\n",
 "<p>StyleGAN 2 changes both the generator and the discriminator of StyleGAN.</p>\n": "<p>\u30b9\u30bf\u30a4\u30eb\u30ac\u30f3 2 \u306f\u30b9\u30bf\u30a4\u30eb\u30ac\u30f3\u306e\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u3068\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u306e\u4e21\u65b9\u3092\u5909\u66f4\u3057\u307e\u3059\u3002</p>\n",
 "<p>StyleGAN improves the generator of Progressive GAN keeping the discriminator architecture the same.</p>\n": "<p>StyleGAN\u306f\u3001\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u540c\u3058\u306b\u4fdd\u3061\u306a\u304c\u3089\u3001\u30d7\u30ed\u30b0\u30ec\u30c3\u30b7\u30d6GAN\u306e\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u3092\u6539\u826f\u3057\u307e\u3057\u305f\u3002</p>\n",
 "<p>StyleGAN2 uses residual connections (with down-sampling) in the discriminator and skip connections  in the generator with up-sampling  (the RGB outputs from each layer are added - no residual connections in feature maps). They show that with experiments that the contribution of low-resolution layers is higher  at beginning of the training and then high-resolution layers take over.</p>\n": "<p>StyleGAN2 \u306f\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u306b\u6b8b\u7559\u63a5\u7d9a (\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3042\u308a) \u3092\u4f7f\u7528\u3057\u3001\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u3067\u306f\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3067\u306f\u30b9\u30ad\u30c3\u30d7\u30b3\u30cd\u30af\u30b7\u30e7\u30f3\u3092\u4f7f\u7528\u3057\u307e\u3059 (\u5404\u30ec\u30a4\u30e4\u30fc\u306e RGB \u51fa\u529b\u304c\u8ffd\u52a0\u3055\u308c\u308b\u305f\u3081\u3001\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u306b\u6b8b\u7559\u63a5\u7d9a\u306f\u3042\u308a\u307e\u305b\u3093)\u3002\u5b9f\u9a13\u3092\u884c\u3063\u305f\u3068\u3053\u308d\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u958b\u59cb\u6642\u306b\u306f\u4f4e\u89e3\u50cf\u5ea6\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u5bc4\u4e0e\u5ea6\u304c\u9ad8\u304f\u3001\u305d\u306e\u5f8c\u306f\u9ad8\u89e3\u50cf\u5ea6\u306e\u30ec\u30a4\u30e4\u30fc\u304c\u5f15\u304d\u7d99\u3050\u3053\u3068\u304c\u793a\u3055\u308c\u3066\u3044\u307e\u3059</p>\u3002\n",
 "<p>The discriminator is a mirror image of the generator network. The progressive growth of the discriminator is done similarly.</p>\n": "<p>\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u306f\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30df\u30e9\u30fc\u30a4\u30e1\u30fc\u30b8\u3067\u3059\u3002\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u306e\u6f38\u9032\u7684\u306a\u6210\u9577\u3082\u540c\u69d8\u306b\u884c\u308f\u308c\u307e\u3059</p>\u3002\n",
 "<p>The first style block </p>\n": "<p>\u6700\u521d\u306e\u30b9\u30bf\u30a4\u30eb\u30d6\u30ed\u30c3\u30af</p>\n",
 "<p>Then <span translate=no>_^_0_^_</span> is transformed into two vectors (<strong>styles</strong>) per layer,  <span translate=no>_^_1_^_</span>, <span translate=no>_^_2_^_</span> and used for scaling and shifting (biasing)  in each layer with <span translate=no>_^_3_^_</span> operator (normalize and scale): <span translate=no>_^_4_^_</span></p>\n": "<p>\u6b21\u306b\u3001<span translate=no>_^_0_^_</span>\u30ec\u30a4\u30e4\u30fc\u3054\u3068\u306b2\u3064\u306e\u30d9\u30af\u30c8\u30eb\uff08<strong>\u30b9\u30bf\u30a4\u30eb</strong>\uff09\u306b\u5909\u63db\u3055\u308c\u3001<span translate=no>_^_1_^_</span><span translate=no>_^_3_^_</span>\u6f14\u7b97\u5b50\uff08\u6b63\u898f\u5316\u3068\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\uff09\u3092\u4f7f\u7528\u3057\u3066\u5404\u30ec\u30a4\u30e4\u30fc\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3068\u30b7\u30d5\u30c8\uff08\u30d0\u30a4\u30a2\u30b9\uff09\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002<span translate=no>_^_2_^_</span> <span translate=no>_^_4_^_</span></p>\n",
 "<p>Then the convolution weights <span translate=no>_^_0_^_</span> are modulated as follows. (<span translate=no>_^_1_^_</span> here on refers to weights not intermediate latent space,  we are sticking to the same notation as the paper.)</p>\n": "<p>\u6b21\u306b\u3001<span translate=no>_^_0_^_</span>\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u306e\u91cd\u307f\u306f\u6b21\u306e\u3088\u3046\u306b\u5909\u8abf\u3055\u308c\u307e\u3059\u3002\uff08<span translate=no>_^_1_^_</span>\u3053\u3053\u3067\u306f\u4e2d\u9593\u306e\u6f5c\u5728\u7a7a\u9593\u3067\u306f\u306a\u304f\u91cd\u307f\u3092\u6307\u3057\u307e\u3059\u3002\u8ad6\u6587\u3068\u540c\u3058\u8868\u8a18\u6cd5\u306b\u3053\u3060\u308f\u3063\u3066\u3044\u307e\u3059</p>\u3002\uff09\n",
 "<p>They remove the <span translate=no>_^_0_^_</span> operator and replace it with  the weight modulation and demodulation step. This is supposed to improve what they call droplet artifacts that are present in generated images,  which are caused by the normalization in <span translate=no>_^_1_^_</span> operator. Style vector per layer is calculated from <span translate=no>_^_2_^_</span> as <span translate=no>_^_3_^_</span>.</p>\n": "<p><span translate=no>_^_0_^_</span>\u30aa\u30da\u30ec\u30fc\u30bf\u3092\u53d6\u308a\u5916\u3057\u3066\u3001\u91cd\u307f\u5909\u8abf\u3068\u5fa9\u8abf\u306e\u30b9\u30c6\u30c3\u30d7\u306b\u7f6e\u304d\u63db\u3048\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u6f14\u7b97\u5b50\u306e\u6b63\u898f\u5316\u306b\u3088\u3063\u3066\u751f\u6210\u3055\u308c\u308b\u753b\u50cf\u306b\u5b58\u5728\u3059\u308b\u3001\u3044\u308f\u3086\u308b\u30c9\u30ed\u30c3\u30d7\u30ec\u30c3\u30c8\u30a2\u30fc\u30c6\u30a3\u30d5\u30a1\u30af\u30c8\u3092\u6539\u5584\u3059\u308b\u305f\u3081\u306e\u3082\u306e\u3067\u3059\u3002<span translate=no>_^_1_^_</span>\u30ec\u30a4\u30e4\u30fc\u3054\u3068\u306e\u30b9\u30bf\u30a4\u30eb\u30d9\u30af\u30c8\u30eb\u306f\u3001<span translate=no>_^_2_^_</span>\u304b\u3089\u8a08\u7b97\u3055\u308c\u307e\u3059<span translate=no>_^_3_^_</span>\u3002</p>\n",
 "<p>They use <strong>minibatch standard deviation</strong> to increase variation and  <strong>equalized learning rate</strong> which we discussed below in the implementation. They also use <strong>pixel-wise normalization</strong> where at each pixel the feature vector is normalized. They apply this to all the convolution layer outputs (except RGB).</p>\n": "<p><strong>\u30df\u30cb\u30d0\u30c3\u30c1\u6a19\u6e96\u504f\u5dee\u3092\u4f7f\u7528\u3057\u3066\u5909\u52d5\u3092\u5897\u3084\u3057</strong>\u3001<strong>\u5b66\u7fd2\u7387\u3092\u5747\u7b49\u5316\u3057\u307e\u3059</strong>\u3002\u3053\u308c\u306b\u3064\u3044\u3066\u306f\u3001\u5b9f\u88c5\u3067\u5f8c\u8ff0\u3057\u307e\u3059\u3002\u307e\u305f\u3001<strong>\u30d4\u30af\u30bb\u30eb\u5358\u4f4d\u306e\u6b63\u898f\u5316\u3082\u4f7f\u7528\u3057\u3066\u304a\u308a</strong>\u3001\u5404\u30d4\u30af\u30bb\u30eb\u3067\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u304c\u6b63\u898f\u5316\u3055\u308c\u307e\u3059\u3002\u3053\u308c\u3092\u3059\u3079\u3066\u306e\u7573\u307f\u8fbc\u307f\u5c64\u51fa\u529b (RGB \u3092\u9664\u304f) \u306b\u9069\u7528\u3057\u307e\u3059</p>\u3002\n",
 "<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper  <a href=\"https://arxiv.org/abs/1912.04958\">Analyzing and Improving the Image Quality of StyleGAN</a>  which introduces <strong>StyleGAN 2</strong>. StyleGAN 2 is an improvement over <strong>StyleGAN</strong> from the paper  <a href=\"https://arxiv.org/abs/1812.04948\">A Style-Based Generator Architecture for Generative Adversarial Networks</a>. And StyleGAN is based on <strong>Progressive GAN</strong> from the paper  <a href=\"https://arxiv.org/abs/1710.10196\">Progressive Growing of GANs for Improved Quality, Stability, and Variation</a>. All three papers are from the same authors from <a href=\"https://twitter.com/NVIDIAAI\">NVIDIA AI</a>.</p>\n": "<p><strong>\u3053\u308c\u306f\u3001<a href=\"https://arxiv.org/abs/1912.04958\">StyleGan 2\u3092\u7d39\u4ecb\u3059\u308b\u8ad6\u6587\u300c\u30b9\u30bf\u30a4\u30eb\u30ac\u30f3\u306e\u753b\u8cea\u306e\u5206\u6790\u3068\u6539\u5584\u300d<a href=\"https://pytorch.org\">\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a></a>\u3002</strong>StyleGan 2\u306f\u3001\u8ad6\u6587\u300c<strong><a href=\"https://arxiv.org/abs/1812.04948\">\u6575\u5bfe\u7684\u751f\u6210\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u305f\u3081\u306e\u30b9\u30bf\u30a4\u30eb\u30d9\u30fc\u30b9\u306e\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u300d\u306eStyleGAN\u3092\u6539\u826f\u3057\u305f\u3082\u306e\u3067\u3059</a></strong>\u3002\u307e\u305f\u3001StyleGan\u306f\u8ad6\u6587\u300c<strong>GAN\u306e\u6f38\u9032\u7684\u6210\u9577\u306b\u3088\u308b\u54c1\u8cea</strong><a href=\"https://arxiv.org/abs/1710.10196\">\u3001\u5b89\u5b9a\u6027\u3001\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\u306e\u5411\u4e0a\u300d\u306e\u30d7\u30ed\u30b0\u30ec\u30c3\u30b7\u30d6GAN\u3092\u30d9\u30fc\u30b9\u306b\u3057\u3066\u3044\u307e\u3059</a>\u30023 \u3064\u306e\u8ad6\u6587\u306f\u3059\u3079\u3066 <a href=\"https://twitter.com/NVIDIAAI\">NVIDIA</a> AI \u306e\u540c\u3058\u8457\u8005\u306b\u3088\u308b\u3082\u306e\u3067\u3059</p>\u3002\n",
 "<p>To prevent the generator from assuming adjacent styles are correlated,  they randomly use different styles for different blocks. That is, they sample two latent vectors <span translate=no>_^_0_^_</span> and corresponding <span translate=no>_^_1_^_</span> and  use <span translate=no>_^_2_^_</span> based styles for some blocks and <span translate=no>_^_3_^_</span> based styles for some blacks randomly.</p>\n": "<p>\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u304c\u96a3\u63a5\u3059\u308b\u30b9\u30bf\u30a4\u30eb\u304c\u76f8\u4e92\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u3068\u898b\u306a\u3055\u306a\u3044\u3088\u3046\u306b\u3001\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u306f\u30d6\u30ed\u30c3\u30af\u3054\u3068\u306b\u7570\u306a\u308b\u30b9\u30bf\u30a4\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001<span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> 2\u3064\u306e\u6f5c\u5728\u30d9\u30af\u30c8\u30eb\u3068\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\u3082\u306e\u3092\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3001<span translate=no>_^_2_^_</span>\u4e00\u90e8\u306e\u30d6\u30ed\u30c3\u30af\u306b\u306f\u30d9\u30fc\u30b9\u30b9\u30bf\u30a4\u30eb\u3092\u4f7f\u7528\u3057\u3001<span translate=no>_^_3_^_</span>\u4e00\u90e8\u306e\u9ed2\u4eba\u306b\u306f\u30d9\u30fc\u30b9\u30b9\u30bf\u30a4\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4f7f\u7528\u3057\u307e\u3059</p>\u3002\n",
 "<p>Trainable <span translate=no>_^_0_^_</span> constant </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u5b9a\u6570 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Try to normalize the image (this is totally optional, but sped up the early training a little) </p>\n": "<p>\u753b\u50cf\u3092\u6b63\u898f\u5316\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\uff08\u3053\u308c\u306f\u5b8c\u5168\u306b\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u3059\u304c\u3001\u521d\u671f\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u5c11\u3057\u30b9\u30d4\u30fc\u30c9\u30a2\u30c3\u30d7\u3067\u304d\u307e\u3059\uff09</p>\n",
 "<p>Two <span translate=no>_^_0_^_</span> convolutions </p>\n": "<p>2 <span translate=no>_^_0_^_</span> \u3064\u306e\u7573\u307f\u8fbc\u307f</p>\n",
 "<p>Up sample the RGB image and add to the rgb from the block </p>\n": "<p>RGB \u30a4\u30e1\u30fc\u30b8\u3092\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3001\u30d6\u30ed\u30c3\u30af\u304b\u3089 RGB \u306b\u8ffd\u52a0\u3057\u307e\u3059</p>\n",
 "<p>Up sample the feature map </p>\n": "<p>\u30d5\u30a3\u30fc\u30c1\u30e3\u30fc\u30de\u30c3\u30d7\u3092\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0</p>\n",
 "<p>Up-sample and smoothen </p>\n": "<p>\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3068\u30b9\u30e0\u30fc\u30b8\u30f3\u30b0</p>\n",
 "<p>Up-sampling layer </p>\n": "<p>\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Update exponential sum </p>\n": "<p>\u6307\u6570\u548c\u3092\u66f4\u65b0</p>\n",
 "<p>Use grouped convolution to efficiently calculate the convolution with sample wise kernel. i.e. we have a different kernel (weights) for each sample in the batch </p>\n": "<p>\u30b0\u30eb\u30fc\u30d7\u5316\u3055\u308c\u305f\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u30b5\u30f3\u30d7\u30eb\u3054\u3068\u306e\u30ab\u30fc\u30cd\u30eb\u3067\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u3092\u52b9\u7387\u7684\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u30d0\u30c3\u30c1\u5185\u306e\u30b5\u30f3\u30d7\u30eb\u3054\u3068\u306b\u7570\u306a\u308b\u30ab\u30fc\u30cd\u30eb\uff08\u91cd\u307f\uff09\u304c\u3042\u308a\u307e\u3059</p>\n",
 "<p>We&#x27;ll first introduce the three papers at a high level.</p>\n": "<p>\u307e\u305a\u30013\u3064\u306e\u8ad6\u6587\u3092\u5927\u307e\u304b\u306b\u7d39\u4ecb\u3057\u307e\u3059\u3002</p>\n",
 "<p>Weight modulated convolution </p>\n": "<p>\u91cd\u307f\u5909\u8abf\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3</p>\n",
 "<p>Weight modulated convolution layer </p>\n": "<p>\u91cd\u307f\u5909\u8abf\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u5c64</p>\n",
 "<p>Weight modulated convolution layer without demodulation </p>\n": "<p>\u5fa9\u8abf\u306a\u3057\u306e\u91cd\u307f\u5909\u8abf\u7573\u307f\u8fbc\u307f\u5c64</p>\n",
 "<p>Weight multiplication coefficient </p>\n": "<p>\u91cd\u91cf\u4e57\u7b97\u4fc2\u6570</p>\n",
 "<p>Whether to normalize weights </p>\n": "<p>\u30a6\u30a7\u30a4\u30c8\u3092\u6b63\u898f\u5316\u3059\u308b\u304b\u3069\u3046\u304b</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is <span translate=no>_^_3_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f <span translate=no>_^_1_^_</span></li>\n</ul><li><span translate=no>_^_2_^_</span>\u306f <span translate=no>_^_3_^_</span></li>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span>. In order to mix-styles (use different <span translate=no>_^_2_^_</span> for different layers), we provide a separate <span translate=no>_^_3_^_</span> for each <a href=\"#generator_block\">generator block</a>. It has shape <span translate=no>_^_4_^_</span>. </li>\n<li><span translate=no>_^_5_^_</span> is the noise for each block. It&#x27;s a list of pairs of noise sensors because each block (except the initial) has two noise inputs after each convolution layer (see the diagram).</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u3067\u3059<span translate=no>_^_1_^_</span>\u3002\u30b9\u30bf\u30a4\u30eb\u3092\u30df\u30c3\u30af\u30b9\u3059\u308b\u305f\u3081\u306b\uff08<span translate=no>_^_2_^_</span>\u30ec\u30a4\u30e4\u30fc\u3054\u3068\u306b\u7570\u306a\u308b\u30b9\u30bf\u30a4\u30eb\u3092\u4f7f\u7528\uff09\u3001<span translate=no>_^_3_^_</span><a href=\"#generator_block\">\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u30d6\u30ed\u30c3\u30af\u3054\u3068\u306b\u500b\u5225\u306e\u30b9\u30bf\u30a4\u30eb\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u307e\u3059</a>\u3002\u5f62\u304c\u3042\u308a\u307e\u3059<span translate=no>_^_4_^_</span>\u3002</li>\n<li><span translate=no>_^_5_^_</span>\u306f\u5404\u30d6\u30ed\u30c3\u30af\u306e\u30ce\u30a4\u30ba\u3067\u3059\u3002\u5404\u30d6\u30ed\u30c3\u30af (\u6700\u521d\u306e\u30d6\u30ed\u30c3\u30af\u3092\u9664\u304f) \u306b\u306f\u5404\u7573\u307f\u8fbc\u307f\u5c64\u306e\u5f8c\u306b 2 \u3064\u306e\u30ce\u30a4\u30ba\u5165\u529b\u304c\u3042\u308b\u305f\u3081\u3001\u3053\u308c\u306f\u30ce\u30a4\u30ba\u30bb\u30f3\u30b5\u30fc\u306e\u30da\u30a2\u306e\u30ea\u30b9\u30c8\u3067\u3059 (\u56f3\u3092\u53c2\u7167</li></ul>)\u3002\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the <span translate=no>_^_1_^_</span> of image resolution </li>\n<li><span translate=no>_^_2_^_</span> is the dimensionality of <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> number of features in the convolution layer at the highest resolution (final block) </li>\n<li><span translate=no>_^_5_^_</span> maximum number of features in any generator block</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u306f\u753b\u50cf\u89e3\u50cf\u5ea6\u306e</li>\n<li><span translate=no>_^_2_^_</span>\u306e\u6b21\u5143\u3067\u3059 <span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span>\u6700\u5927\u89e3\u50cf\u5ea6\u3067\u306e\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u6570 (\u6700\u7d42\u30d6\u30ed\u30c3\u30af)</li>\n<li><span translate=no>_^_5_^_</span>\u4efb\u610f\u306e\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30d6\u30ed\u30c3\u30af\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6700\u5927\u6570</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the <span translate=no>_^_1_^_</span> of image resolution </li>\n<li><span translate=no>_^_2_^_</span> number of features in the convolution layer at the highest resolution (first block) </li>\n<li><span translate=no>_^_3_^_</span> maximum number of features in any generator block</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u306f\u753b\u50cf\u89e3\u50cf\u5ea6\u306e</li>\n<li><span translate=no>_^_2_^_</span>\u6700\u5927\u89e3\u50cf\u5ea6\u3067\u306e\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u6570 (1 \u756a\u76ee\u306e\u30d6\u30ed\u30c3\u30af)</li>\n<li><span translate=no>_^_3_^_</span>\u4efb\u610f\u306e\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30d6\u30ed\u30c3\u30af\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6700\u5927\u6570</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the batch of <span translate=no>_^_1_^_</span> of shape <span translate=no>_^_2_^_</span> </li>\n<li><span translate=no>_^_3_^_</span> are the generated images of shape <span translate=no>_^_4_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u5f62\u72b6\u306e\u30d0\u30c3\u30c1\u3067\u3059 <span translate=no>_^_2_^_</span></li>\n<li><span translate=no>_^_3_^_</span>\u751f\u6210\u3055\u308c\u305f\u5f62\u72b6\u306e\u753b\u50cf\u3067\u3059 <span translate=no>_^_4_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the constant <span translate=no>_^_1_^_</span> used to calculate the exponential moving average <span translate=no>_^_2_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u6307\u6570\u79fb\u52d5\u5e73\u5747\u306e\u8a08\u7b97\u306b\u4f7f\u7528\u3055\u308c\u308b\u5b9a\u6570\u3067\u3059 <span translate=no>_^_2_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the dimensionality of <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the number of features in the feature map</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306e\u6b21\u5143\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u306f\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the dimensionality of <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the number of features in the input feature map </li>\n<li><span translate=no>_^_3_^_</span> is the number of features in the output feature map</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306e\u6b21\u5143\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u306f\u5165\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059</li>\n<li><span translate=no>_^_3_^_</span>\u306f\u51fa\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the feature map</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u6a5f\u80fd\u30de\u30c3\u30d7\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input feature map of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is <span translate=no>_^_3_^_</span> with shape <span translate=no>_^_4_^_</span> </li>\n<li><span translate=no>_^_5_^_</span> is a tensor of shape <span translate=no>_^_6_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u30b7\u30a7\u30a4\u30d7\u306e\u5165\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span>\u5f62\u4ed8\u304d\u3067\u3059 <span translate=no>_^_4_^_</span></li>\n<li><span translate=no>_^_5_^_</span>\u5f62\u72b6\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_6_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input feature map of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is <span translate=no>_^_3_^_</span> with shape <span translate=no>_^_4_^_</span> </li>\n<li><span translate=no>_^_5_^_</span> is a tuple of two noise tensors of shape <span translate=no>_^_6_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u30b7\u30a7\u30a4\u30d7\u306e\u5165\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span>\u5f62\u4ed8\u304d\u3067\u3059 <span translate=no>_^_4_^_</span></li>\n<li><span translate=no>_^_5_^_</span>\u5f62\u72b6\u306e2\u3064\u306e\u30ce\u30a4\u30ba\u30c6\u30f3\u30bd\u30eb\u306e\u30bf\u30d7\u30eb\u3067\u3059 <span translate=no>_^_6_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input feature map of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is <span translate=no>_^_3_^_</span> with shape <span translate=no>_^_4_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u30b7\u30a7\u30a4\u30d7\u306e\u5165\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span>\u5f62\u4ed8\u304d\u3067\u3059 <span translate=no>_^_4_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input feature map of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is style based scaling tensor of shape <span translate=no>_^_3_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u30b7\u30a7\u30a4\u30d7\u306e\u5165\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u30b9\u30bf\u30a4\u30eb\u30d9\u30fc\u30b9\u306e\u5f62\u72b6\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_3_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input image of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u5f62\u72b6\u306e\u5165\u529b\u753b\u50cf\u3067\u3059 <span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features in <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> </li>\n<li><span translate=no>_^_3_^_</span> is the number of layers in the mapping network.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u306f\u304a\u3088\u3073\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059 <span translate=no>_^_2_^_</span></li>\n<li><span translate=no>_^_3_^_</span>\u306f\u3001\u30de\u30c3\u30d4\u30f3\u30b0\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5185\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u6570\u3067\u3059\u3002</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features in the input feature map </li>\n<li><span translate=no>_^_1_^_</span> is the number of features in the output feature map </li>\n<li><span translate=no>_^_2_^_</span> is the bias initialization constant</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u5165\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span>\u306f\u51fa\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059</li>\n<li><span translate=no>_^_2_^_</span>\u306f\u30d0\u30a4\u30a2\u30b9\u521d\u671f\u5316\u5b9a\u6570\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features in the input feature map </li>\n<li><span translate=no>_^_1_^_</span> is the number of features in the output feature map </li>\n<li><span translate=no>_^_2_^_</span> is the size of the convolution kernel </li>\n<li><span translate=no>_^_3_^_</span> is flag whether to normalize weights by its standard deviation </li>\n<li><span translate=no>_^_4_^_</span> is the <span translate=no>_^_5_^_</span> for normalizing</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u5165\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span>\u306f\u51fa\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059</li>\n<li><span translate=no>_^_2_^_</span>\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u30ab\u30fc\u30cd\u30eb\u306e\u30b5\u30a4\u30ba\u3067\u3059</li>\n<li><span translate=no>_^_3_^_</span>\u91cd\u307f\u3092\u305d\u306e\u6a19\u6e96\u504f\u5dee\u3067\u6b63\u898f\u5316\u3059\u308b\u304b\u3069\u3046\u304b\u304c\u30d5\u30e9\u30b0</li>\n<li><span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\u6b63\u898f\u5316\u7528\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features in the input feature map </li>\n<li><span translate=no>_^_1_^_</span> is the number of features in the output feature map </li>\n<li><span translate=no>_^_2_^_</span> is the size of the convolution kernel </li>\n<li><span translate=no>_^_3_^_</span> is the padding to be added on both sides of each size dimension</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u5165\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span>\u306f\u51fa\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059</li>\n<li><span translate=no>_^_2_^_</span>\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u30ab\u30fc\u30cd\u30eb\u306e\u30b5\u30a4\u30ba\u3067\u3059</li>\n<li><span translate=no>_^_3_^_</span>\u5404\u30b5\u30a4\u30ba\u5bf8\u6cd5\u306e\u4e21\u5074\u306b\u8ffd\u52a0\u3059\u308b\u30d1\u30c7\u30a3\u30f3\u30b0\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features in the input feature map </li>\n<li><span translate=no>_^_1_^_</span> is the number of features in the output feature map</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u5165\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span>\u306f\u51fa\u529b\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of samples to calculate standard deviation across.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u3059\u308b\u30b5\u30f3\u30d7\u30eb\u306e\u6570\u3067\u3059\u3002</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the shape of the weight parameter</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u91cd\u307f\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5f62\u72b6\u3067\u3059</li></ul>\n",
 "An annotated PyTorch implementation of StyleGAN2.": "\u30b9\u30bf\u30a4\u30ebGAN2\u306e\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u4ed8\u304dPyTorch\u5b9f\u88c5\u3002",
 "StyleGAN 2": "\u30b9\u30bf\u30a4\u30eb\u30ac\u30f3 2"
}