{
 "<h1>Generative Adversarial Networks (GAN)</h1>\n<p>This is an implementation of <a href=\"https://arxiv.org/abs/1406.2661\">Generative Adversarial Networks</a>.</p>\n<p>The generator, <span translate=no>_^_0_^_</span> generates samples that match the distribution of data, while the discriminator, <span translate=no>_^_1_^_</span> gives the probability that <span translate=no>_^_2_^_</span> came from data rather than <span translate=no>_^_3_^_</span>.</p>\n<p>We train <span translate=no>_^_4_^_</span> and <span translate=no>_^_5_^_</span> simultaneously on a two-player min-max game with value function <span translate=no>_^_6_^_</span>.</p>\n<p><span translate=no>_^_7_^_</span></p>\n<p><span translate=no>_^_8_^_</span> is the probability distribution over data, whilst <span translate=no>_^_9_^_</span> probability distribution of <span translate=no>_^_10_^_</span>, which is set to gaussian noise.</p>\n<p>This file defines the loss functions. <a href=\"experiment.html\">Here</a> is an MNIST example with two multilayer perceptron for the generator and discriminator.</p>\n": "<h1>\u751f\u6210\u5bf9\u6297\u7f51\u7edc (GAN)</h1>\n<p>\u8fd9\u662f<a href=\"https://arxiv.org/abs/1406.2661\">\u751f\u6210\u5bf9\u6297\u7f51\u7edc</a>\u7684\u5b9e\u73b0\u3002</p>\n\u751f\u6210@@ <p>\u5668<span translate=no>_^_0_^_</span>\u751f\u6210\u4e0e\u6570\u636e\u5206\u5e03\u76f8\u5339\u914d\u7684\u6837\u672c\uff0c\u800c\u9274\u522b\u5668\u5219<span translate=no>_^_1_^_</span>\u7ed9\u51fa\u6765\u81ea\u6570\u636e\u800c\u4e0d\u662f<span translate=no>_^_2_^_</span>\u6765\u81ea\u6570\u636e\u7684\u6982\u7387<span translate=no>_^_3_^_</span>\u3002</p>\n<p>\u6211\u4eec\u5728\u5177\u6709\u503c\u529f\u80fd\u7684\u53cc\u4eba\u6700\u5c0f\u6700\u5927\u6e38\u620f\u4e2d<span translate=no>_^_5_^_</span>\u540c\u65f6\u8fdb\u884c\u8bad\u7ec3<span translate=no>_^_4_^_</span><span translate=no>_^_6_^_</span>\u3002</p>\n<p><span translate=no>_^_7_^_</span></p>\n<p><span translate=no>_^_8_^_</span>\u662f\u6570\u636e\u7684\u6982\u7387\u5206\u5e03\uff0c\u800c<span translate=no>_^_9_^_</span>\u6982\u7387\u5206<span translate=no>_^_10_^_</span>\u5e03\u5219\u8bbe\u7f6e\u4e3a\u9ad8\u65af\u566a\u58f0\u3002</p>\n<p>\u8fd9\u4e2a\u6587\u4ef6\u5b9a\u4e49\u4e86\u635f\u5931\u51fd\u6570\u3002<a href=\"experiment.html\">\u8fd9\u662f</a>\u4e00\u4e2a MNIST \u793a\u4f8b\uff0c\u5176\u4e2d\u5305\u542b\u4e24\u4e2a\u7528\u4e8e\u751f\u6210\u5668\u548c\u9274\u522b\u5668\u7684\u591a\u5c42\u611f\u77e5\u5668\u3002</p>\n",
 "<h2>Discriminator Loss</h2>\n<p>Discriminator should <strong>ascend</strong> on the gradient,</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p><span translate=no>_^_1_^_</span> is the mini-batch size and <span translate=no>_^_2_^_</span> is used to index samples in the mini-batch. <span translate=no>_^_3_^_</span> are samples from <span translate=no>_^_4_^_</span> and <span translate=no>_^_5_^_</span> are samples from <span translate=no>_^_6_^_</span>.</p>\n": "<h2>\u9274\u522b\u5668\u4e22\u5931</h2>\n<p>\u9274\u522b\u5668\u5e94\u8be5\u5728\u68af\u5ea6\u4e0a<strong>\u5347</strong>\uff0c</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p><span translate=no>_^_1_^_</span>\u662f\u5fae\u578b\u6279\u6b21\u5927\u5c0f\uff0c<span translate=no>_^_2_^_</span>\u7528\u4e8e\u7d22\u5f15\u5fae\u578b\u6279\u6b21\u4e2d\u7684\u6837\u672c\u3002<span translate=no>_^_3_^_</span>\u662f\u6765\u81ea\u7684\u6837\u672c<span translate=no>_^_4_^_</span>\uff0c<span translate=no>_^_5_^_</span>\u4e5f\u662f\u6765\u81ea\u7684\u6837\u672c<span translate=no>_^_6_^_</span>\u3002</p>\n",
 "<h2>Generator Loss</h2>\n<p>Generator should <strong>descend</strong> on the gradient,</p>\n<p><span translate=no>_^_0_^_</span></p>\n": "<h2>\u53d1\u7535\u673a\u635f\u5931</h2>\n<p>\u53d1\u7535\u673a\u5e94\u8be5<strong>\u4e0b\u964d\u5230</strong>\u68af\u5ea6\u4e0a\uff0c</p>\n<p><span translate=no>_^_0_^_</span></p>\n",
 "<p> <span translate=no>_^_0_^_</span> are logits from <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> are logits from <span translate=no>_^_3_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span>\u662f logits \u6765\u81ea<span translate=no>_^_1_^_</span>\uff0c<span translate=no>_^_2_^_</span>logits \u6765\u81ea<span translate=no>_^_3_^_</span></p>\n",
 "<p> Create smoothed labels</p>\n": "<p>\u521b\u5efa\u7ecf\u8fc7\u5e73\u6ed1\u5904\u7406\u7684\u6807\u6ce8</p>\n",
 "<p>Labels are registered as buffered and persistence is set to <span translate=no>_^_0_^_</span>. </p>\n": "<p>\u6807\u7b7e\u6ce8\u518c\u4e3a\u7f13\u51b2\u533a\uff0c\u5e76\u5c06\u6301\u4e45\u6027\u8bbe\u7f6e\u4e3a<span translate=no>_^_0_^_</span>\u3002</p>\n",
 "<p>We use PyTorch Binary Cross Entropy Loss, which is <span translate=no>_^_0_^_</span>, where <span translate=no>_^_1_^_</span> are the labels and <span translate=no>_^_2_^_</span> are the predictions. <em>Note the negative sign</em>. We use labels equal to <span translate=no>_^_3_^_</span> for <span translate=no>_^_4_^_</span> from <span translate=no>_^_5_^_</span> and labels equal to <span translate=no>_^_6_^_</span> for <span translate=no>_^_7_^_</span> from <span translate=no>_^_8_^_</span> Then descending on the sum of these is the same as ascending on the above gradient.</p>\n<p><span translate=no>_^_9_^_</span> combines softmax and binary cross entropy loss. </p>\n": "<p>\u6211\u4eec\u4f7f\u7528 PyTorch \u4e8c\u8fdb\u5236\u4ea4\u53c9\u71b5\u635f\u5931<span translate=no>_^_0_^_</span>\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u6807\u7b7e\u5728<span translate=no>_^_1_^_</span>\u54ea\u91cc\uff0c\u9884\u6d4b\u5728<span translate=no>_^_2_^_</span>\u54ea\u91cc\u3002<em>\u6ce8\u610f\u8d1f\u53f7</em>\u3002\u6211\u4eec\u4f7f\u7528\u7b49\u4e8e for fro<span translate=no>_^_3_^_</span> m<span translate=no>_^_4_^_</span> \u7684\u6807\u7b7e<span translate=no>_^_5_^_</span>\u548c\u7b49\u4e8e f<span translate=no>_^_6_^_</span> or from<span translate=no>_^_7_^_</span> \u7684\u6807\u7b7e<span translate=no>_^_8_^_</span>\u7136\u540e\u6309\u8fd9\u4e9b\u603b\u548c\u964d\u5e8f\u4e0e\u4e0a\u9762\u7684\u68af\u5ea6\u4e0a\u5347\u76f8\u540c\u3002</p>\n<p><span translate=no>_^_9_^_</span>\u7ed3\u5408\u4e86 softmax \u548c\u4e8c\u8fdb\u5236\u4ea4\u53c9\u71b5\u635f\u5931\u3002</p>\n",
 "<p>We use label smoothing because it seems to work better in some cases </p>\n": "<p>\u6211\u4eec\u4f7f\u7528\u6807\u7b7e\u5e73\u6ed1\uff0c\u56e0\u4e3a\u5b83\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6548\u679c\u66f4\u597d</p>\n",
 "<p>We use labels equal to <span translate=no>_^_0_^_</span> for <span translate=no>_^_1_^_</span> from <span translate=no>_^_2_^_</span> Then descending on this loss is the same as descending on the above gradient. </p>\n": "<p>\u6211\u4eec\u4f7f\u7528\u7b49\u4e8e f<span translate=no>_^_0_^_</span> or fro<span translate=no>_^_1_^_</span> m \u7684\u6807\u7b7e\uff0c<span translate=no>_^_2_^_</span>\u7136\u540e\u5728\u6b64\u635f\u5931\u4e0a\u964d\u5e8f\u4e0e\u4e0a\u9762\u68af\u5ea6\u4e0a\u7684\u964d\u5e8f\u76f8\u540c\u3002</p>\n",
 "A simple PyTorch implementation/tutorial of Generative Adversarial Networks (GAN) loss functions.": "\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u635f\u5931\u51fd\u6570\u7684\u7b80\u5355PyTorch\u5b9e\u73b0/\u6559\u7a0b\u3002",
 "Generative Adversarial Networks (GAN)": "\u751f\u6210\u5bf9\u6297\u7f51\u7edc (GAN)"
}