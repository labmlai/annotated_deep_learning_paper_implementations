{
 "<h1>Generative Adversarial Networks (GAN)</h1>\n<p>This is an implementation of <a href=\"https://arxiv.org/abs/1406.2661\">Generative Adversarial Networks</a>.</p>\n<p>The generator, <span translate=no>_^_0_^_</span> generates samples that match the distribution of data, while the discriminator, <span translate=no>_^_1_^_</span> gives the probability that <span translate=no>_^_2_^_</span> came from data rather than <span translate=no>_^_3_^_</span>.</p>\n<p>We train <span translate=no>_^_4_^_</span> and <span translate=no>_^_5_^_</span> simultaneously on a two-player min-max game with value function <span translate=no>_^_6_^_</span>.</p>\n<p><span translate=no>_^_7_^_</span></p>\n<p><span translate=no>_^_8_^_</span> is the probability distribution over data, whilst <span translate=no>_^_9_^_</span> probability distribution of <span translate=no>_^_10_^_</span>, which is set to gaussian noise.</p>\n<p>This file defines the loss functions. <a href=\"experiment.html\">Here</a> is an MNIST example with two multilayer perceptron for the generator and discriminator.</p>\n": "<h1>\u30b8\u30a7\u30cd\u30ec\u30fc\u30c6\u30a3\u30d6\u30fb\u30a2\u30c9\u30d0\u30fc\u30b5\u30ea\u30a2\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af (GAN)</h1>\n<p><a href=\"https://arxiv.org/abs/1406.2661\">\u3053\u308c\u306f\u30b8\u30a7\u30cd\u30ec\u30fc\u30c6\u30a3\u30d6\u30fb\u30a2\u30c9\u30d0\u30fc\u30b5\u30ea\u30a2\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</p>\n<p><span translate=no>_^_0_^_</span>\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u306f\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u306b\u4e00\u81f4\u3059\u308b\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210\u3057\u3001<span translate=no>_^_1_^_</span>\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u306f\u30c7\u30fc\u30bf\u304b\u3089\u5f97\u3089\u308c\u308b\u78ba\u7387\u3067\u306f\u306a\u304f\u3001<span translate=no>_^_2_^_</span>\u30c7\u30fc\u30bf\u304b\u3089\u5f97\u3089\u308c\u308b\u78ba\u7387\u3092\u8fd4\u3057\u307e\u3059\u3002<span translate=no>_^_3_^_</span></p>\n<p><span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\u30d0\u30ea\u30e5\u30fc\u6a5f\u80fd\u3092\u5099\u3048\u305f2\u4eba\u7528\u306e\u30df\u30cb\u30de\u30c3\u30af\u30b9\u30b2\u30fc\u30e0\u3067\u540c\u6642\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u307e\u3059\u3002<span translate=no>_^_6_^_</span></p>\n<p><span translate=no>_^_7_^_</span></p>\n<p><span translate=no>_^_8_^_</span>\u306f\u30c7\u30fc\u30bf\u5168\u4f53\u306e\u78ba\u7387\u5206\u5e03\u3067<span translate=no>_^_10_^_</span>\u3001<span translate=no>_^_9_^_</span>\u306e\u78ba\u7387\u5206\u5e03\u306f\u30ac\u30a6\u30b9\u30ce\u30a4\u30ba\u306b\u8a2d\u5b9a\u3055\u308c\u307e\u3059\u3002</p>\n<p>\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u306f\u640d\u5931\u95a2\u6570\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002<a href=\"experiment.html\">\u3053\u308c\u306f</a>\u3001\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u3068\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u306b2\u3064\u306e\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3092\u4f7f\u3063\u305fMNIST\u306e\u4f8b\u3067\u3059</p>\u3002\n",
 "<h2>Discriminator Loss</h2>\n<p>Discriminator should <strong>ascend</strong> on the gradient,</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p><span translate=no>_^_1_^_</span> is the mini-batch size and <span translate=no>_^_2_^_</span> is used to index samples in the mini-batch. <span translate=no>_^_3_^_</span> are samples from <span translate=no>_^_4_^_</span> and <span translate=no>_^_5_^_</span> are samples from <span translate=no>_^_6_^_</span>.</p>\n": "<h2>\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u30ed\u30b9</h2>\n<p><strong>\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u306f\u52fe\u914d\u306e\u4e0a\u3092\u5411\u3044\u3066\u3044\u308b\u306f\u305a\u3067\u3059\u304c</strong></p>\n<p><span translate=no>_^_0_^_</span></p>\n<p><span translate=no>_^_1_^_</span>\u306f\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3067\u3001<span translate=no>_^_2_^_</span>\u30df\u30cb\u30d0\u30c3\u30c1\u5185\u306e\u30b5\u30f3\u30d7\u30eb\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002<span translate=no>_^_3_^_</span><span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\u304b\u3089\u306e\u30b5\u30f3\u30d7\u30eb\u3067\u3042\u308a\u3001<span translate=no>_^_6_^_</span>\u304b\u3089\u306e\u30b5\u30f3\u30d7\u30eb\u3067\u3059\u3002</p>\n",
 "<h2>Generator Loss</h2>\n<p>Generator should <strong>descend</strong> on the gradient,</p>\n<p><span translate=no>_^_0_^_</span></p>\n": "<h2>\u767a\u96fb\u6a5f\u640d\u5931</h2>\n<p><strong>\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u306f\u52fe\u914d\u306b\u6cbf\u3063\u3066\u4e0b\u964d\u3059\u308b\u306f\u305a\u3067\u3059\u304c</strong>\u3001</p>\n<p><span translate=no>_^_0_^_</span></p>\n",
 "<p> <span translate=no>_^_0_^_</span> are logits from <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> are logits from <span translate=no>_^_3_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\u5143\u306e\u30ed\u30b8\u30c3\u30c8\u3068\u5143\u306e\u30ed\u30b8\u30c3\u30c8 <span translate=no>_^_3_^_</span></p>\n",
 "<p> Create smoothed labels</p>\n": "<p>\u306a\u3081\u3089\u304b\u306a\u30e9\u30d9\u30eb\u3092\u4f5c\u6210</p>\n",
 "<p>Labels are registered as buffered and persistence is set to <span translate=no>_^_0_^_</span>. </p>\n": "<p>\u30e9\u30d9\u30eb\u306f\u30d0\u30c3\u30d5\u30a1\u30ea\u30f3\u30b0\u3055\u308c\u3066\u767b\u9332\u3055\u308c\u3001\u30d1\u30fc\u30b7\u30b9\u30bf\u30f3\u30b9\u306f\u306b\u8a2d\u5b9a\u3055\u308c\u307e\u3059\u3002<span translate=no>_^_0_^_</span></p>\n",
 "<p>We use PyTorch Binary Cross Entropy Loss, which is <span translate=no>_^_0_^_</span>, where <span translate=no>_^_1_^_</span> are the labels and <span translate=no>_^_2_^_</span> are the predictions. <em>Note the negative sign</em>. We use labels equal to <span translate=no>_^_3_^_</span> for <span translate=no>_^_4_^_</span> from <span translate=no>_^_5_^_</span> and labels equal to <span translate=no>_^_6_^_</span> for <span translate=no>_^_7_^_</span> from <span translate=no>_^_8_^_</span> Then descending on the sum of these is the same as ascending on the above gradient.</p>\n<p><span translate=no>_^_9_^_</span> combines softmax and binary cross entropy loss. </p>\n": "<p>PyTorch\u306e\u30d0\u30a4\u30ca\u30ea\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3092\u4f7f\u3044\u307e\u3059\u3002\u3064\u307e\u308a<span translate=no>_^_0_^_</span>\u3001<span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\u30e9\u30d9\u30eb\u306f\u3069\u3053\u3067\u4e88\u6e2c\u306f\u3069\u3053\u3067\u3059\u304b\u3002<em>\u30de\u30a4\u30ca\u30b9\u8a18\u53f7\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044</em>\u3002for from <span translate=no>_^_5_^_</span> \u3068\u540c\u3058\u30e9\u30d9\u30eb\u3068 <span translate=no>_^_3_^_</span> for <span translate=no>_^_4_^_</span> from <span translate=no>_^_6_^_</span> \u306b\u7b49\u3057\u3044\u30e9\u30d9\u30eb\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u5408\u8a08\u3067\u964d\u9806\u306b\u306a\u308b\u3068\u3001\u4e0a\u8a18\u306e\u52fe\u914d\u3067\u6607\u9806\u306b\u306a\u308b\u306e\u3068\u540c\u3058\u306b\u306a\u308a\u307e\u3059</p>\u3002<span translate=no>_^_7_^_</span> <span translate=no>_^_8_^_</span>\n<p><span translate=no>_^_9_^_</span>\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u3068\u30d0\u30a4\u30ca\u30ea\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u3082\u306e\u3067\u3059\u3002</p>\n",
 "<p>We use label smoothing because it seems to work better in some cases </p>\n": "<p>\u30e9\u30d9\u30eb\u30b9\u30e0\u30fc\u30b8\u30f3\u30b0\u3092\u4f7f\u7528\u3059\u308b\u306e\u306f\u3001\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u3046\u307e\u304f\u3044\u304f\u3068\u601d\u308f\u308c\u308b\u305f\u3081\u3067\u3059\u3002</p>\n",
 "<p>We use labels equal to <span translate=no>_^_0_^_</span> for <span translate=no>_^_1_^_</span> from <span translate=no>_^_2_^_</span> Then descending on this loss is the same as descending on the above gradient. </p>\n": "<p><span translate=no>_^_0_^_</span>for <span translate=no>_^_1_^_</span> \u3068\u7b49\u3057\u3044\u30e9\u30d9\u30eb\u3092\u4f7f\u3044\u307e\u3059\u3002<span translate=no>_^_2_^_</span>\u3053\u306e\u640d\u5931\u3067\u964d\u9806\u3092\u964d\u9806\u3059\u308b\u3068\u3001\u4e0a\u306e\u52fe\u914d\u3067\u964d\u9806\u306b\u306a\u308b\u306e\u3068\u540c\u3058\u306b\u306a\u308a\u307e\u3059\u3002</p>\n",
 "A simple PyTorch implementation/tutorial of Generative Adversarial Networks (GAN) loss functions.": "\u30b8\u30a7\u30cd\u30ec\u30fc\u30c6\u30a3\u30d6\u30fb\u30a2\u30c9\u30d0\u30fc\u30b5\u30ea\u30a2\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08GAN\uff09\u640d\u5931\u95a2\u6570\u306e\u7c21\u5358\u306aPyTorch\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3002",
 "Generative Adversarial Networks (GAN)": "\u30b8\u30a7\u30cd\u30ec\u30fc\u30c6\u30a3\u30d6\u30fb\u30a2\u30c9\u30d0\u30fc\u30b5\u30ea\u30a2\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af (GAN)"
}