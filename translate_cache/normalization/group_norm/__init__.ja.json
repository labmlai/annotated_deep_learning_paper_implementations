{
 "<h1>Group Normalization</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the <a href=\"https://arxiv.org/abs/1803.08494\">Group Normalization</a> paper.</p>\n<p><a href=\"../batch_norm/index.html\">Batch Normalization</a> works well for large enough batch sizes but not well for small batch sizes, because it normalizes over the batch. Training large models with large batch sizes is not possible due to the memory capacity of the devices.</p>\n<p>This paper introduces Group Normalization, which normalizes a set of features together as a group. This is based on the observation that classical features such as <a href=\"https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\">SIFT</a> and <a href=\"https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\">HOG</a> are group-wise features. The paper proposes dividing feature channels into groups and then separately normalizing all channels within each group.</p>\n<h2>Formulation</h2>\n<p>All normalization layers can be defined by the following computation.</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p>where <span translate=no>_^_1_^_</span> is the tensor representing the batch, and <span translate=no>_^_2_^_</span> is the index of a single value. For instance, when it&#x27;s 2D images <span translate=no>_^_3_^_</span> is a 4-d vector for indexing image within batch, feature channel, vertical coordinate and horizontal coordinate. <span translate=no>_^_4_^_</span> and <span translate=no>_^_5_^_</span> are mean and standard deviation.</p>\n<span translate=no>_^_6_^_</span><p><span translate=no>_^_7_^_</span> is the set of indexes across which the mean and standard deviation are calculated for index <span translate=no>_^_8_^_</span>. <span translate=no>_^_9_^_</span> is the size of the set <span translate=no>_^_10_^_</span> which is the same for all <span translate=no>_^_11_^_</span>.</p>\n<p>The definition of <span translate=no>_^_12_^_</span> is different for <a href=\"../batch_norm/index.html\">Batch normalization</a>, <a href=\"../layer_norm/index.html\">Layer normalization</a>, and <a href=\"../instance_norm/index.html\">Instance normalization</a>.</p>\n<h3><a href=\"../batch_norm/index.html\">Batch Normalization</a></h3>\n<p><span translate=no>_^_13_^_</span></p>\n<p>The values that share the same feature channel are normalized together.</p>\n<h3><a href=\"../layer_norm/index.html\">Layer Normalization</a></h3>\n<p><span translate=no>_^_14_^_</span></p>\n<p>The values from the same sample in the batch are normalized together.</p>\n<h3><a href=\"../instance_norm/index.html\">Instance Normalization</a></h3>\n<p><span translate=no>_^_15_^_</span></p>\n<p>The values from the same sample and same feature channel are normalized together.</p>\n<h3>Group Normalization</h3>\n<p><span translate=no>_^_16_^_</span></p>\n<p>where <span translate=no>_^_17_^_</span> is the number of groups and <span translate=no>_^_18_^_</span> is the number of channels.</p>\n<p>Group normalization normalizes values of the same sample and the same group of channels together.</p>\n<p>Here&#x27;s a <a href=\"experiment.html\">CIFAR 10 classification model</a> that uses instance normalization.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/group_norm/experiment.ipynb\"><span translate=no>_^_19_^_</span></a></p>\n": "<h1>\u30b0\u30eb\u30fc\u30d7\u6b63\u898f\u5316</h1>\n<p><a href=\"https://pytorch.org\"><a href=\"https://arxiv.org/abs/1803.08494\">\u3053\u308c\u306f\u30b0\u30eb\u30fc\u30d7\u6b63\u898f\u5316\u8ad6\u6587\u306ePyTorch\u5b9f\u88c5\u3067\u3059</a></a>\u3002</p>\n<p><a href=\"../batch_norm/index.html\">\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u306f\u30d0\u30c3\u30c1\u5168\u4f53\u3067\u6b63\u898f\u5316\u3055\u308c\u308b\u305f\u3081</a>\u3001\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u304c\u5341\u5206\u5927\u304d\u3044\u5834\u5408\u306f\u3046\u307e\u304f\u6a5f\u80fd\u3057\u307e\u3059\u304c\u3001\u5c0f\u3055\u306a\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306b\u306f\u9069\u3057\u3066\u3044\u307e\u305b\u3093\u3002\u30c7\u30d0\u30a4\u30b9\u306e\u30e1\u30e2\u30ea\u5bb9\u91cf\u306b\u3088\u308a\u3001\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306e\u5927\u304d\u3044\u5927\u898f\u6a21\u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306f\u4e0d\u53ef\u80fd\u3067\u3059\u3002</p>\n<p>\u672c\u7a3f\u3067\u306f\u3001\u4e00\u9023\u306e\u7279\u5fb4\u3092\u30b0\u30eb\u30fc\u30d7\u3068\u3057\u3066\u307e\u3068\u3081\u3066\u6b63\u898f\u5316\u3059\u308b\u30b0\u30eb\u30fc\u30d7\u6b63\u898f\u5316\u306b\u3064\u3044\u3066\u7d39\u4ecb\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001<a href=\"https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\"><a href=\"https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\">SIFT\u3084HOG\u306a\u3069\u306e\u53e4\u5178\u7684\u7279\u5fb4\u306f\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e\u7279\u5fb4\u3067\u3042\u308b\u3068\u3044\u3046\u89b3\u5bdf\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059</a></a>\u3002\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001\u30d5\u30a3\u30fc\u30c1\u30e3\u30c1\u30e3\u30cd\u30eb\u3092\u30b0\u30eb\u30fc\u30d7\u306b\u5206\u5272\u3057\u3001\u5404\u30b0\u30eb\u30fc\u30d7\u5185\u306e\u3059\u3079\u3066\u306e\u30c1\u30e3\u30cd\u30eb\u3092\u500b\u5225\u306b\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u307e\u3059</p>\u3002\n<h2>\u30d5\u30a9\u30fc\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3</h2>\n<p>\u3059\u3079\u3066\u306e\u6b63\u898f\u5316\u5c64\u306f\u3001\u6b21\u306e\u8a08\u7b97\u3067\u5b9a\u7fa9\u3067\u304d\u307e\u3059\u3002</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p>\u3053\u3053\u3067\u3001<span translate=no>_^_1_^_</span>\u306f\u30d0\u30c3\u30c1\u3092\u8868\u3059\u30c6\u30f3\u30bd\u30eb\u3067\u3001<span translate=no>_^_2_^_</span>\u306f\u5358\u4e00\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3067\u3059\u3002\u305f\u3068\u3048\u3070\u3001<span translate=no>_^_3_^_</span> 2D\u753b\u50cf\u304c\u30d0\u30c3\u30c1\u3001\u30d5\u30a3\u30fc\u30c1\u30e3\u30c1\u30e3\u30f3\u30cd\u30eb\u3001\u5782\u76f4\u5ea7\u6a19\u3001\u6c34\u5e73\u5ea7\u6a19\u5185\u306e\u753b\u50cf\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3059\u308b\u305f\u3081\u306e4\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308b\u5834\u5408\u3002<span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u3067\u3059\u3002</p>\n<span translate=no>_^_6_^_</span><p><span translate=no>_^_7_^_</span>\u306f\u3001\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u3059\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u30bb\u30c3\u30c8\u3067\u3059\u3002<span translate=no>_^_8_^_</span><span translate=no>_^_9_^_</span><span translate=no>_^_10_^_</span>\u30bb\u30c3\u30c8\u306e\u30b5\u30a4\u30ba\u306f\u3059\u3079\u3066\u540c\u3058\u3067\u3059<span translate=no>_^_11_^_</span>\u3002</p>\n<p><span translate=no>_^_12_^_</span>\u306e\u5b9a\u7fa9\u306f\u3001<a href=\"../batch_norm/index.html\">\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3001\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316</a><a href=\"../layer_norm/index.html\">\u3001<a href=\"../instance_norm/index.html\">\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u6b63\u898f\u5316\u3067\u306f\u7570\u306a\u308a\u307e\u3059</a></a>\u3002</p>\n<h3><a href=\"../batch_norm/index.html\">\u30d0\u30c3\u30c1\u6b63\u898f\u5316</a></h3>\n<p><span translate=no>_^_13_^_</span></p>\n<p>\u540c\u3058\u30d5\u30a3\u30fc\u30c1\u30e3\u30c1\u30e3\u30cd\u30eb\u3092\u5171\u6709\u3059\u308b\u5024\u306f\u307e\u3068\u3081\u3066\u6b63\u898f\u5316\u3055\u308c\u307e\u3059\u3002</p>\n<h3><a href=\"../layer_norm/index.html\">\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316</a></h3>\n<p><span translate=no>_^_14_^_</span></p>\n<p>\u30d0\u30c3\u30c1\u5185\u306e\u540c\u3058\u30b5\u30f3\u30d7\u30eb\u306e\u5024\u306f\u307e\u3068\u3081\u3066\u6b63\u898f\u5316\u3055\u308c\u307e\u3059\u3002</p>\n<h3><a href=\"../instance_norm/index.html\">\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u6b63\u898f\u5316</a></h3>\n<p><span translate=no>_^_15_^_</span></p>\n<p>\u540c\u3058\u30b5\u30f3\u30d7\u30eb\u3068\u540c\u3058\u7279\u5fb4\u30c1\u30e3\u30f3\u30cd\u30eb\u306e\u5024\u304c\u4e00\u7dd2\u306b\u6b63\u898f\u5316\u3055\u308c\u307e\u3059\u3002</p>\n<h3>\u30b0\u30eb\u30fc\u30d7\u6b63\u898f\u5316</h3>\n<p><span translate=no>_^_16_^_</span></p>\n<p>\u3053\u3053\u3067\u3001<span translate=no>_^_17_^_</span>\u306f\u30b0\u30eb\u30fc\u30d7\u6570\u3001<span translate=no>_^_18_^_</span>\u306f\u30c1\u30e3\u30cd\u30eb\u6570\u3067\u3059\u3002</p>\n<p>\u30b0\u30eb\u30fc\u30d7\u6b63\u898f\u5316\u306f\u3001\u540c\u3058\u30b5\u30f3\u30d7\u30eb\u3068\u540c\u3058\u30c1\u30e3\u30cd\u30eb\u30b0\u30eb\u30fc\u30d7\u306e\u5024\u3092\u307e\u3068\u3081\u3066\u6b63\u898f\u5316\u3057\u307e\u3059\u3002</p>\n<p>\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u6b63\u898f\u5316\u3092\u4f7f\u7528\u3059\u308b <a href=\"experiment.html\">CIFAR 10 \u5206\u985e\u30e2\u30c7\u30eb\u3092\u6b21\u306b\u793a\u3057\u307e\u3059</a>\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/group_norm/experiment.ipynb\"><span translate=no>_^_19_^_</span></a></p>\n",
 "<h2>Group Normalization Layer</h2>\n": "<h2>\u30b0\u30eb\u30fc\u30d7\u6b63\u898f\u5316\u30ec\u30a4\u30e4\u30fc</h2>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> <span translate=no>_^_0_^_</span> is a tensor of shape <span translate=no>_^_1_^_</span>. <span translate=no>_^_2_^_</span> denotes any number of (possibly 0) dimensions.  For example, in an image (2D) convolution this will be <span translate=no>_^_3_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u5f62\u72b6\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059\u3002<span translate=no>_^_2_^_</span>\u4efb\u610f\u306e\u6570 (0 \u306e\u5834\u5408\u3082\u3042\u308a\u307e\u3059) \u306e\u6b21\u5143\u3092\u793a\u3057\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u753b\u50cf (2D) \u306e\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u3067\u306f\u3001\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059</p>\u3002<span translate=no>_^_3_^_</span>\n",
 "<p> Simple test</p>\n": "<p>\u7c21\u5358\u306a\u30c6\u30b9\u30c8</p>\n",
 "<p>Calculate the mean across last dimension; i.e. the means for each sample and channel group <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6700\u5f8c\u306e\u6b21\u5143\u306e\u5e73\u5747\u3001\u3064\u307e\u308a\u5404\u30b5\u30f3\u30d7\u30eb\u3068\u30c1\u30e3\u30cd\u30eb\u30b0\u30eb\u30fc\u30d7\u306e\u5e73\u5747\u3092\u8a08\u7b97\u3057\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate the squared mean across last dimension; i.e. the means for each sample and channel group <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6700\u5f8c\u306e\u6b21\u5143\u306e\u4e8c\u4e57\u5e73\u5747\u3001\u3064\u307e\u308a\u5404\u30b5\u30f3\u30d7\u30eb\u3068\u30c1\u30e3\u30cd\u30eb\u30b0\u30eb\u30fc\u30d7\u306e\u5e73\u5747\u3092\u8a08\u7b97\u3057\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Create parameters for <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> for scale and shift </p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u30b9\u30b1\u30fc\u30eb\u3068\u30b7\u30d5\u30c8\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u4f5c\u6210</p>\n",
 "<p>Get the batch size </p>\n": "<p>\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3092\u53d6\u5f97</p>\n",
 "<p>Keep the original shape </p>\n": "<p>\u5143\u306e\u5f62\u3092\u4fdd\u3064</p>\n",
 "<p>Normalize <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30ce\u30fc\u30de\u30e9\u30a4\u30ba <span translate=no>_^_0_^_</span></p>\n",
 "<p>Reshape into <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5f62\u3092\u5909\u3048\u3066 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Reshape to original and return </p>\n": "<p>\u5143\u306e\u5f62\u306b\u623b\u3057\u3066\u623b\u3059</p>\n",
 "<p>Sanity check to make sure the number of features is the same </p>\n": "<p>\u6a5f\u80fd\u306e\u6570\u304c\u540c\u3058\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306e\u30b5\u30cb\u30c6\u30a3\u30c1\u30a7\u30c3\u30af</p>\n",
 "<p>Scale and shift channel-wise <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30c1\u30e3\u30f3\u30cd\u30eb\u5358\u4f4d\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3068\u30b7\u30d5\u30c8 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Variance for each sample and feature group <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5404\u30b5\u30f3\u30d7\u30eb\u3068\u6a5f\u80fd\u30b0\u30eb\u30fc\u30d7\u306e\u5dee\u7570 <span translate=no>_^_0_^_</span></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of groups the features are divided into </li>\n<li><span translate=no>_^_1_^_</span> is the number of features in the input </li>\n<li><span translate=no>_^_2_^_</span> is <span translate=no>_^_3_^_</span>, used in <span translate=no>_^_4_^_</span> for numerical stability </li>\n<li><span translate=no>_^_5_^_</span> is whether to scale and shift the normalized value</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30d5\u30a3\u30fc\u30c1\u30e3\u304c\u5206\u5272\u3055\u308c\u3066\u3044\u308b\u30b0\u30eb\u30fc\u30d7\u306e\u6570\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span>\u306f\u5165\u529b\u5185\u306e\u7279\u5fb4\u306e\u6570\u3067\u3059</li>\n<li><span translate=no>_^_2_^_</span><span translate=no>_^_4_^_</span>\u6570\u5024\u306e\u5b89\u5b9a\u6027\u306e\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059 <span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_5_^_</span>\u6b63\u898f\u5316\u3055\u308c\u305f\u5024\u3092\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3057\u3066\u30b7\u30d5\u30c8\u3059\u308b\u304b\u3069\u3046\u304b\u3067\u3059</li></ul>\n",
 "A PyTorch implementation/tutorial of group normalization.": "\u30b0\u30eb\u30fc\u30d7\u6b63\u898f\u5316\u306ePyTorch\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3002",
 "Group Normalization": "\u30b0\u30eb\u30fc\u30d7\u6b63\u898f\u5316"
}