{
 "<h1><a href=\"https://nn.labml.ai/normalization/group_norm/index.html\">Group Normalization</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the <a href=\"https://arxiv.org/abs/1803.08494\">Group Normalization</a> paper.</p>\n<p><a href=\"https://nn.labml.ai/normalization/batch_norm/index.html\">Batch Normalization</a> works well for large enough batch sizes but not well for small batch sizes, because it normalizes over the batch. Training large models with large batch sizes is not possible due to the memory capacity of the devices.</p>\n<p>This paper introduces Group Normalization, which normalizes a set of features together as a group. This is based on the observation that classical features such as <a href=\"https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\">SIFT</a> and <a href=\"https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\">HOG</a> are group-wise features. The paper proposes dividing feature channels into groups and then separately normalizing all channels within each group.</p>\n<p>Here&#x27;s a <a href=\"https://nn.labml.ai/normalization/group_norm/experiment.html\">CIFAR 10 classification model</a> that uses group normalization.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/group_norm/experiment.ipynb\"><span translate=no>_^_0_^_</span></a> </p>\n": "<h1><a href=\"https://nn.labml.ai/normalization/group_norm/index.html\">\u7fa4\u7ec4\u6807\u51c6\u5316</a></h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch</a> \u5bf9<a href=\"https://arxiv.org/abs/1803.08494\">\u7fa4\u7ec4\u6807\u51c6\u5316</a>\u8bba\u6587\u7684\u5b9e\u73b0\u3002</p>\n<p><a href=\"https://nn.labml.ai/normalization/batch_norm/index.html\">\u6279\u91cf\u6807\u51c6\u5316</a>\u9002\u7528\u4e8e\u8db3\u591f\u5927\u7684\u6279\u91cf\u5927\u5c0f\uff0c\u4f46\u5bf9\u4e8e\u5c0f\u6279\u91cf\u6765\u8bf4\u5374\u4e0d\u592a\u597d\uff0c\u56e0\u4e3a\u5b83\u4f1a\u5728\u6279\u6b21\u4e0a\u8fdb\u884c\u6807\u51c6\u5316\u3002\u7531\u4e8e\u8bbe\u5907\u7684\u5185\u5b58\u5bb9\u91cf\uff0c\u65e0\u6cd5\u8bad\u7ec3\u6279\u91cf\u8f83\u5927\u7684\u5927\u578b\u6a21\u578b\u3002</p>\n<p>\u672c\u6587\u4ecb\u7ecd\u4e86\u7fa4\u7ec4\u5f52\u4e00\u5316\uff0c\u5b83\u5c06\u4e00\u7ec4\u7279\u5f81\u5f52\u4e00\u5316\u4e3a\u4e00\u4e2a\u7ec4\u3002\u8fd9\u662f\u57fa\u4e8e\u8fd9\u6837\u7684\u89c2\u5bdf\uff0c\u5373\u8bf8\u5982 <a href=\"https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\">SIFT</a> \u548c <a href=\"https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\">HO</a> G\u4e4b\u7c7b\u7684\u7ecf\u5178\u7279\u5f81\u662f\u6309\u7ec4\u5212\u5206\u7684\u7279\u5f81\u3002\u8be5\u8bba\u6587\u5efa\u8bae\u5c06\u7279\u5f81\u4fe1\u9053\u5206\u6210\u7ec4\uff0c\u7136\u540e\u5206\u522b\u5bf9\u6bcf\u4e2a\u7ec4\u5185\u7684\u6240\u6709\u4fe1\u9053\u8fdb\u884c\u6807\u51c6\u5316\u3002</p>\n<p>\u8fd9\u662f\u4f7f\u7528\u5b9e\u4f8b\u6807\u51c6\u5316\u7684 <a href=\"https://nn.labml.ai/normalization/group_norm/experiment.html\">CIFAR 10 \u5206\u7c7b\u6a21\u578b</a>\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/group_norm/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n",
 "Group Normalization": "\u7fa4\u7ec4\u89c4\u8303\u5316"
}