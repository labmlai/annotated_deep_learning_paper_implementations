{
 "<h1><a href=\"https://nn.labml.ai/normalization/layer_norm/index.html\">Layer Normalization</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of <a href=\"https://arxiv.org/abs/1607.06450\">Layer Normalization</a>.</p>\n<h3>Limitations of <a href=\"https://nn.labml.ai/normalization/batch_norm/index.html\">Batch Normalization</a></h3>\n<ul><li>You need to maintain running means. </li>\n<li>Tricky for RNNs. Do you need different normalizations for each step? </li>\n<li>Doesn&#x27;t work with small batch sizes; large NLP models are usually trained with small batch sizes. </li>\n<li>Need to compute means and variances across devices in distributed training.</li></ul>\n<h2>Layer Normalization</h2>\n<p>Layer normalization is a simpler normalization method that works on a wider range of settings. Layer normalization transforms the inputs to have zero mean and unit variance across the features. <em>Note that batch normalization fixes the zero mean and unit variance for each element.</em> Layer normalization does it for each batch across all elements.</p>\n<p>Layer normalization is generally used for NLP tasks.</p>\n<p>We have used layer normalization in most of the <a href=\"https://nn.labml.ai/transformers/gpt/index.html\">transformer implementations</a>.</p>\n": "<h1><a href=\"https://nn.labml.ai/normalization/layer_norm/index.html\">\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316</a></h1>\n<p><a href=\"https://arxiv.org/abs/1607.06450\">\u3053\u308c\u306f\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316\u306e</a> <a href=\"https://pytorch.org\">PyTorch</a> \u5b9f\u88c5\u3067\u3059\u3002</p>\n<h3><a href=\"https://nn.labml.ai/normalization/batch_norm/index.html\">\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u306e\u5236\u9650\u4e8b\u9805</a></h3>\n<ul><li>\u30e9\u30f3\u30cb\u30f3\u30b0\u624b\u6bb5\u3092\u7dad\u6301\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</li>\n<li>RNN\u306b\u3068\u3063\u3066\u306f\u6271\u3044\u306b\u304f\u3044\u3002\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7570\u306a\u308b\u6b63\u898f\u5316\u304c\u5fc5\u8981\u3067\u3059\u304b</li>?\n<li>\u5c0f\u3055\u306a\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3067\u306f\u6a5f\u80fd\u3057\u307e\u305b\u3093\u3002\u5927\u898f\u6a21\u306aNLP\u30e2\u30c7\u30eb\u306f\u901a\u5e38\u3001\u5c0f\u3055\u306a\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u307e\u3059\u3002</li>\n</ul><li>\u5206\u6563\u578b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3067\u306f\u3001\u30c7\u30d0\u30a4\u30b9\u9593\u306e\u5e73\u5747\u3068\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</li>\n<h2>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316</h2>\n<p>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316\u306f\u3001\u3088\u308a\u5e45\u5e83\u3044\u8a2d\u5b9a\u306b\u9069\u7528\u3067\u304d\u308b\u3001\u3088\u308a\u5358\u7d14\u306a\u6b63\u898f\u5316\u65b9\u6cd5\u3067\u3059\u3002\u5c64\u306e\u6b63\u898f\u5316\u306b\u3088\u308a\u3001\u5165\u529b\u306f\u7279\u5fb4\u5168\u4f53\u3067\u5e73\u5747\u304c\u30bc\u30ed\u3067\u5358\u4f4d\u5206\u6563\u304c\u306a\u304f\u306a\u308b\u3088\u3046\u306b\u5909\u63db\u3055\u308c\u307e\u3059\u3002<em>\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3067\u306f\u3001\u5404\u8981\u7d20\u306e\u30bc\u30ed\u5e73\u5747\u3068\u5358\u4f4d\u5206\u6563\u304c\u56fa\u5b9a\u3055\u308c\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002</em>\u30ec\u30a4\u30e4\u30fc\u306e\u6b63\u898f\u5316\u306f\u3001\u3059\u3079\u3066\u306e\u8981\u7d20\u306e\u30d0\u30c3\u30c1\u3054\u3068\u306b\u6b63\u898f\u5316\u3092\u884c\u3044\u307e\u3059</p>\u3002\n<p>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316\u306f\u901a\u5e38\u3001NLP \u30bf\u30b9\u30af\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002</p>\n<p><a href=\"https://nn.labml.ai/transformers/gpt/index.html\">\u307b\u3068\u3093\u3069\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u5b9f\u88c5\u3067\u5c64\u306e\u6b63\u898f\u5316\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059</a>\u3002</p>\n",
 "Layer Normalization": "\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316"
}