{
 "<h1>Weight Standardization</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of Weight Standardization from the paper  <a href=\"https://arxiv.org/abs/1903.10520\">Micro-Batch Training with Batch-Channel Normalization and Weight Standardization</a>. We also have an <a href=\"../batch_channel_norm/index.html\">annotated implementation of Batch-Channel Normalization</a>.</p>\n<p>Batch normalization <strong>gives a smooth loss landscape</strong> and <strong>avoids elimination singularities</strong>. Elimination singularities are nodes of the network that become useless (e.g. a ReLU that gives 0 all the time).</p>\n<p>However, batch normalization doesn&#x27;t work well when the batch size is too small, which happens when training large networks because of device memory limitations. The paper introduces Weight Standardization with Batch-Channel Normalization as a better alternative.</p>\n<p>Weight Standardization: 1. Normalizes the gradients 2. Smoothes the landscape (reduced Lipschitz constant) 3. Avoids elimination singularities</p>\n<p>The Lipschitz constant is the maximum slope a function has between two points. That is, <span translate=no>_^_0_^_</span> is the Lipschitz constant where <span translate=no>_^_1_^_</span> is the smallest value that satisfies, <span translate=no>_^_2_^_</span> where <span translate=no>_^_3_^_</span>.</p>\n<p>Elimination singularities are avoided because it keeps the statistics of the outputs similar to the inputs. So as long as the inputs are normally distributed the outputs remain close to normal. This avoids outputs of nodes from always falling beyond the active range of the activation function (e.g. always negative input for a ReLU).</p>\n<p><em><a href=\"https://arxiv.org/abs/1903.10520\">Refer to the paper for proofs</a></em>.</p>\n<p>Here is <a href=\"experiment.html\">the training code</a> for training a VGG network that uses weight standardization to classify CIFAR-10 data. This uses a <a href=\"conv2d.html\">2D-Convolution Layer with Weight Standardization</a>.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/weight_standardization/experiment.ipynb\"><span translate=no>_^_4_^_</span></a></p>\n": "<h1>\u91cd\u91cf\u6807\u51c6\u5316</h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch</a> \u5b9e\u73b0\u7684\u6743\u91cd\u6807\u51c6\u5316\uff0c\u6e90\u81ea\u8bba\u6587\u300a<a href=\"https://arxiv.org/abs/1903.10520\">\u5177\u6709\u6279\u5904\u7406\u901a\u9053\u6807\u51c6\u5316\u548c\u6743\u91cd\u6807\u51c6\u5316\u7684\u5fae\u6279\u91cf\u8bad\u7ec3</a>\u300b\u3002\u6211\u4eec\u8fd8\u6709\u4e00\u4e2a<a href=\"../batch_channel_norm/index.html\">\u5e26\u6ce8\u91ca\u7684\u6279\u5904\u7406\u901a\u9053\u6807\u51c6\u5316\u5b9e\u73b0</a>\u3002</p>\n<p>\u6279\u91cf\u5f52<strong>\u4e00\u5316\u63d0\u4f9b\u4e86\u5e73\u6ed1\u7684\u635f\u5931\u683c\u5c40</strong>\uff0c<strong>\u907f\u514d\u4e86\u6d88\u9664\u5947\u5f02\u6027</strong>\u3002\u6d88\u9664\u5947\u70b9\u662f\u7f51\u7edc\u4e2d\u53d8\u5f97\u6beb\u65e0\u7528\u5904\u7684\u8282\u70b9\uff08\u4f8b\u5982\uff0c\u4e00\u76f4\u7ed9\u51fa 0 \u7684 ReLU\uff09\u3002</p>\n<p>\u4f46\u662f\uff0c\u5f53\u6279\u91cf\u5927\u5c0f\u592a\u5c0f\u65f6\uff0c\u6279\u91cf\u6807\u51c6\u5316\u6548\u679c\u4e0d\u4f73\uff0c\u7531\u4e8e\u8bbe\u5907\u5185\u5b58\u9650\u5236\uff0c\u5728\u8bad\u7ec3\u5927\u578b\u7f51\u7edc\u65f6\u4f1a\u53d1\u751f\u8fd9\u79cd\u60c5\u51b5\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u91c7\u7528\u6279\u5904\u7406\u4fe1\u9053\u6807\u51c6\u5316\u7684\u6743\u91cd\u6807\u51c6\u5316\u4f5c\u4e3a\u66f4\u597d\u7684\u66ff\u4ee3\u65b9\u6848\u3002</p>\n<p>\u91cd\u91cf\u6807\u51c6\u5316\uff1a1.\u5f52\u4e00\u5316\u68af\u5ea6 2.\u5e73\u6ed1\u5730\u5f62\uff08\u964d\u4f4e\u4e86 Lipschitz \u5e38\u6570\uff093.\u907f\u514d\u6d88\u9664\u5947\u70b9</p>\n<p>Lipschitz \u5e38\u91cf\u662f\u51fd\u6570\u5728\u4e24\u70b9\u4e4b\u95f4\u7684\u6700\u5927\u659c\u7387\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c<span translate=no>_^_0_^_</span>\u662f Lipschitz \u5e38\u6570\uff0c\u5176\u4e2d<span translate=no>_^_1_^_</span>\u662f\u6ee1\u8db3\u7684\u6700\u5c0f\u503c\uff0c<span translate=no>_^_2_^_</span>\u5176\u4e2d<span translate=no>_^_3_^_</span>\u3002</p>\n<p>\u907f\u514d\u4e86\u6d88\u9664\u5947\u5f02\u6027\uff0c\u56e0\u4e3a\u5b83\u4f7f\u8f93\u51fa\u7684\u7edf\u8ba1\u6570\u636e\u4e0e\u8f93\u5165\u7684\u7edf\u8ba1\u6570\u636e\u76f8\u4f3c\u3002\u56e0\u6b64\uff0c\u53ea\u8981\u8f93\u5165\u5448\u6b63\u6001\u5206\u5e03\uff0c\u8f93\u51fa\u5c31\u4fdd\u6301\u63a5\u8fd1\u6b63\u5e38\u6c34\u5e73\u3002\u8fd9\u6837\u53ef\u4ee5\u907f\u514d\u8282\u70b9\u7684\u8f93\u51fa\u603b\u662f\u8d85\u51fa\u6fc0\u6d3b\u51fd\u6570\u7684\u6709\u6548\u8303\u56f4\uff08\u4f8b\u5982\uff0cReLU \u7684\u8f93\u5165\u603b\u662f\u8d1f\u6570\uff09\u3002</p>\n<p>\u6709\u5173<em><a href=\"https://arxiv.org/abs/1903.10520\">\u6837\u5f20\uff0c\u8bf7\u53c2\u9605\u8bba\u6587</a></em>\u3002</p>\n<p><a href=\"experiment.html\">\u4ee5\u4e0b\u662f\u8bad\u7ec3 VGG \u7f51\u7edc\u7684\u8bad\u7ec3\u4ee3\u7801</a>\uff0c\u8be5\u7f51\u7edc\u4f7f\u7528\u6743\u91cd\u6807\u51c6\u5316\u5bf9 CIFAR-10 \u6570\u636e\u8fdb\u884c\u5206\u7c7b\u3002\u8fd9\u4f7f\u7528\u4e86<a href=\"conv2d.html\">\u5177\u6709\u6743\u91cd\u6807\u51c6\u5316\u529f\u80fd\u7684\u4e8c\u7ef4\u5377\u79ef\u5c42</a>\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/weight_standardization/experiment.ipynb\"><span translate=no>_^_4_^_</span></a></p>\n",
 "<h2>Weight Standardization</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p>where,</p>\n<span translate=no>_^_1_^_</span><p>for a 2D-convolution layer <span translate=no>_^_2_^_</span> is the number of output channels (<span translate=no>_^_3_^_</span>) and <span translate=no>_^_4_^_</span> is the number of input channels times the kernel size (<span translate=no>_^_5_^_</span>)</p>\n": "<h2>\u91cd\u91cf\u6807\u51c6\u5316</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p>\u5728\u54ea\u91cc\uff0c</p>\n<span translate=no>_^_1_^_</span><p>\u5bf9\u4e8e 2D \u5377\u79ef\u5c42\uff0c<span translate=no>_^_2_^_</span>\u662f\u8f93\u51fa\u901a\u9053\u6570 (<span translate=no>_^_3_^_</span>)\uff0c<span translate=no>_^_4_^_</span>\u662f\u8f93\u5165\u901a\u9053\u6570\u4e58\u4ee5\u5185\u6838\u5927\u5c0f (<span translate=no>_^_5_^_</span></p>)\n",
 "<p>Calculate</p>\n<span translate=no>_^_0_^_</span><p> </p>\n": "<p>\u8ba1\u7b97</p>\n<span translate=no>_^_0_^_</span><p></p>\n",
 "<p>Change back to original shape and return </p>\n": "<p>\u6539\u56de\u539f\u59cb\u5f62\u72b6\u5e76\u8fd4\u56de</p>\n",
 "<p>Get <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and kernel shape </p>\n": "<p>\u83b7\u53d6<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u548c\u5185\u6838\u5f62\u72b6</p>\n",
 "<p>Normalize <span translate=no>_^_0_^_</span> </p>\n": "<p>\u89c4\u8303\u5316<span translate=no>_^_0_^_</span></p>\n",
 "<p>Reshape <span translate=no>_^_0_^_</span> to <span translate=no>_^_1_^_</span> </p>\n": "<p>\u91cd\u5851<span translate=no>_^_0_^_</span>\u4e3a<span translate=no>_^_1_^_</span></p>\n",
 "A PyTorch implementation/tutorial of Weight Standardization.": "\u6743\u91cd\u6807\u51c6\u5316\u7684 PyTorch \u5b9e\u73b0/\u6559\u7a0b\u3002",
 "Weight Standardization": "\u91cd\u91cf\u6807\u51c6\u5316"
}