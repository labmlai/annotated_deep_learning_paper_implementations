{
 "<h1>DeepNorm</h1>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/deep_norm/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the DeepNorm from the paper <a href=\"https://arxiv.org/abs/2203.00555\">DeepNet: Scaling Transformers to 1,000 Layers</a>.</p>\n<p>The paper proposes a method to stabilize extremely deep transformers through a new normalizing function to replace LayerNorm and a weight initialization scheme. This combines the performance of Post-LayerNorm and the stability of Pre-LayerNorm. Transformers with DeepNorms are supposed to be stable even without a learning rate warm-up.</p>\n<p>The paper first shows that the changes to layer outputs (for the same input)  change gradually during stable training; when unstable it changes rapidly during the initial training steps. This happens with initializing weights to small values, and learning rate warm-ups where the training is stable. They use the idea of keeping the changes to layer outputs small to derive the new  normalization and weight initialization mechanism.</p>\n<h2>Weight Initializations</h2>\n<p>Usually, the weights are initialized with Xavier or Kaiming initializations. This paper scales (sets the gain) the weights by a constant <span translate=no>_^_1_^_</span> depending on the size of the  transformer.</p>\n<p>DeepNorm suggests scaling the weights of the two linear transforms in the <a href=\"../../transformers/feed_forward.html\">Feed-Forward Network</a>, the value projection transform, and the output projection transform of the attention layer. Weights of these transforms are scaled by (has a gain equal to) <span translate=no>_^_2_^_</span>.</p>\n<p>The scaling is implemented in the</p>\n<h2>Normalization Function</h2>\n<p><span translate=no>_^_3_^_</span></p>\n<p>where <span translate=no>_^_4_^_</span> is a constant that depends on the depth of the transformer,  <span translate=no>_^_5_^_</span> is <a href=\"../layer_norm/index.html\">Layer Normalization</a>, and  <span translate=no>_^_6_^_</span> is the function of the <span translate=no>_^_7_^_</span>-th transformer sub-layer (FFN or attention).</p>\n<p>This function is used to replace Post-LayerNorm.</p>\n<h2><span translate=no>_^_8_^_</span> and <span translate=no>_^_9_^_</span> constants</h2>\n<span translate=no>_^_10_^_</span><p>Where <span translate=no>_^_11_^_</span> is the number of layers in the encoder and <span translate=no>_^_12_^_</span> is the number of layers in the decoder.</p>\n<p>Refer to <a href=\"https://arxiv.org/abs/2203.00555\">the paper</a> for derivation.</p>\n<p><a href=\"experiment.html\">Here is an experiment implementation</a> that uses DeepNorm.</p>\n": "<h1>\u30c7\u30a3\u30fc\u30d7\u30fb\u30ce\u30fc\u30e0</h1>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/deep_norm/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n<p>\u3053\u308c\u306f\u3001\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/2203.00555\">DeepNet: \u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30921,000\u30ec\u30a4\u30e4\u30fc\u306b\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u300d<a href=\"https://pytorch.org\">\u306b\u63b2\u8f09\u3055\u308c\u305fDeepNorm\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a></a>\u3002</p>\n<p>\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001LayerNorm\u306b\u4ee3\u308f\u308b\u65b0\u3057\u3044\u6b63\u898f\u5316\u95a2\u6570\u3068\u91cd\u307f\u521d\u671f\u5316\u30b9\u30ad\u30fc\u30e0\u306b\u3088\u308a\u3001\u975e\u5e38\u306b\u6df1\u3044\u5909\u5727\u5668\u3092\u5b89\u5b9a\u3055\u305b\u308b\u65b9\u6cd5\u3092\u63d0\u6848\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u30dd\u30b9\u30c8\u30ec\u30a4\u30e4\u30fc\u30ce\u30fc\u30e0\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3068\u30d7\u30ec\u30ec\u30a4\u30e4\u30fc\u30ce\u30fc\u30e0\u306e\u5b89\u5b9a\u6027\u3092\u517c\u306d\u5099\u3048\u3066\u3044\u307e\u3059\u3002DeepNorms\u3092\u642d\u8f09\u3057\u305f\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306f\u3001\u5b66\u7fd2\u7387\u306e\u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7\u304c\u306a\u304f\u3066\u3082\u5b89\u5b9a\u3057\u3066\u3044\u308b\u306f\u305a\u3067\u3059</p>\u3002\n<p>\u3053\u306e\u8ad6\u6587\u306f\u307e\u305a\u3001\uff08\u540c\u3058\u5165\u529b\u306e\uff09\u30ec\u30a4\u30e4\u30fc\u51fa\u529b\u306e\u5909\u5316\u304c\u5b89\u5b9a\u3057\u305f\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u4e2d\u306b\u5f90\u3005\u306b\u5909\u5316\u3057\u3001\u4e0d\u5b89\u5b9a\u306a\u5834\u5408\u306f\u6700\u521d\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b9\u30c6\u30c3\u30d7\u3067\u6025\u901f\u306b\u5909\u5316\u3059\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u91cd\u307f\u3092\u5c0f\u3055\u3044\u5024\u306b\u521d\u671f\u5316\u3057\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u304c\u5b89\u5b9a\u3057\u3066\u3044\u308b\u3068\u3053\u308d\u3067\u5b66\u7fd2\u7387\u306e\u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7\u3092\u884c\u3046\u3068\u8d77\u3053\u308a\u307e\u3059\u3002\u5f7c\u3089\u306f\u3001\u5c64\u51fa\u529b\u3078\u306e\u5909\u66f4\u3092\u5c0f\u3055\u304f\u6291\u3048\u308b\u3068\u3044\u3046\u8003\u3048\u3092\u5229\u7528\u3057\u3066\u3001\u65b0\u3057\u3044\u6b63\u898f\u5316\u3068\u91cd\u307f\u521d\u671f\u5316\u306e\u30e1\u30ab\u30cb\u30ba\u30e0\u3092\u5c0e\u304d\u51fa\u3057\u3066\u3044\u307e\u3059</p>\u3002\n<h2>\u30a6\u30a7\u30a4\u30c8\u521d\u671f\u5316</h2>\n<p>\u901a\u5e38\u3001\u30a6\u30a7\u30a4\u30c8\u306f Xavier \u307e\u305f\u306f Kaiming \u306e\u521d\u671f\u5316\u3067\u521d\u671f\u5316\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u30da\u30fc\u30d1\u30fc\u3067\u306f\u3001<span translate=no>_^_1_^_</span>\u30c8\u30e9\u30f3\u30b9\u306e\u30b5\u30a4\u30ba\u306b\u5fdc\u3058\u3066\u4e00\u5b9a\u306e\u5272\u5408\u3067\u30a6\u30a7\u30a4\u30c8\u3092\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\uff08\u30b2\u30a4\u30f3\u8a2d\u5b9a\uff09\u3057\u307e\u3059</p>\u3002\n<p>DeepNorm\u306f\u3001<a href=\"../../transformers/feed_forward.html\">\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5185\u306e2\u3064\u306e\u7dda\u5f62\u5909\u63db</a>\u3001\u3064\u307e\u308a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc\u306e\u5024\u6295\u5f71\u5909\u63db\u3068\u51fa\u529b\u6295\u5f71\u5909\u63db\u306e\u91cd\u307f\u3092\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30e0\u306e\u91cd\u307f\u306f (\u30b2\u30a4\u30f3\u304c\u3068\u7b49\u3057\u3044) \u3067\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3055\u308c\u307e\u3059</p>\u3002<span translate=no>_^_2_^_</span>\n<p>\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u306f\u3067\u5b9f\u88c5\u3055\u308c\u3066\u3044\u307e\u3059</p>\n<h2>\u6b63\u898f\u5316\u6a5f\u80fd</h2>\n<p><span translate=no>_^_3_^_</span></p>\n<p>\u3053\u3053\u3067\u3001<span translate=no>_^_4_^_</span>\u306f\u5909\u5727\u5668\u306e\u6df1\u3055\u306b\u4f9d\u5b58\u3059\u308b\u5b9a\u6570\u3001<span translate=no>_^_5_^_</span><a href=\"../layer_norm/index.html\">\u306f\u5c64\u306e\u6b63\u898f\u5316\u3001\u306f <span translate=no>_^_7_^_</span>-\u756a\u76ee\u306e\u5909\u5727\u5668\u30b5\u30d6\u30ec\u30a4\u30e4\u30fc</a> (FFN \u307e\u305f\u306f\u6ce8\u610f) <span translate=no>_^_6_^_</span> \u306e\u95a2\u6570\u3067\u3059\u3002</p>\n<p>\u3053\u306e\u95a2\u6570\u306f\u30dd\u30b9\u30c8\u30ec\u30a4\u30e4\u30fc\u30ce\u30eb\u30e0\u306e\u4ee3\u308f\u308a\u306b\u4f7f\u308f\u308c\u307e\u3059\u3002</p>\n<h2><span translate=no>_^_8_^_</span><span translate=no>_^_9_^_</span>\u3068\u5b9a\u6570</h2>\n<span translate=no>_^_10_^_</span><p>\u3053\u3053\u3067\u3001<span translate=no>_^_11_^_</span>\u306f\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u30ec\u30a4\u30e4\u30fc\u6570\u3001<span translate=no>_^_12_^_</span>\u306f\u30c7\u30b3\u30fc\u30c0\u30fc\u306e\u30ec\u30a4\u30e4\u30fc\u6570\u3067\u3059\u3002</p>\n<p><a href=\"https://arxiv.org/abs/2203.00555\">\u5c0e\u51fa\u306b\u3064\u3044\u3066\u306f\u8ad6\u6587\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044</a>\u3002</p>\n<p><a href=\"experiment.html\">DeepNorm\u3092\u4f7f\u3063\u305f\u5b9f\u9a13\u7684\u306a\u5b9f\u88c5\u3067\u3059</a>\u3002</p>\n",
 "<h2>DeepNorm Normalization</h2>\n<p><span translate=no>_^_0_^_</span></p>\n": "<h2>\u30c7\u30a3\u30fc\u30d7\u30ce\u30eb\u30e0\u6b63\u898f\u5316</h2>\n<p><span translate=no>_^_0_^_</span></p>\n",
 "<h2>Transformer Decoder Layer with DeepNorm</h2>\n<p>This implements a transformer decoder layer with DeepNorm. Encoder layers will have a similar form.</p>\n": "<h2>DeepNorm\u642d\u8f09\u30c8\u30e9\u30f3\u30b9\u30c7\u30b3\u30fc\u30c0\u30ec\u30a4\u30e4\u30fc</h2>\n<p>\u3053\u308c\u306fDeepNorm\u3067\u30c8\u30e9\u30f3\u30b9\u30c7\u30b3\u30fc\u30c0\u30fc\u30ec\u30a4\u30e4\u30fc\u3092\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002\u30a8\u30f3\u30b3\u30fc\u30c0\u30ec\u30a4\u30e4\u30fc\u3082\u540c\u69d8\u306e\u5f62\u5f0f\u306b\u306a\u308a\u307e\u3059</p>\u3002\n",
 "<p> </p>\n": "<p></p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>Attention output project </p>\n": "<p>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\u30d7\u30ed\u30b8\u30a7\u30af\u30c8</p>\n",
 "<p>Attention value projection </p>\n": "<p>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30fb\u30d0\u30ea\u30e5\u30fc\u30fb\u30d7\u30ed\u30b8\u30a7\u30af\u30b7\u30e7\u30f3</p>\n",
 "<p>Create causal mask </p>\n": "<p>\u30ab\u30b8\u30e5\u30a2\u30eb\u30de\u30b9\u30af\u306e\u4f5c\u6210</p>\n",
 "<p>DeepNorms after attention and feed forward network </p>\n": "<p>DeepNorms \u30a2\u30d5\u30bf\u30fc\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30a2\u30f3\u30c9\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af</p>\n",
 "<p>Feed forward network linear transformations </p>\n": "<p>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u7dda\u5f62\u5909\u63db</p>\n",
 "<p>Initialize <span translate=no>_^_0_^_</span> </p>\n": "<p>[\u521d\u671f\u5316] <span translate=no>_^_0_^_</span></p>\n",
 "<p>Pass through the feed-forward network </p>\n": "<p>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u901a\u904e</p>\n",
 "<p>Run through self attention, i.e. keys and values are from self </p>\n": "<p>\u81ea\u5df1\u6ce8\u610f\u3092\u5411\u3051\u308b\u3002\u3064\u307e\u308a\u3001\u30ad\u30fc\u3068\u5024\u306f\u81ea\u5df1\u304b\u3089\u306e\u3082\u306e\u3060</p>\n",
 "<p>Scale weights after initialization </p>\n": "<p>\u521d\u671f\u5316\u5f8c\u306b\u30a6\u30a7\u30a4\u30c8\u3092\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0</p>\n",
 "<p>Subsequent mask, will mask out tokens from seeing future tokens </p>\n": "<p>\u6b21\u306b\u30de\u30b9\u30af\u3059\u308b\u3068\u3001\u30c8\u30fc\u30af\u30f3\u304c\u30de\u30b9\u30af\u3055\u308c\u3001\u5c06\u6765\u306e\u30c8\u30fc\u30af\u30f3\u304c\u898b\u3048\u306a\u304f\u306a\u308a\u307e\u3059</p>\n",
 "<p>The mask will be initialized on the first call </p>\n": "<p>\u30de\u30b9\u30af\u306f\u6700\u521d\u306e\u547c\u3073\u51fa\u3057\u3067\u521d\u671f\u5316\u3055\u308c\u307e\u3059</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  are the embeddings of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u5f62\u304c\u57cb\u3081\u8fbc\u307e\u308c\u3066\u3044\u308b\u3082\u306e\u3067\u3059 <span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span>  is the shape for LayerNorm <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span>  is <span translate=no>_^_5_^_</span> for LayerNorm </li>\n<li><span translate=no>_^_6_^_</span>  is a flag indicating whether to do an elementwise transformation in LayerNorm</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u306f\u30ec\u30a4\u30e4\u30fc\u30ce\u30eb\u30e0\u306e\u5f62\u72b6\u3067\u3059 <span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\u30ec\u30a4\u30e4\u30fc\u30ce\u30eb\u30e0\u7528</li>\n<li><span translate=no>_^_6_^_</span>LayerNorm \u3067\u8981\u7d20\u5358\u4f4d\u306e\u5909\u63db\u3092\u884c\u3046\u304b\u3069\u3046\u304b\u3092\u793a\u3059\u30d5\u30e9\u30b0\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the output from the previous layer <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span>  is the output of the current sub-layer <span translate=no>_^_3_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u524d\u306e\u30ec\u30a4\u30e4\u30fc\u304b\u3089\u306e\u51fa\u529b\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u73fe\u5728\u306e\u30b5\u30d6\u30ec\u30a4\u30e4\u30fc\u306e\u51fa\u529b\u3067\u3059 <span translate=no>_^_3_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the token embedding size </li>\n<li><span translate=no>_^_1_^_</span>  is the self attention module </li>\n<li><span translate=no>_^_2_^_</span>  is the feed forward module </li>\n<li><span translate=no>_^_3_^_</span>  is <span translate=no>_^_4_^_</span> coefficient in DeepNorm </li>\n<li><span translate=no>_^_5_^_</span>  is <span translate=no>_^_6_^_</span> constant for scaling weights initialization</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u30b5\u30a4\u30ba\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span>\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059</li>\n<li><span translate=no>_^_2_^_</span>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059</li>\n<li><span translate=no>_^_3_^_</span><span translate=no>_^_4_^_</span>\u306f\u30c7\u30a3\u30fc\u30d7\u30ce\u30fc\u30e0\u306e\u4fc2\u6570</li>\n<li><span translate=no>_^_5_^_</span><span translate=no>_^_6_^_</span>\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u30a6\u30a7\u30a4\u30c8\u306e\u521d\u671f\u5316\u3067\u306f\u5b9a\u6570\u3067\u3059</li></ul>\n",
 "A PyTorch implementation/tutorial of DeepNorm from paper DeepNet: Scaling Transformers to 1,000 Layers.": "DeepNet\u300c\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30921,000\u30ec\u30a4\u30e4\u30fc\u306b\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u300d\u3068\u3044\u3046\u8ad6\u6587\u306b\u63b2\u8f09\u3055\u308c\u3066\u3044\u308b\u3001DeepNorm\u306ePyTorch\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u3059\u3002",
 "DeepNorm": "\u30c7\u30a3\u30fc\u30d7\u30fb\u30ce\u30fc\u30e0"
}