{
 "<h1>Batch Normalization</h1>\n": "<h1>\u6279\u91cf\u6807\u51c6\u5316</h1>\n",
 "<h2>Batch Normalization Layer</h2>\n<p>Batch normalization layer <span translate=no>_^_0_^_</span> normalizes the input <span translate=no>_^_1_^_</span> as follows:</p>\n<p>When input <span translate=no>_^_2_^_</span> is a batch of image representations, where <span translate=no>_^_3_^_</span> is the batch size, <span translate=no>_^_4_^_</span> is the number of channels, <span translate=no>_^_5_^_</span> is the height and <span translate=no>_^_6_^_</span> is the width. <span translate=no>_^_7_^_</span> and <span translate=no>_^_8_^_</span>. <span translate=no>_^_9_^_</span></p>\n<p>When input <span translate=no>_^_10_^_</span> is a batch of embeddings, where <span translate=no>_^_11_^_</span> is the batch size and <span translate=no>_^_12_^_</span> is the number of features. <span translate=no>_^_13_^_</span> and <span translate=no>_^_14_^_</span>. <span translate=no>_^_15_^_</span></p>\n<p>When input <span translate=no>_^_16_^_</span> is a batch of a sequence embeddings, where <span translate=no>_^_17_^_</span> is the batch size, <span translate=no>_^_18_^_</span> is the number of features, and <span translate=no>_^_19_^_</span> is the length of the sequence. <span translate=no>_^_20_^_</span> and <span translate=no>_^_21_^_</span>. <span translate=no>_^_22_^_</span></p>\n": "<h2>\u6279\u91cf\u5f52\u4e00\u5316\u5c42</h2>\n<p>\u6279\u91cf\u5f52\u4e00\u5316\u5c42\u5c06\u8f93\u5165<span translate=no>_^_0_^_</span>\u5f52\u4e00\u5316\uff0c<span translate=no>_^_1_^_</span>\u5982\u4e0b\u6240\u793a\uff1a</p>\n<p>\u5f53\u8f93\u5165<span translate=no>_^_2_^_</span>\u662f\u4e00\u6279\u56fe\u50cf\u8868\u793a\u65f6\uff0c\u5176\u4e2d<span translate=no>_^_3_^_</span>\u662f\u6279\u6b21\u5927\u5c0f\uff0c<span translate=no>_^_4_^_</span>\u662f\u901a\u9053\u6570\uff0c<span translate=no>_^_5_^_</span>\u662f\u9ad8\u5ea6\u548c<span translate=no>_^_6_^_</span>\u662f\u5bbd\u5ea6\u3002<span translate=no>_^_7_^_</span>\u548c<span translate=no>_^_8_^_</span>\u3002<span translate=no>_^_9_^_</span></p>\n<p>\u5f53\u8f93\u5165<span translate=no>_^_10_^_</span>\u662f\u4e00\u6279\u5d4c\u5165\u65f6\uff0c\u5176\u4e2d<span translate=no>_^_11_^_</span>\u662f\u6279\u6b21\u5927\u5c0f\uff0c<span translate=no>_^_12_^_</span>\u662f\u8981\u7d20\u7684\u6570\u91cf\u3002<span translate=no>_^_13_^_</span>\u548c<span translate=no>_^_14_^_</span>\u3002<span translate=no>_^_15_^_</span></p>\n<p>\u5f53\u8f93\u5165<span translate=no>_^_16_^_</span>\u662f\u4e00\u6279\u5e8f\u5217\u5d4c\u5165\u65f6\uff0c\u5176\u4e2d<span translate=no>_^_17_^_</span>\u662f\u6279\u6b21\u5927\u5c0f\uff0c<span translate=no>_^_18_^_</span>\u662f\u8981\u7d20\u7684\u6570\u91cf\uff0c<span translate=no>_^_19_^_</span>\u662f\u987a\u5e8f\u3002<span translate=no>_^_20_^_</span>\u548c<span translate=no>_^_21_^_</span>\u3002<span translate=no>_^_22_^_</span></p>\n",
 "<h2>Inference</h2>\n": "<h2>\u63a8\u65ad</h2>\n",
 "<h2>Normalization</h2>\n": "<h2>\u89c4\u8303\u5316</h2>\n",
 "<h3>Batch Normalization</h3>\n": "<h3>\u6279\u91cf\u6807\u51c6\u5316</h3>\n",
 "<h3>Internal Covariate Shift</h3>\n": "<h3>\u5185\u90e8\u534f\u53d8\u91cf\u79fb\u4f4d</h3>\n",
 "<h3>Normalizing outside gradient computation doesn&#x27;t work</h3>\n": "<h3>\u5bf9\u5916\u90e8\u68af\u5ea6\u8ba1\u7b97\u8fdb\u884c\u5f52\u4e00\u5316\u4e0d\u8d77\u4f5c\u7528</h3>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> <span translate=no>_^_0_^_</span> is a tensor of shape <span translate=no>_^_1_^_</span>. <span translate=no>_^_2_^_</span> denotes any number of (possibly 0) dimensions.  For example, in an image (2D) convolution this will be <span translate=no>_^_3_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span>\u662f\u5f62\u72b6\u5f20\u91cf<span translate=no>_^_1_^_</span>\u3002<span translate=no>_^_2_^_</span>\u8868\u793a\u4efb\u610f\u6570\u91cf\uff08\u53ef\u80fd\u4e3a 0\uff09\u7684\u7ef4\u5ea6\u3002\u4f8b\u5982\uff0c\u5728\u56fe\u50cf\uff082D\uff09\u5377\u79ef\u4e2d\uff0c\u8fd9\u5c06\u662f<span translate=no>_^_3_^_</span></p>\n",
 "<p> Simple test</p>\n": "<p>\u7b80\u5355\u6d4b\u8bd5</p>\n",
 "<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/batch_norm/mnist.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n": "<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/batch_norm/mnist.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n",
 "<p>Batch normalization also makes the back propagation invariant to the scale of the weights and empirically it improves generalization, so it has regularization effects too.</p>\n": "<p>\u6279\u91cf\u5f52\u4e00\u5316\u8fd8\u4f7f\u53cd\u5411\u4f20\u64ad\u4e0e\u6743\u91cd\u7684\u6bd4\u4f8b\u4fdd\u6301\u4e0d\u53d8\uff0c\u4ece\u7ecf\u9a8c\u4e0a\u8bb2\uff0c\u5b83\u6539\u5584\u4e86\u6cdb\u5316\uff0c\u56e0\u6b64\u5b83\u4e5f\u5177\u6709\u6b63\u5219\u5316\u6548\u679c\u3002</p>\n",
 "<p>By stabilizing the distribution, batch normalization minimizes the internal covariate shift.</p>\n": "<p>\u901a\u8fc7\u7a33\u5b9a\u5206\u5e03\uff0c\u6279\u91cf\u5f52\u4e00\u5316\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u3002</p>\n",
 "<p>Calculate the mean across first and last dimension; i.e. the means for each feature <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8ba1\u7b97\u7b2c\u4e00\u7ef4\u548c\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u7684\u5e73\u5747\u503c\uff1b\u5373\u6bcf\u4e2a\u8981\u7d20\u7684\u5747\u503c<span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate the squared mean across first and last dimension; i.e. the means for each feature <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8ba1\u7b97\u7b2c\u4e00\u7ef4\u548c\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u7684\u5747\u65b9\u503c\uff1b\u5373\u6bcf\u4e2a\u8981\u7d20\u7684\u5747\u503c<span translate=no>_^_0_^_</span></p>\n",
 "<p>Create buffers to store exponential moving averages of mean <span translate=no>_^_0_^_</span> and variance <span translate=no>_^_1_^_</span> </p>\n": "<p>\u521b\u5efa\u7f13\u51b2\u533a\u4ee5\u5b58\u50a8\u5747\u503c<span translate=no>_^_0_^_</span>\u548c\u65b9\u5dee\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7ebf<span translate=no>_^_1_^_</span></p>\n",
 "<p>Create parameters for <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> for scale and shift </p>\n": "<p><span translate=no>_^_1_^_</span>\u4e3a\u7f29\u653e<span translate=no>_^_0_^_</span>\u548c\u79fb\u4f4d\u521b\u5efa\u53c2\u6570</p>\n",
 "<p>Get the batch size </p>\n": "<p>\u83b7\u53d6\u6279\u6b21\u5927\u5c0f</p>\n",
 "<p>Here&#x27;s <a href=\"mnist.html\">the training code</a> and a notebook for training a CNN classifier that uses batch normalization for MNIST dataset.</p>\n": "<p><a href=\"mnist.html\">\u4ee5\u4e0b\u662f\u8bad\u7ec3\u4ee3\u7801</a>\u548c\u7528\u4e8e\u8bad\u7ec3 CNN \u5206\u7c7b\u5668\u7684\u7b14\u8bb0\u672c\uff0c\u8be5\u5206\u7c7b\u5668\u4f7f\u7528 MNIST \u6570\u636e\u96c6\u7684\u6279\u91cf\u5f52\u4e00\u5316\u3002</p>\n",
 "<p>Internal covariate shift will adversely affect training speed because the later layers (<span translate=no>_^_0_^_</span> in the above example) have to adapt to this shifted distribution.</p>\n": "<p>\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u5c06\u5bf9\u8bad\u7ec3\u901f\u5ea6\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\uff0c\u56e0\u4e3a\u540e\u9762\u7684\u56fe\u5c42\uff08\u5728\u4e0a\u9762\u7684\u4f8b\u5b50<span translate=no>_^_0_^_</span>\u4e2d\uff09\u5fc5\u987b\u9002\u5e94\u8fd9\u79cd\u504f\u79fb\u5206\u5e03\u3002</p>\n",
 "<p>It is known that whitening improves training speed and convergence. <em>Whitening</em> is linearly transforming inputs to have zero mean, unit variance, and be uncorrelated.</p>\n": "<p>\u4f17\u6240\u5468\u77e5\uff0c\u7f8e\u767d\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6\u548c\u6536\u655b\u6027\u3002<em>\u7f8e\u767d</em>\u662f\u5c06\u8f93\u5165\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\uff0c\u4f7f\u5176\u5747\u503c\u4e3a\u96f6\u3001\u5355\u4f4d\u65b9\u5dee\u4e14\u4e0d\u76f8\u5173\u3002</p>\n",
 "<p>Keep the original shape </p>\n": "<p>\u4fdd\u6301\u539f\u59cb\u5f62\u72b6</p>\n",
 "<p>Normalize <span translate=no>_^_0_^_</span> </p>\n": "<p>\u89c4\u8303\u5316<span translate=no>_^_0_^_</span></p>\n",
 "<p>Normalizing each feature to zero mean and unit variance could affect what the layer can represent. As an example paper illustrates that, if the inputs to a sigmoid are normalized most of it will be within <span translate=no>_^_0_^_</span> range where the sigmoid is linear. To overcome this each feature is scaled and shifted by two trained parameters <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span>. <span translate=no>_^_3_^_</span> where <span translate=no>_^_4_^_</span> is the output of the batch normalization layer.</p>\n": "<p>\u5c06\u6bcf\u4e2a\u8981\u7d20\u5f52\u4e00\u5316\u4e3a\u96f6\u5747\u503c\u548c\u5355\u4f4d\u65b9\u5dee\u53ef\u80fd\u4f1a\u5f71\u54cd\u56fe\u5c42\u53ef\u4ee5\u8868\u793a\u7684\u5185\u5bb9\u3002\u4f5c\u4e3a\u793a\u4f8b\u8bba\u6587\u8bf4\u660e\uff0c\u5982\u679csigmoid\u7684\u8f93\u5165\u88ab\u5f52\u4e00\u5316\uff0c\u5219\u5927\u90e8\u5206\u5c06\u5728sigmoid\u4e3a\u7ebf\u6027\u7684<span translate=no>_^_0_^_</span>\u8303\u56f4\u5185\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6bcf\u4e2a\u7279\u5f81\u90fd\u901a\u8fc7\u4e24\u4e2a\u7ecf\u8fc7\u8bad\u7ec3\u7684\u53c2\u6570\u8fdb\u884c\u7f29\u653e<span translate=no>_^_1_^_</span>\u548c\u79fb\u52a8<span translate=no>_^_2_^_</span>\u3002<span translate=no>_^_3_^_</span>\u5176\u4e2d<span translate=no>_^_4_^_</span>\u662f\u6279\u91cf\u5f52\u4e00\u5316\u5c42\u7684\u8f93\u51fa\u3002</p>\n",
 "<p>Normalizing outside the gradient computation using pre-computed (detached) means and variances doesn&#x27;t work. For instance. (ignoring variance), let <span translate=no>_^_0_^_</span> where <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> is a trained bias and <span translate=no>_^_3_^_</span> is an outside gradient computation (pre-computed constant).</p>\n": "<p>\u4f7f\u7528\u9884\u5148\u8ba1\u7b97\uff08\u5206\u79bb\uff09\u5747\u503c\u548c\u65b9\u5dee\u5728\u68af\u5ea6\u8ba1\u7b97\u4e4b\u5916\u8fdb\u884c\u5f52\u4e00\u5316\u4e0d\u8d77\u4f5c\u7528\u3002\u4f8b\u5982\u3002\uff08\u5ffd\u7565\u65b9\u5dee\uff09<span translate=no>_^_0_^_</span>\uff0c\u8ba9 wher<span translate=no>_^_1_^_</span> e an<span translate=no>_^_2_^_</span> d \u662f\u4e00\u4e2a\u8bad\u7ec3\u8fc7\u7684\u504f\u5dee\uff0c<span translate=no>_^_3_^_</span>\u662f\u5916\u90e8\u68af\u5ea6\u8ba1\u7b97\uff08\u9884\u5148\u8ba1\u7b97\u7684\u5e38\u91cf\uff09\u3002</p>\n",
 "<p>Note that <span translate=no>_^_0_^_</span> has no effect on <span translate=no>_^_1_^_</span>. Therefore, <span translate=no>_^_2_^_</span> will increase or decrease based <span translate=no>_^_3_^_</span>, and keep on growing indefinitely in each training update. The paper notes that similar explosions happen with variances.</p>\n": "<p>\u8bf7\u6ce8\u610f<span translate=no>_^_0_^_</span>\uff0c\u8fd9\u5bf9<span translate=no>_^_1_^_</span>\u3002\u56e0\u6b64\uff0c<span translate=no>_^_2_^_</span>\u5c06\u5728\u6bcf\u6b21\u8bad\u7ec3\u66f4\u65b0\u4e2d\u589e\u52a0\u6216\u51cf\u5c11<span translate=no>_^_3_^_</span>\uff0c\u5e76\u4e14\u4f1a\u65e0\u9650\u671f\u5730\u589e\u957f\u3002\u8be5\u62a5\u6307\u51fa\uff0c\u7c7b\u4f3c\u7684\u7206\u70b8\u4f1a\u53d1\u751f\u5dee\u5f02\u3002</p>\n",
 "<p>Note that when applying batch normalization after a linear transform like <span translate=no>_^_0_^_</span> the bias parameter <span translate=no>_^_1_^_</span> gets cancelled due to normalization. So you can and should omit bias parameter in linear transforms right before the batch normalization.</p>\n": "<p>\u8bf7\u6ce8\u610f\uff0c\u5728\u7ebf\u6027\u53d8\u6362\u4e4b\u540e\u5e94\u7528\u6279\u91cf\u5f52\u4e00\u5316\u65f6\uff0c\u6bd4\u5982<span translate=no>_^_0_^_</span>\u504f\u7f6e\u53c2\u6570<span translate=no>_^_1_^_</span>\u4f1a\u56e0\u5f52\u4e00\u5316\u800c\u88ab\u53d6\u6d88\u3002\u56e0\u6b64\uff0c\u4f60\u53ef\u4ee5\u800c\u4e14\u5e94\u8be5\u5728\u6279\u91cf\u5f52\u4e00\u5316\u4e4b\u524d\u7701\u7565\u7ebf\u6027\u53d8\u6362\u4e2d\u7684\u504f\u7f6e\u53c2\u6570\u3002</p>\n",
 "<p>Reshape into <span translate=no>_^_0_^_</span> </p>\n": "<p>\u91cd\u5851\u6210<span translate=no>_^_0_^_</span></p>\n",
 "<p>Reshape to original and return </p>\n": "<p>\u91cd\u5851\u4e3a\u539f\u59cb\u5f62\u72b6\u7136\u540e\u8fd4\u56de</p>\n",
 "<p>Sanity check to make sure the number of features is the same </p>\n": "<p>\u8fdb\u884c\u5065\u5168\u6027\u68c0\u67e5\u4ee5\u786e\u4fdd\u8981\u7d20\u6570\u91cf\u76f8\u540c</p>\n",
 "<p>Scale and shift <span translate=no>_^_0_^_</span> </p>\n": "<p>\u7f29\u653e\u548c\u79fb\u52a8<span translate=no>_^_0_^_</span></p>\n",
 "<p>The paper defines <em>Internal Covariate Shift</em> as the change in the distribution of network activations due to the change in network parameters during training. For example, let&#x27;s say there are two layers <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span>. During the beginning of the training <span translate=no>_^_2_^_</span> outputs (inputs to <span translate=no>_^_3_^_</span>) could be in distribution <span translate=no>_^_4_^_</span>. Then, after some training steps, it could move to <span translate=no>_^_5_^_</span>. This is <em>internal covariate shift</em>.</p>\n": "<p>\u672c\u6587\u5c06<em>\u5185\u90e8\u534f\u53d8\u91cf\u79fb\u4f4d</em>\u5b9a\u4e49\u4e3a\u8bad\u7ec3\u671f\u95f4\u7531\u4e8e\u7f51\u7edc\u53c2\u6570\u7684\u53d8\u5316\u800c\u5bfc\u81f4\u7684\u7f51\u7edc\u6fc0\u6d3b\u5206\u5e03\u7684\u53d8\u5316\u3002\u4f8b\u5982\uff0c\u5047\u8bbe\u6709\u4e24\u5c42<span translate=no>_^_0_^_</span>\u548c<span translate=no>_^_1_^_</span>\u3002\u5728\u57f9\u8bad\u5f00\u59cb\u65f6\uff0c\u53ef\u4ee5\u5206\u53d1<span translate=no>_^_2_^_</span>\u8f93\u51fa\uff08\u8f93\u5165<span translate=no>_^_3_^_</span>\uff09<span translate=no>_^_4_^_</span>\u3002\u7136\u540e\uff0c\u7ecf\u8fc7\u4e00\u4e9b\u8bad\u7ec3\u6b65\u9aa4\u540e\uff0c\u5b83\u53ef\u80fd\u4f1a\u79fb\u81f3<span translate=no>_^_5_^_</span>\u3002\u8fd9\u662f<em>\u5185\u90e8\u534f\u53d8\u91cf\u79fb\u4f4d</em>\u3002</p>\n",
 "<p>The paper introduces a simplified version which they call <em>Batch Normalization</em>. First simplification is that it normalizes each feature independently to have zero mean and unit variance: <span translate=no>_^_0_^_</span> where <span translate=no>_^_1_^_</span> is the <span translate=no>_^_2_^_</span>-dimensional input.</p>\n": "<p>\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7b80\u5316\u7248\u672c\uff0c\u4ed6\u4eec\u79f0\u4e4b\u4e3a<em>\u6279\u91cf\u89c4\u8303\u5316</em>\u3002\u9996\u5148\u7b80\u5316\u7684\u662f\uff0c\u5b83\u5c06\u6bcf\u4e2a\u8981\u7d20\u72ec\u7acb\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u5355\u4f4d\u65b9\u5dee\u4e3a\u96f6\uff1a<span translate=no>_^_0_^_</span>\u5176\u4e2d<span translate=no>_^_1_^_</span>\u662f<span translate=no>_^_2_^_</span>\u7ef4\u5ea6\u8f93\u5165\u3002</p>\n",
 "<p>The second simplification is to use estimates of mean <span translate=no>_^_0_^_</span> and variance <span translate=no>_^_1_^_</span> from the mini-batch for normalization; instead of calculating the mean and variance across the whole dataset.</p>\n": "<p>\u7b2c\u4e8c\u79cd\u7b80\u5316\u65b9\u6cd5\u662f\u4f7f\u7528<span translate=no>_^_1_^_</span>\u6765\u81ea\u5fae\u578b\u6279\u6b21\u7684\u5747\u503c<span translate=no>_^_0_^_</span>\u548c\u65b9\u5dee\u7684\u4f30\u8ba1\u503c\u8fdb\u884c\u5f52\u4e00\u5316\uff1b\u800c\u4e0d\u662f\u8ba1\u7b97\u6574\u4e2a\u6570\u636e\u96c6\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002</p>\n",
 "<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of Batch Normalization from paper  <a href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>.</p>\n": "<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch</a> \u4ece\u7eb8\u8d28\u6279\u91cf\u89c4\u8303\u5316\u4e2d<a href=\"https://arxiv.org/abs/1502.03167\">\u5b9e\u73b0\u6279\u91cf\u5f52\u4e00\u5316\uff1a\u901a\u8fc7\u51cf\u5c11\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u52a0\u901f\u6df1\u5ea6\u7f51\u7edc\u8bad\u7ec3</a>\u3002</p>\n",
 "<p>Update exponential moving averages </p>\n": "<p>\u66f4\u65b0\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7ebf</p>\n",
 "<p>Use exponential moving averages as estimates </p>\n": "<p>\u4f7f\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7ebf\u4f5c\u4e3a\u4f30\u8ba1\u503c</p>\n",
 "<p>Variance for each feature <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6bcf\u4e2a\u8981\u7d20\u7684\u65b9\u5dee<span translate=no>_^_0_^_</span></p>\n",
 "<p>We need to know <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> in order to perform the normalization. So during inference, you either need to go through the whole (or part of) dataset and find the mean and variance, or you can use an estimate calculated during training. The usual practice is to calculate an exponential moving average of mean and variance during the training phase and use that for inference.</p>\n": "<p>\u6211\u4eec\u9700\u8981\u77e5\u9053 an<span translate=no>_^_0_^_</span> d<span translate=no>_^_1_^_</span> \u624d\u80fd\u6267\u884c\u89c4\u8303\u5316\u3002\u56e0\u6b64\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u60a8\u8981\u4e48\u9700\u8981\u904d\u5386\u6574\u4e2a\uff08\u6216\u90e8\u5206\uff09\u6570\u636e\u96c6\u5e76\u627e\u5230\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8981\u4e48\u53ef\u4ee5\u4f7f\u7528\u8bad\u7ec3\u671f\u95f4\u8ba1\u7b97\u7684\u4f30\u8ba1\u503c\u3002\u901a\u5e38\u7684\u505a\u6cd5\u662f\u5728\u8bad\u7ec3\u9636\u6bb5\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7ebf\uff0c\u7136\u540e\u5c06\u5176\u7528\u4e8e\u63a8\u65ad\u3002</p>\n",
 "<p>We will calculate the mini-batch mean and variance if we are in training mode or if we have not tracked exponential moving averages </p>\n": "<p>\u5982\u679c\u6211\u4eec\u5904\u4e8e\u8bad\u7ec3\u6a21\u5f0f\u6216\u8005\u6ca1\u6709\u8ddf\u8e2a\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7ebf\uff0c\u6211\u4eec\u5c06\u8ba1\u7b97\u5c0f\u6279\u6b21\u5747\u503c\u548c\u65b9\u5dee</p>\n",
 "<p>Whitening is computationally expensive because you need to de-correlate and the gradients must flow through the full whitening calculation.</p>\n": "<p>\u7f8e\u767d\u5728\u8ba1\u7b97\u4e0a\u5f88\u6602\u8d35\uff0c\u56e0\u4e3a\u4f60\u9700\u8981\u53bb\u5173\u8054\uff0c\u800c\u4e14\u68af\u5ea6\u5fc5\u987b\u901a\u8fc7\u5b8c\u6574\u7684\u7f8e\u767d\u8ba1\u7b97\u3002</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features in the input </li>\n<li><span translate=no>_^_1_^_</span> is <span translate=no>_^_2_^_</span>, used in <span translate=no>_^_3_^_</span> for numerical stability </li>\n<li><span translate=no>_^_4_^_</span> is the momentum in taking the exponential moving average </li>\n<li><span translate=no>_^_5_^_</span> is whether to scale and shift the normalized value </li>\n<li><span translate=no>_^_6_^_</span> is whether to calculate the moving averages or mean and variance</li></ul>\n<p>We&#x27;ve tried to use the same names for arguments as PyTorch <span translate=no>_^_7_^_</span> implementation.</p>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u8f93\u5165\u4e2d\u7684\u8981\u7d20\u6570</li>\n<li><span translate=no>_^_1_^_</span>\u662f<span translate=no>_^_2_^_</span>\uff0c<span translate=no>_^_3_^_</span>\u7528\u4e8e\u6570\u503c\u7a33\u5b9a\u6027</li>\n<li><span translate=no>_^_4_^_</span>\u662f\u53d6\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7ebf\u7684\u52a8\u91cf</li>\n<li><span translate=no>_^_5_^_</span>\u662f\u5426\u7f29\u653e\u548c\u79fb\u52a8\u5f52\u4e00\u5316\u503c</li>\n<li><span translate=no>_^_6_^_</span>\u662f\u8ba1\u7b97\u79fb\u52a8\u5e73\u5747\u7ebf\u8fd8\u662f\u5747\u503c\u548c\u65b9\u5dee</li></ul>\n<p>\u6211\u4eec\u5df2\u7ecf\u5c1d\u8bd5\u4f7f\u7528\u4e0e PyTorch<span translate=no>_^_7_^_</span> \u5b9e\u73b0\u76f8\u540c\u7684\u53c2\u6570\u540d\u79f0\u3002</p>\n",
 "A PyTorch implementation/tutorial of batch normalization.": "\u4e00\u4e2a\u5173\u4e8e\u6279\u91cf\u89c4\u8303\u5316\u7684 PyTorch \u5b9e\u73b0/\u6559\u7a0b\u3002",
 "Batch Normalization": "\u6279\u91cf\u6807\u51c6\u5316"
}