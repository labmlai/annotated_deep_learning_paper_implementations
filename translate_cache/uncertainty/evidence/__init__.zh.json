{
 "<h1>Evidential Deep Learning to Quantify Classification Uncertainty</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/1806.01768\">Evidential Deep Learning to Quantify Classification Uncertainty</a>.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory\">Dampster-Shafer Theory of Evidence</a> assigns belief masses a set of classes (unlike assigning a probability to a single class). Sum of the masses of all subsets is <span translate=no>_^_0_^_</span>. Individual class probabilities (plausibilities) can be derived from these masses.</p>\n<p>Assigning a mass to the set of all classes means it can be any one of the classes; i.e. saying &quot;I don&#x27;t know&quot;.</p>\n<p>If there are <span translate=no>_^_1_^_</span> classes, we assign masses <span translate=no>_^_2_^_</span> to each of the classes and  an overall uncertainty mass <span translate=no>_^_3_^_</span> to all classes.</p>\n<p><span translate=no>_^_4_^_</span></p>\n<p>Belief masses <span translate=no>_^_5_^_</span> and <span translate=no>_^_6_^_</span> can be computed from evidence <span translate=no>_^_7_^_</span>, as <span translate=no>_^_8_^_</span> and <span translate=no>_^_9_^_</span> where <span translate=no>_^_10_^_</span>. Paper uses term evidence as a measure of the amount of support collected from data in favor of a sample to be classified into a certain class.</p>\n<p>This corresponds to a <a href=\"https://en.wikipedia.org/wiki/Dirichlet_distribution\">Dirichlet distribution</a> with parameters <span translate=no>_^_11_^_</span>, and  <span translate=no>_^_12_^_</span> is known as the Dirichlet strength. Dirichlet distribution <span translate=no>_^_13_^_</span>  is a distribution over categorical distribution; i.e. you can sample class probabilities from a Dirichlet distribution. The expected probability for class <span translate=no>_^_14_^_</span> is <span translate=no>_^_15_^_</span>.</p>\n<p>We get the model to output evidences <span translate=no>_^_16_^_</span>  for a given input <span translate=no>_^_17_^_</span>. We use a function such as  <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\">ReLU</a> or a  <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html\">Softplus</a>  at the final layer to get <span translate=no>_^_18_^_</span>.</p>\n<p>The paper proposes a few loss functions to train the model, which we have implemented below.</p>\n<p>Here is the <a href=\"experiment.html\">training code <span translate=no>_^_19_^_</span></a> to train a model on MNIST dataset.</p>\n": "<h1>\u7528\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u6765\u91cf\u5316\u5206\u7c7b\u4e0d\u786e\u5b9a\u6027</h1>\n<p>\u8fd9\u662f P <a href=\"https://pytorch.org\">yTorch</a> \u5bf9\u300a<a href=\"https://arxiv.org/abs/1806.01768\">\u91cf\u5316\u5206\u7c7b\u4e0d\u786e\u5b9a\u6027\u7684\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60</a>\u300b\u8bba\u6587\u7684\u5b9e\u73b0\u3002</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory\">Dampster-Shafer \u8bc1\u636e\u7406\u8bba</a>\u4e3a\u4fe1\u4ef0\u7fa4\u4f53\u5206\u914d\u4e86\u4e00\u7ec4\u7c7b\u522b\uff08\u4e0e\u4e3a\u5355\u4e2a\u7c7b\u522b\u5206\u914d\u6982\u7387\u4e0d\u540c\uff09\u3002\u6240\u6709\u5b50\u96c6\u7684\u8d28\u91cf\u603b\u548c\u4e3a<span translate=no>_^_0_^_</span>\u3002\u4e2a\u522b\u7c7b\u522b\u7684\u6982\u7387\uff08\u5408\u7406\u6027\uff09\u53ef\u4ee5\u4ece\u8fd9\u4e9b\u8d28\u91cf\u4e2d\u63a8\u5bfc\u51fa\u6765\u3002</p>\n<p>\u4e3a\u6240\u6709\u7c7b\u522b\u7684\u96c6\u5408\u5206\u914d\u8d28\u91cf\u610f\u5473\u7740\u5b83\u53ef\u4ee5\u662f\u4efb\u4f55\u4e00\u4e2a\u7c7b\u522b\uff1b\u5373\u8bf4 \u201c\u6211\u4e0d\u77e5\u9053\u201d\u3002</p>\n<p>\u5982\u679c\u6709<span translate=no>_^_1_^_</span>\u7b49\u7ea7\uff0c\u6211\u4eec<span translate=no>_^_2_^_</span>\u4e3a\u6bcf\u4e2a\u7b49\u7ea7\u5206\u914d\u8d28\u91cf\uff0c\u4e3a\u6240\u6709\u7c7b\u522b\u5206\u914d\u603b\u4f53\u4e0d<span translate=no>_^_3_^_</span>\u786e\u5b9a\u6027\u8d28\u91cf\u3002</p>\n<p><span translate=no>_^_4_^_</span></p>\n<p>\u4fe1\u4ef0\u4f17<span translate=no>_^_5_^_</span>\u591a\uff0c<span translate=no>_^_6_^_</span>\u53ef\u4ee5\u6839\u636e\u8bc1\u636e\u8ba1\u7b97\u5f97\u51fa<span translate=no>_^_7_^_</span><span translate=no>_^_8_^_</span>\uff0c\u65e0\u8bba<span translate=no>_^_9_^_</span>\u5728\u4f55\u5904<span translate=no>_^_10_^_</span>\u3002Paper \u4f7f\u7528\u672f\u8bed\u8bc1\u636e\u6765\u8861\u91cf\u4ece\u6570\u636e\u4e2d\u6536\u96c6\u7684\u652f\u6301\u91cf\uff0c\u652f\u6301\u5c06\u6837\u672c\u5f52\u7c7b\u4e3a\u7279\u5b9a\u7c7b\u522b\u3002</p>\n<p>\u8fd9\u5bf9\u5e94\u4e8e\u5e26\u6709\u53c2\u6570\u7684<a href=\"https://en.wikipedia.org/wiki/Dirichlet_distribution\">\u72c4\u5229\u514b\u96f7\u5206\u5e03</a><span translate=no>_^_11_^_</span>\uff0c\u88ab<span translate=no>_^_12_^_</span>\u79f0\u4e3a\u72c4\u5229\u514b\u96f7\u5f3a\u5ea6\u3002\u72c4\u5229\u514b\u96f7\u5206\u5e03<span translate=no>_^_13_^_</span>\u662f\u7c7b\u522b\u5206\u5e03\u4e0a\u7684\u5206\u5e03\uff1b\u4e5f\u5c31\u662f\u8bf4\uff0c\u4f60\u53ef\u4ee5\u4ece\u72c4\u5229\u514b\u96f7\u5206\u5e03\u4e2d\u62bd\u53d6\u7c7b\u6982\u7387\u3002\u4e0a\u8bfe\u7684\u9884\u671f\u6982\u7387<span translate=no>_^_14_^_</span>\u662f<span translate=no>_^_15_^_</span>\u3002</p>\n<p>\u6211\u4eec\u8ba9\u6a21\u578b\u8f93\u51fa\u7ed9\u5b9a\u8f93\u5165<span translate=no>_^_16_^_</span>\u7684\u8bc1\u636e<span translate=no>_^_17_^_</span>\u3002\u6211\u4eec\u5728\u6700\u540e\u4e00\u5c42\u4f7f\u7528\u8bf8\u5982 <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\">ReLU</a> \u6216 <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html\">Softplus</a> \u4e4b\u7c7b\u7684\u51fd\u6570\u6765\u83b7\u53d6<span translate=no>_^_18_^_</span>\u3002</p>\n<p>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e9b\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\uff0c\u6211\u4eec\u5728\u4e0b\u9762\u5b9e\u73b0\u4e86\u8fd9\u4e9b\u51fd\u6570\u3002</p>\n<p>\u8fd9\u662f\u5728 MNIST \u6570\u636e\u96c6\u4e0a<a href=\"experiment.html\">\u8bad\u7ec3\u6a21\u578b\u7684\u8bad\u7ec3\u4ee3\u7801<span translate=no>_^_19_^_</span></a>\u3002</p>\n",
 "<p> <a id=\"CrossEntropyBayesRisk\"></a></p>\n<h2>Bayes Risk with Cross Entropy Loss</h2>\n<p>Bayes risk is the overall maximum cost of making incorrect estimates. It takes a cost function that gives the cost of making an incorrect estimate and sums it over all possible outcomes based on probability distribution.</p>\n<p>Here the cost function is cross-entropy loss, for one-hot coded <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span></p>\n<p>We integrate this cost over all <span translate=no>_^_2_^_</span></p>\n<span translate=no>_^_3_^_</span><p>where <span translate=no>_^_4_^_</span> is the <span translate=no>_^_5_^_</span> function.</p>\n": "<p><a id=\"CrossEntropyBayesRisk\"></a></p>\n<h2>\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u8d1d\u53f6\u65af\u98ce\u9669</h2>\n<p>\u8d1d\u53f6\u65af\u98ce\u9669\u662f\u505a\u51fa\u9519\u8bef\u4f30\u7b97\u7684\u603b\u4f53\u6700\u5927\u6210\u672c\u3002\u5b83\u91c7\u7528\u4e00\u4e2a\u6210\u672c\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u7ed9\u51fa\u4e86\u505a\u51fa\u9519\u8bef\u4f30\u8ba1\u7684\u6210\u672c\uff0c\u5e76\u6839\u636e\u6982\u7387\u5206\u5e03\u5c06\u5176\u4e0e\u6240\u6709\u53ef\u80fd\u7684\u7ed3\u679c\u76f8\u52a0\u3002</p>\n<p>\u8fd9\u91cc\u7684\u4ee3\u4ef7\u51fd\u6570\u662f\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u7528\u4e8e\u4e00\u6b21\u70ed\u7f16\u7801<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span></p>\n<p>\u6211\u4eec\u6574\u5408\u4e86\u8fd9\u4e2a\u6210\u672c<span translate=no>_^_2_^_</span></p>\n<span translate=no>_^_3_^_</span><p><span translate=no>_^_5_^_</span>\u51fd\u6570\u5728<span translate=no>_^_4_^_</span>\u54ea\u91cc\u3002</p>\n",
 "<p> <a id=\"KLDivergenceLoss\"></a></p>\n<h2>KL Divergence Regularization Loss</h2>\n<p>This tries to shrink the total evidence to zero if the sample cannot be correctly classified.</p>\n<p>First we calculate <span translate=no>_^_0_^_</span> the Dirichlet parameters after remove the correct evidence.</p>\n<span translate=no>_^_1_^_</span><p>where <span translate=no>_^_2_^_</span> is the gamma function, <span translate=no>_^_3_^_</span> is the <span translate=no>_^_4_^_</span> function and <span translate=no>_^_5_^_</span></p>\n": "<p><a id=\"KLDivergenceLoss\"></a></p>\n<h2>KL \u80cc\u79bb\u6b63\u5219\u5316\u635f\u5931</h2>\n<p>\u5982\u679c\u6837\u672c\u65e0\u6cd5\u6b63\u786e\u5206\u7c7b\uff0c\u8fd9\u4f1a\u8bd5\u56fe\u5c06\u603b\u8bc1\u636e\u7f29\u5c0f\u4e3a\u96f6\u3002</p>\n<p>\u9996\u5148<span translate=no>_^_0_^_</span>\uff0c\u6211\u4eec\u5728\u79fb\u9664\u6b63\u786e\u7684\u8bc1\u636e\u540e\u8ba1\u7b97\u72c4\u5229\u514b\u96f7\u53c2\u6570\u3002</p>\n<span translate=no>_^_1_^_</span><p>\u5176\u4e2d<span translate=no>_^_2_^_</span>\u662f gamma \u51fd\u6570\uff0c<span translate=no>_^_3_^_</span>\u662f<span translate=no>_^_4_^_</span>\u51fd\u6570\u548c<span translate=no>_^_5_^_</span></p>\n",
 "<p> <a id=\"MaximumLikelihoodLoss\"></a></p>\n<h2>Type II Maximum Likelihood Loss</h2>\n<p>The distribution <span translate=no>_^_0_^_</span> is a prior on the likelihood <span translate=no>_^_1_^_</span>,  and the negative log marginal likelihood is calculated by integrating over class probabilities  <span translate=no>_^_2_^_</span>.</p>\n<p>If target probabilities (one-hot targets) are <span translate=no>_^_3_^_</span> for a given sample the loss is,</p>\n<span translate=no>_^_4_^_</span>": "<p><a id=\"MaximumLikelihoodLoss\"></a></p>\n<h2>\u7c7b\u578b II \u6700\u5927\u4f3c\u7136\u635f\u5931</h2>\n<p>\u5206\u5e03<span translate=no>_^_0_^_</span>\u662f\u4f3c\u7136\u7684\u5148\u9a8c<span translate=no>_^_1_^_</span>\uff0c\u8d1f\u5bf9\u6570\u8fb9\u9645\u4f3c\u7136\u662f\u901a\u8fc7\u79ef\u5206\u7c7b\u6982\u7387\u6765\u8ba1\u7b97\u7684<span translate=no>_^_2_^_</span>\u3002</p>\n<p>\u5982\u679c\u76ee\u6807\u6982\u7387\uff08\u4e00\u70ed\u76ee\u6807\uff09\u662f<span translate=no>_^_3_^_</span>\u9488\u5bf9\u7ed9\u5b9a\u6837\u672c\u7684\uff0c\u5219\u635f\u5931\u4e3a\uff0c</p>\n<span translate=no>_^_4_^_</span>",
 "<p> <a id=\"SquaredErrorBayesRisk\"></a></p>\n<h2>Bayes Risk with Squared Error Loss</h2>\n<p>Here the cost function is squared error, <span translate=no>_^_0_^_</span></p>\n<p>We integrate this cost over all <span translate=no>_^_1_^_</span></p>\n<span translate=no>_^_2_^_</span><p>Where <span translate=no>_^_3_^_</span> is the expected probability when sampled from the Dirichlet distribution and <span translate=no>_^_4_^_</span>  where <span translate=no>_^_5_^_</span>  is the variance.</p>\n<p>This gives,</p>\n<span translate=no>_^_6_^_</span><p>This first part of the equation <span translate=no>_^_7_^_</span> is the error term and the second part is the variance.</p>\n": "<p><a id=\"SquaredErrorBayesRisk\"></a></p>\n<h2>\u8bef\u5dee\u635f\u5931\u5e73\u65b9\u65f6\u7684\u8d1d\u53f6\u65af\u98ce\u9669</h2>\n<p>\u8fd9\u91cc\u7684\u6210\u672c\u51fd\u6570\u662f\u5e73\u65b9\u8bef\u5dee\uff0c<span translate=no>_^_0_^_</span></p>\n<p>\u6211\u4eec\u6574\u5408\u4e86\u8fd9\u4e2a\u6210\u672c<span translate=no>_^_1_^_</span></p>\n<span translate=no>_^_2_^_</span><p>\u4ece\u72c4<span translate=no>_^_3_^_</span>\u5229\u514b\u96f7\u5206\u5e03\u91c7\u6837\u65f6\u7684\u9884\u671f\u6982\u7387\u5728\u54ea\u91cc\uff0c\u65b9\u5dee<span translate=no>_^_4_^_</span>\u5728<span translate=no>_^_5_^_</span>\u54ea\u91cc\u3002</p>\n<p>\u8fd9\u7ed9\u4e86\uff0c</p>\n<span translate=no>_^_6_^_</span><p>\u65b9\u7a0b\u7684\u7b2c\u4e00\u90e8\u5206<span translate=no>_^_7_^_</span>\u662f\u8bef\u5dee\u9879\uff0c\u7b2c\u4e8c\u90e8\u5206\u662f\u65b9\u5dee\u3002</p>\n",
 "<p> <a id=\"TrackStatistics\"></a></p>\n<h3>Track statistics</h3>\n<p>This module computes statistics and tracks them with <a href=\"https://docs.labml.ai/api/tracker.html\">labml <span translate=no>_^_0_^_</span></a>.</p>\n": "<p><a id=\"TrackStatistics\"></a></p>\n<h3>\u8ffd\u8e2a\u7edf\u8ba1\u6570\u636e</h3>\n<p>\u8be5\u6a21\u5757\u8ba1\u7b97\u7edf\u8ba1\u6570\u636e\u5e76\u4f7f\u7528 <a href=\"https://docs.labml.ai/api/tracker.html\">labml</a> \u5bf9\u5176\u8fdb\u884c\u8ddf\u8e2a<span translate=no>_^_0_^_</span>\u3002</p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>Error <span translate=no>_^_0_^_</span> </p>\n": "<p>\u9519\u8bef<span translate=no>_^_0_^_</span></p>\n",
 "<p>Expected probability of the selected (greedy highset probability) class </p>\n": "<p>\u6240\u9009\uff08\u8d2a\u5a6a\u7684\u6700\u9ad8\u6982\u7387\uff09\u7c7b\u522b\u7684\u9884\u671f\u6982\u7387</p>\n",
 "<p>Losses <span translate=no>_^_0_^_</span> </p>\n": "<p>\u4e8f\u635f<span translate=no>_^_0_^_</span></p>\n",
 "<p>Mean loss over the batch </p>\n": "<p>\u6574\u4e2a\u6279\u6b21\u7684\u5e73\u5747\u635f\u5931</p>\n",
 "<p>Number of classes </p>\n": "<p>\u73ed\u7ea7\u6570</p>\n",
 "<p>Predictions that correctly match with the target (greedy sampling based on highest probability) </p>\n": "<p>\u4e0e\u76ee\u6807\u6b63\u786e\u5339\u914d\u7684\u9884\u6d4b\uff08\u57fa\u4e8e\u6700\u9ad8\u6982\u7387\u7684\u8d2a\u5a6a\u62bd\u6837\uff09</p>\n",
 "<p>Remove non-misleading evidence <span translate=no>_^_0_^_</span> </p>\n": "<p>\u79fb\u9664\u975e\u8bef\u5bfc\u6027\u8bc1\u636e<span translate=no>_^_0_^_</span></p>\n",
 "<p>Sum of the terms </p>\n": "<p>\u6761\u6b3e\u603b\u548c</p>\n",
 "<p>Sum of them </p>\n": "<p>\u5b83\u4eec\u7684\u603b\u548c</p>\n",
 "<p>The first term</p>\n<span translate=no>_^_0_^_</span><p> </p>\n": "<p>\u7b2c\u4e00\u5b66\u671f</p>\n<span translate=no>_^_0_^_</span><p></p>\n",
 "<p>The second term <span translate=no>_^_0_^_</span> </p>\n": "<p>\u7b2c\u4e8c\u5b66\u671f<span translate=no>_^_0_^_</span></p>\n",
 "<p>Track <span translate=no>_^_0_^_</span> for correctly predictions </p>\n": "<p>\u8ffd\u8e2a<span translate=no>_^_0_^_</span>\u6b63\u786e\u7684\u9884\u6d4b</p>\n",
 "<p>Track <span translate=no>_^_0_^_</span> for incorrect predictions </p>\n": "<p>\u8ffd\u8e2a\u9519\u8bef<span translate=no>_^_0_^_</span>\u7684\u9884\u6d4b</p>\n",
 "<p>Track accuracy </p>\n": "<p>\u8f68\u9053\u7cbe\u5ea6</p>\n",
 "<p>Uncertainty mass <span translate=no>_^_0_^_</span> </p>\n": "<p>\u4e0d\u786e\u5b9a\u6027\u8d28\u91cf<span translate=no>_^_0_^_</span></p>\n",
 "<p>Variance <span translate=no>_^_0_^_</span> </p>\n": "<p>\u65b9\u5dee<span translate=no>_^_0_^_</span></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> with shape <span translate=no>_^_2_^_</span> </li>\n<li><span translate=no>_^_3_^_</span> is <span translate=no>_^_4_^_</span> with shape <span translate=no>_^_5_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f<span translate=no>_^_1_^_</span>\u6709\u5f62\u72b6\u7684<span translate=no>_^_2_^_</span></li>\n</ul><li><span translate=no>_^_3_^_</span>\u662f<span translate=no>_^_4_^_</span>\u6709\u5f62\u72b6\u7684<span translate=no>_^_5_^_</span></li>\n",
 "A PyTorch implementation/tutorial of the paper Evidential Deep Learning to Quantify Classification Uncertainty.": "\u8bba\u6587\u300a\u91cf\u5316\u5206\u7c7b\u4e0d\u786e\u5b9a\u6027\u7684\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u300b\u7684 PyTorch \u5b9e\u73b0/\u6559\u7a0b\u3002",
 "Evidential Deep Learning to Quantify Classification Uncertainty": "\u7528\u4e8e\u91cf\u5316\u5206\u7c7b\u4e0d\u786e\u5b9a\u6027\u7684\u8bc1\u636e\u6027\u6df1\u5ea6\u5b66\u4e60"
}