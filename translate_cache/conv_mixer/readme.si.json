{
 " Patches Are All You Need?": " \u0db4\u0dd0\u0da0\u0dca \u0d94\u0db6\u0da7 \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0dc3\u0dd2\u0dba\u0dbd\u0dd4\u0db8 \u0daf?",
 "<h1><a href=\"https://nn.labml.ai/conv_mixer/index.html\">Patches Are All You Need?</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2201.09792\">Patches Are All You Need?</a>.</p>\n<p>ConvMixer is Similar to <a href=\"https://nn.labml.ai/transformers/mlp_mixer/index.html\">MLP-Mixer</a>. MLP-Mixer separates mixing of spatial and channel dimensions, by applying an MLP across spatial dimension and then an MLP across the channel dimension (spatial MLP replaces the <a href=\"https://nn.labml.ai/transformers/vit/index.html\">ViT</a> attention and channel MLP is the <a href=\"https://nn.labml.ai/transformers/feed_forward.html\">FFN</a> of ViT).</p>\n<p>ConvMixer uses a 1x1 convolution for channel mixing and a depth-wise convolution for spatial mixing. Since it&#x27;s a convolution instead of a full MLP across the space, it mixes only the nearby batches in contrast to ViT or MLP-Mixer. Also, the MLP-mixer uses MLPs of two layers for each mixing and ConvMixer uses a single layer for each mixing.</p>\n<p>The paper recommends removing the residual connection across the channel mixing (point-wise convolution) and having only a residual connection over the spatial mixing (depth-wise convolution). They also use <a href=\"https://nn.labml.ai/normalization/batch_norm/index.html\">Batch normalization</a> instead of <a href=\"../normalization/layer_norm/index.html\">Layer normalization</a>.</p>\n<p>Here&#x27;s <a href=\"https://nn.labml.ai/conv_mixer/experiment.html\">an experiment</a> that trains ConvMixer on CIFAR-10.</p>\n<p><a href=\"https://app.labml.ai/run/0fc344da2cd011ecb0bc3fdb2e774a3d\"><span translate=no>_^_0_^_</span></a></p>\n": "<h1><a href=\"https://nn.labml.ai/conv_mixer/index.html\">\u0db4\u0dd0\u0da0\u0dca \u0d94\u0db6\u0da7 \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0dc3\u0dd2\u0dba\u0dbd\u0dd4\u0db8 \u0daf? </a></h1>\n<p>\u0db8\u0dd9\u0dba <a href=\"https://pytorch.org\">PyTorch</a> \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dba\u0dd2 <a href=\"https://arxiv.org/abs/2201.09792\">\u0db4\u0dd0\u0da0\u0dca \u0d94\u0db6\u0da7 \u0d85\u0dc0\u0dc1\u0dca\u0dba\u0daf? </a>. </p>\n<p>ConvMixer <a href=\"https://nn.labml.ai/transformers/mlp_mixer/index.html\">MLP-\u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca</a>\u0dc3\u0db8\u0dcf\u0db1 \u0dc0\u0dda. \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3-\u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0db8\u0dcf\u0db1\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3 \u0dba\u0dd9\u0daf\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0dc4\u0dcf \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0db8\u0dcf\u0db1\u0dba\u0db1\u0dca \u0db8\u0dd2\u0dc1\u0dca\u0dbb \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc0\u0dd9\u0db1\u0dca \u0d9a\u0dbb\u0dba\u0dd2, \u0db4\u0dc3\u0dd4\u0dc0 \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0db8\u0dcf\u0db1\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3 \u0dba\u0dd9\u0daf\u0dd3\u0db8\u0dd9\u0db1\u0dca (\u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3 <a href=\"https://nn.labml.ai/transformers/vit/index.html\">\u0dc0\u0dd3\u0d85\u0dba\u0dd2\u0da7\u0dd3</a> \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0dc3\u0dca\u0dae\u0dcf\u0db4\u0db1\u0dba \u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dbb \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf\u0dc0 \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3 \u0dba\u0db1\u0dd4 \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3 \u0dba <a href=\"https://nn.labml.ai/transformers/feed_forward.html\">FFN</a> \u0dc4\u0ddd VIT). </p>\n<p>ConvMixer\u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0db8\u0dd2\u0dc1\u0dca\u0dbb \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf 1x1 convolution \u0dc3\u0dc4 \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0db8\u0dd2\u0dc1\u0dca\u0dbb \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d9c\u0dd0\u0db9\u0dd4\u0dbb-wise convolution \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. \u0d91\u0dba \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0dc3\u0db8\u0dca\u0db4\u0dd6\u0dbb\u0dca\u0dab \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3 \u0dc0\u0dd9\u0db1\u0dd4\u0dc0\u0da7 \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dca\u0dc0\u0dd3\u0db8\u0d9a\u0dca \u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca, \u0d91\u0dba VIT \u0dc4\u0ddd MLP-\u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca \u0dc0\u0dbd\u0da7 \u0dc0\u0da9\u0dcf \u0dc0\u0dd9\u0db1\u0dc3\u0dca\u0dc0 \u0d85\u0dc3\u0dbd \u0d87\u0dad\u0dd2 \u0d9a\u0dcf\u0dab\u0dca\u0da9 \u0db4\u0db8\u0dab\u0d9a\u0dca \u0db8\u0dd2\u0dc1\u0dca\u0dbb \u0d9a\u0dbb\u0dba\u0dd2. \u0d91\u0dc3\u0dda\u0db8, MLP-\u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db8\u0dd2\u0dc1\u0dca\u0dbb\u0dab\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0dca\u0dae\u0dbb \u0daf\u0dd9\u0d9a\u0d9a MLPs \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dbb ConvMixer \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db8\u0dd2\u0dc1\u0dca\u0dbb\u0dab\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0dad\u0db1\u0dd2 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<p>\u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2\u0db1\u0dd2\u0dbb\u0dca\u0daf\u0dda\u0dc1 \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0db8\u0dd2\u0dc1\u0dca\u0dbb\u0dab\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0d87\u0dad\u0dd2 \u0d85\u0dc0\u0dc1\u0dda\u0dc2 \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dad\u0dcf\u0dc0\u0dba \u0d89\u0dc0\u0dad\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 (\u0dbd\u0d9a\u0dca\u0dc2\u0dca\u0dba-\u0d85\u0db1\u0dd4\u0dc0 \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0db1\u0dca\u0dc0\u0dd2\u0db8) \u0dc3\u0dc4 \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0db8\u0dd2\u0dc1\u0dca\u0dbb\u0dab\u0dba\u0da7 \u0dc0\u0da9\u0dcf \u0d85\u0dc0\u0dc1\u0dda\u0dc2 \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dad\u0dcf\u0dc0\u0dba\u0d9a\u0dca \u0db4\u0db8\u0dab\u0d9a\u0dca \u0dad\u0dd2\u0db6\u0dd3\u0db8 (\u0d9c\u0dd0\u0db9\u0dd4\u0dbb-wise convolution) \u0dba. <a href=\"../normalization/layer_norm/index.html\">\u0dc3\u0dca\u0dae\u0dbb</a> <a href=\"https://nn.labml.ai/normalization/batch_norm/index.html\">\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0dc0\u0dd9\u0db1\u0dd4\u0dc0\u0da7 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba</a> \u0daf \u0d94\u0dc0\u0dd4\u0db1\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<p><a href=\"https://nn.labml.ai/conv_mixer/experiment.html\">CIFRA-10 \u0dc4\u0dd2 ConvMixer \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dca\u0dc4\u0daf\u0dcf \u0db6\u0dd0\u0dbd\u0dd3\u0db8\u0d9a\u0dca</a> \u0db8\u0dd9\u0db1\u0dca\u0db1. </p>\n<p><a href=\"https://app.labml.ai/run/0fc344da2cd011ecb0bc3fdb2e774a3d\"><span translate=no>_^_0_^_</span></a></p>\n"
}