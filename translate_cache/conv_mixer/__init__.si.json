{
 "<h1>Patches Are All You Need? (ConvMixer)</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/2201.09792\">Patches Are All You Need?</a>.</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p>ConvMixer is Similar to <a href=\"../transformers/mlp_mixer/index.html\">MLP-Mixer</a>. MLP-Mixer separates mixing of spatial and channel dimensions, by applying an MLP across spatial dimension and then an MLP across the channel dimension (spatial MLP replaces the <a href=\"../transformers/vit/index.html\">ViT</a> attention and channel MLP is the <a href=\"../transformers/feed_forward.html\">FFN</a> of ViT).</p>\n<p>ConvMixer uses a <span translate=no>_^_1_^_</span> convolution for channel mixing and a depth-wise convolution for spatial mixing. Since it&#x27;s a convolution instead of a full MLP across the space, it mixes only the nearby batches in contrast to ViT or MLP-Mixer. Also, the MLP-mixer uses MLPs of two layers for each mixing and ConvMixer uses a single layer for each mixing.</p>\n<p>The paper recommends removing the residual connection across the channel mixing (point-wise convolution) and having only a residual connection over the spatial mixing (depth-wise convolution). They also use <a href=\"../normalization/batch_norm/index.html\">Batch normalization</a> instead of <a href=\"../normalization/layer_norm/index.html\">Layer normalization</a>.</p>\n<p>Here&#x27;s <a href=\"experiment.html\">an experiment</a> that trains ConvMixer on CIFAR-10.</p>\n<p><a href=\"https://app.labml.ai/run/0fc344da2cd011ecb0bc3fdb2e774a3d\"><span translate=no>_^_2_^_</span></a></p>\n": "<h1>\u0db4\u0dd0\u0da0\u0dca\u0d94\u0db6\u0da7 \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0dc3\u0dd2\u0dba\u0dbd\u0dd4\u0db8 \u0daf? (\u0d9a\u0ddc\u0db1\u0dca\u0dc0\u0dd3 \u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca)</h1>\n<p>\u0db8\u0dd9\u0dba <a href=\"https://pytorch.org\">PyTorch</a> \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dba\u0dd2 <a href=\"https://arxiv.org/abs/2201.09792\">\u0db4\u0dd0\u0da0\u0dca \u0d94\u0db6\u0da7 \u0d85\u0dc0\u0dc1\u0dca\u0dba\u0daf? </a>. </p>\n<p><span translate=no>_^_0_^_</span></p>\n<p>ConvMixer <a href=\"../transformers/mlp_mixer/index.html\">MLP-\u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca</a>\u0dc3\u0db8\u0dcf\u0db1 \u0dc0\u0dda. \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3-\u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0dc4\u0dcf \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0db8\u0dcf\u0db1\u0dba\u0db1\u0dca \u0db8\u0dd2\u0dc1\u0dca\u0dbb \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc0\u0dd9\u0db1\u0dca \u0d9a\u0dbb\u0dba\u0dd2, \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0db8\u0dcf\u0db1\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3 \u0dba\u0dd9\u0daf\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0dc3\u0dc4 \u0db4\u0dc3\u0dd4\u0dc0 \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0db8\u0dcf\u0db1\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3 \u0dba\u0dd9\u0daf\u0dd3\u0db8\u0dd9\u0db1\u0dca (\u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3 <a href=\"../transformers/vit/index.html\">\u0dc0\u0dd3\u0d85\u0dba\u0dd2\u0da7\u0dd3</a> \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0dc3\u0dca\u0dae\u0dcf\u0db4\u0db1\u0dba \u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dbb \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf\u0dc0 \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3 \u0dba\u0db1\u0dd4 <a href=\"../transformers/feed_forward.html\">\u0d91\u0dc6\u0dca\u0d91\u0dc6\u0dca\u0d91\u0db1\u0dca</a> \u0dc0\u0dda vit). </p>\n<p>ConvMixer\u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf\u0dc0 \u0db8\u0dd2\u0dc1\u0dca\u0dbb <span translate=no>_^_1_^_</span> \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf convvolution \u0dc3\u0dc4 \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0db8\u0dd2\u0dc1\u0dca\u0dbb \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d9c\u0dd0\u0db9\u0dd4\u0dbb-wise convolution \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. \u0d91\u0dba \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0dc3\u0db8\u0dca\u0db4\u0dd6\u0dbb\u0dca\u0dab \u0d91\u0db8\u0dca\u0d91\u0dbd\u0dca\u0db4\u0dd3 \u0dc0\u0dd9\u0db1\u0dd4\u0dc0\u0da7 \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dca\u0dc0\u0dd3\u0db8\u0d9a\u0dca \u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca, \u0d91\u0dba VIT \u0dc4\u0ddd MLP-\u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca \u0dc0\u0dbd\u0da7 \u0dc0\u0da9\u0dcf \u0dc0\u0dd9\u0db1\u0dc3\u0dca\u0dc0 \u0d85\u0dc3\u0dbd \u0d87\u0dad\u0dd2 \u0d9a\u0dcf\u0dab\u0dca\u0da9 \u0db4\u0db8\u0dab\u0d9a\u0dca \u0db8\u0dd2\u0dc1\u0dca\u0dbb \u0d9a\u0dbb\u0dba\u0dd2. \u0d91\u0dc3\u0dda\u0db8, MLP-\u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db8\u0dd2\u0dc1\u0dca\u0dbb\u0dab\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0dca\u0dae\u0dbb \u0daf\u0dd9\u0d9a\u0d9a MLPs \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dbb ConvMixer \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db8\u0dd2\u0dc1\u0dca\u0dbb\u0dab\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0dad\u0db1\u0dd2 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<p>\u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2\u0db1\u0dd2\u0dbb\u0dca\u0daf\u0dda\u0dc1 \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0db8\u0dd2\u0dc1\u0dca\u0dbb\u0dab\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0d87\u0dad\u0dd2 \u0d85\u0dc0\u0dc1\u0dda\u0dc2 \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dad\u0dcf\u0dc0\u0dba \u0d89\u0dc0\u0dad\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 (\u0dbd\u0d9a\u0dca\u0dc2\u0dca\u0dba-\u0d85\u0db1\u0dd4\u0dc0 \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0db1\u0dca\u0dc0\u0dd2\u0db8) \u0dc3\u0dc4 \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0db8\u0dd2\u0dc1\u0dca\u0dbb\u0dab\u0dba\u0da7 \u0dc0\u0da9\u0dcf \u0d85\u0dc0\u0dc1\u0dda\u0dc2 \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dad\u0dcf\u0dc0\u0dba\u0d9a\u0dca \u0db4\u0db8\u0dab\u0d9a\u0dca \u0dad\u0dd2\u0db6\u0dd3\u0db8 (\u0d9c\u0dd0\u0db9\u0dd4\u0dbb-wise convolution) \u0dba. <a href=\"../normalization/layer_norm/index.html\">\u0dc3\u0dca\u0dae\u0dbb</a> <a href=\"../normalization/batch_norm/index.html\">\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0dc0\u0dd9\u0db1\u0dd4\u0dc0\u0da7 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba</a> \u0daf \u0d94\u0dc0\u0dd4\u0db1\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<p><a href=\"experiment.html\">CIFRA-10 \u0dc4\u0dd2 ConvMixer \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dca\u0dc4\u0daf\u0dcf \u0db6\u0dd0\u0dbd\u0dd3\u0db8\u0d9a\u0dca</a> \u0db8\u0dd9\u0db1\u0dca\u0db1. </p>\n<p><a href=\"https://app.labml.ai/run/0fc344da2cd011ecb0bc3fdb2e774a3d\"><span translate=no>_^_2_^_</span></a></p>\n",
 "<h2>ConvMixer</h2>\n<p>This combines the patch embeddings block, a number of ConvMixer layers and a classification head.</p>\n": "<h2>ConvMixer</h2>\n<p>\u0db8\u0dd9\u0dba\u0db4\u0dd0\u0da0\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0db6\u0dca\u0dbd\u0ddc\u0d9a\u0dca, \u0d9a\u0ddc\u0db1\u0dca\u0dc0\u0dd3\u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb \u0d9c\u0dab\u0db1\u0dcf\u0dc0\u0d9a\u0dca \u0dc3\u0dc4 \u0dc0\u0dbb\u0dca\u0d9c\u0dd3\u0d9a\u0dbb\u0dab \u0dc4\u0dd2\u0dc3\u0d9a\u0dca \u0d92\u0d9a\u0dcf\u0db6\u0daf\u0dca\u0db0 \u0d9a\u0dbb\u0dba\u0dd2. </p>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p> <a id=\"ClassificationHead\"></a></p>\n<h2>Classification Head</h2>\n<p>They do average pooling (taking the mean of all patch embeddings) and a final linear transformation to predict the log-probabilities of the image classes.</p>\n": "<p> <a id=\"ClassificationHead\"></a></p>\n<h2>\u0dc0\u0dbb\u0dca\u0d9c\u0dd3\u0d9a\u0dbb\u0dab\u0dc4\u0dd2\u0dc3</h2>\n<p>\u0dbb\u0dd6\u0db4\u0db4\u0d82\u0dad\u0dd2\u0dc0\u0dbd \u0dbd\u0ddc\u0d9c\u0dca-\u0dc3\u0db8\u0dca\u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf\u0dc0\u0db1\u0dca \u0db4\u0dd4\u0dbb\u0ddd\u0d9a\u0dae\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d94\u0dc0\u0dd4\u0db1\u0dca \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba \u0dad\u0da7\u0dcf\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 (\u0dc3\u0dd2\u0dba\u0dbd\u0dd4 \u0db4\u0dd0\u0da0\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc0\u0dbd \u0db8\u0db0\u0dca\u0dba\u0db1\u0dca\u0dba\u0dba \u0d9c\u0db1\u0dd2\u0db8\u0dd2\u0db1\u0dca) \u0dc3\u0dc4 \u0d85\u0dc0\u0dc3\u0dcf\u0db1 \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0d9a\u0dca \u0dc3\u0dd2\u0daf\u0dd4 \u0d9a\u0dbb\u0dba\u0dd2. </p>\n",
 "<p> <a id=\"ConvMixerLayer\"></a></p>\n<h2>ConvMixer layer</h2>\n<p>This is a single ConvMixer layer. The model will have a series of these.</p>\n": "<p> <a id=\"ConvMixerLayer\"></a></p>\n<h2>\u0d9a\u0ddc\u0db1\u0dca\u0d9a\u0dca\u0dbb\u0dd3\u0da7\u0dca\u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb\u0dba</h2>\n<p>\u0db8\u0dd9\u0dba\u0dad\u0db1\u0dd2 \u0d9a\u0ddc\u0db1\u0dca\u0dc0\u0dd3\u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0dd2. \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0da7 \u0db8\u0dda\u0dc0\u0dcf\u0dba\u0dd2\u0db1\u0dca \u0db8\u0dcf\u0dbd\u0dcf\u0dc0\u0d9a\u0dca \u0d87\u0dad. </p>\n",
 "<p> <a id=\"PatchEmbeddings\"></a></p>\n<h2>Get patch embeddings</h2>\n<p>This splits the image into patches of size <span translate=no>_^_0_^_</span> and gives an embedding for each patch.</p>\n": "<p> <a id=\"PatchEmbeddings\"></a></p>\n<h2>\u0db4\u0dd0\u0da0\u0dca\u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1</h2>\n<p>\u0db8\u0dd9\u0dba\u0dbb\u0dd6\u0db4\u0dba \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dd9\u0db1\u0dca \u0db4\u0dd0\u0da0\u0dca \u0dc0\u0dbd\u0da7 \u0db6\u0dd9\u0daf\u0dd9\u0db1 <span translate=no>_^_0_^_</span> \u0d85\u0dad\u0dbb \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db4\u0dd0\u0da0\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8 \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0dba\u0dd2. </p>\n",
 "<p>Activation after depth-wise convolution </p>\n": "<p>\u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4-\u0d85\u0db1\u0dd4\u0dc0\u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0db4\u0dc3\u0dd4 \u0dc3\u0d9a\u0dca\u0dbb\u0dd2\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 </p>\n",
 "<p>Activation after point-wise convolution </p>\n": "<p>\u0dbd\u0d9a\u0dca\u0dc2\u0dca\u0dba\u0dba\u0d85\u0db1\u0dd4\u0dc0 \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0db4\u0dc3\u0dd4 \u0dc3\u0d9a\u0dca\u0dbb\u0dd2\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 </p>\n",
 "<p>Activation and normalization </p>\n": "<p>\u0dc3\u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0dc4 \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba </p>\n",
 "<p>Activation function </p>\n": "<p>\u0dc3\u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0dba </p>\n",
 "<p>Add residual connection </p>\n": "<p>\u0d85\u0dc0\u0dc1\u0dda\u0dc2\u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dad\u0dcf\u0dc0\u0dba \u0d91\u0d9a\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Apply convolution layer </p>\n": "<p>\u0d9a\u0dd0\u0da7\u0dd2\u0d9c\u0dd0\u0dc3\u0dd4\u0dab\u0dd4 \u0dc3\u0dca\u0dae\u0dbb\u0dba \u0dba\u0ddc\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Average Pool </p>\n": "<p>\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0dad\u0da7\u0dcf\u0d9a\u0dba </p>\n",
 "<p>Average pooling </p>\n": "<p>\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0dad\u0da7\u0dcf\u0d9a </p>\n",
 "<p>Batch normalization </p>\n": "<p>\u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba </p>\n",
 "<p>Classification head </p>\n": "<p>\u0dc0\u0dbb\u0dca\u0d9c\u0dd3\u0d9a\u0dbb\u0dab\u0dc4\u0dd2\u0dc3 </p>\n",
 "<p>Classification head, to get logits </p>\n": "<p>\u0dc0\u0dbb\u0dca\u0d9c\u0dd3\u0d9a\u0dbb\u0dab\u0dc4\u0dd2\u0dc3, \u0db4\u0dd2\u0dc0\u0dd2\u0dc3\u0dd4\u0db8\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0dd0\u0db1\u0dd3\u0db8\u0da7 </p>\n",
 "<p>Depth-wise convolution is separate convolution for each channel. We do this with a convolution layer with the number of groups equal to the number of channels. So that each channel is it&#x27;s own group. </p>\n": "<p>\u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4-\u0db4\u0dca\u0dbb\u0da5\u0dcf\u0dc0\u0db1\u0dca\u0dad\u0dc3\u0d82\u0d9a\u0ddd\u0da0\u0db1\u0dba \u0dba\u0db1\u0dd4 \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf\u0dc0 \u0dc3\u0db3\u0dc4\u0dcf \u0dc0\u0dd9\u0db1\u0db8 \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dd4\u0db8\u0d9a\u0dd2. \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0dc3\u0d82\u0d9b\u0dca\u0dba\u0dcf\u0dc0\u0da7 \u0dc3\u0db8\u0dcf\u0db1 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dc3\u0d82\u0d9b\u0dca\u0dba\u0dcf\u0dc0\u0d9a\u0dca \u0dc3\u0db8\u0d9f \u0dc3\u0d82\u0d9a\u0ddd\u0da0\u0db1\u0dba \u0dc0\u0db1 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0dca \u0dc3\u0db8\u0d9f \u0d85\u0db4\u0dd2 \u0db8\u0dd9\u0dba \u0dc3\u0dd2\u0daf\u0dd4 \u0d9a\u0dbb\u0db8\u0dd4. \u0d91\u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca \u0dc3\u0dd1\u0db8 \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf\u0dc0\u0d9a\u0dca\u0db8 \u0dad\u0db8\u0db1\u0dca\u0d9c\u0dda\u0db8 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0d9a\u0dca \u0dc0\u0dda. </p>\n",
 "<p>Depth-wise convolution, activation and normalization </p>\n": "<p>\u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4-\u0db1\u0dd0\u0dab\u0dc0\u0dad\u0dca\u0d9a\u0dd0\u0da7\u0dd2 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8, \u0dc3\u0d9a\u0dca\u0dbb\u0dd2\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0dc4 \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 </p>\n",
 "<p>For the residual connection around the depth-wise convolution </p>\n": "<p>\u0d9c\u0dd0\u0db9\u0dd4\u0dbb-\u0db1\u0dd0\u0dab\u0dc0\u0dad\u0dca\u0d9a\u0dd0\u0da7\u0dd2 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc0\u0da7\u0dcf \u0d87\u0dad\u0dd2 \u0d85\u0dc0\u0dc1\u0dda\u0dc2 \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dad\u0dcf\u0dc0\u0dba \u0dc3\u0db3\u0dc4\u0dcf </p>\n",
 "<p>Get patch embeddings. This gives a tensor of shape <span translate=no>_^_0_^_</span>. </p>\n": "<p>\u0db4\u0dd0\u0da0\u0dca\u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1. \u0db8\u0dd9\u0dba \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d86\u0dad\u0dad\u0dd2\u0d9a\u0dba\u0d9a\u0dca \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0dba\u0dd2 <span translate=no>_^_0_^_</span>. </p>\n",
 "<p>Get the embedding, <span translate=no>_^_0_^_</span> will have shape <span translate=no>_^_1_^_</span> </p>\n": "<p>\u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1, \u0dc4\u0dd0\u0da9\u0dba <span translate=no>_^_0_^_</span> \u0d87\u0dad <span translate=no>_^_1_^_</span> </p>\n",
 "<p>Linear layer </p>\n": "<p>\u0dbb\u0dda\u0d9b\u0dd3\u0dba\u0dc3\u0dca\u0dae\u0dbb\u0dba </p>\n",
 "<p>Make copies of the <a href=\"#ConvMixerLayer\">ConvMixer layer</a> </p>\n": "<p><a href=\"#ConvMixerLayer\">ConvMixer \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda</a> \u0db4\u0dd2\u0da7\u0db4\u0dad\u0dca \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Normalization after depth-wise convolution </p>\n": "<p>\u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4-\u0d85\u0db1\u0dd4\u0dc0\u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0db4\u0dc3\u0dd4 \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba </p>\n",
 "<p>Normalization after point-wise convolution </p>\n": "<p>\u0dbd\u0d9a\u0dca\u0dc2\u0dca\u0dba\u0dba\u0d85\u0db1\u0dd4\u0dc0 \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0db4\u0dc3\u0dd4 \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba </p>\n",
 "<p>Pass through <a href=\"#ConvMixerLayer\">ConvMixer layers</a> </p>\n": "<p><a href=\"#ConvMixerLayer\">ConvMixer \u0dc3\u0dca\u0dae\u0dbb</a> \u0dc4\u0dbb\u0dc4\u0dcf \u0d9c\u0db8\u0db1\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Patch embeddings </p>\n": "<p>\u0db4\u0dd0\u0da0\u0dca\u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca </p>\n",
 "<p>Point-wise convolution is a <span translate=no>_^_0_^_</span> convolution. i.e. a linear transformation of patch embeddings </p>\n": "<p>\u0dbd\u0d9a\u0dca\u0dc2\u0dca\u0dba-\u0db1\u0dd0\u0dab\u0dc0\u0dad\u0dca\u0d9a\u0dd0\u0da7\u0dd2 <span translate=no>_^_0_^_</span> \u0d9c\u0dd0\u0dc3\u0dd3\u0db8\u0d9a\u0dd2. \u0d91\u0db1\u0db8\u0dca \u0db4\u0dd0\u0da0\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0d9a\u0dd2 </p>\n",
 "<p>Point-wise convolution, activation and normalization </p>\n": "<p>\u0dbd\u0d9a\u0dca\u0dc2\u0dca\u0dba-\u0db1\u0dd0\u0dab\u0dc0\u0dad\u0dca\u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0db1\u0dca\u0dc0\u0dd2\u0db8, \u0dc3\u0d9a\u0dca\u0dbb\u0dd2\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0dc4 \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba </p>\n",
 "<p>We create a convolution layer with a kernel size and and stride length equal to patch size. This is equivalent to splitting the image into patches and doing a linear transformation on each patch. </p>\n": "<p>\u0d85\u0db4\u0dd2\u0d9a\u0dbb\u0dca\u0db1\u0dbd\u0dca \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dd9\u0db1\u0dca \u0dc4\u0dcf \u0dbd\u0db4 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba \u0dc3\u0db8\u0dcf\u0db1 stride \u0daf\u0dd2\u0d9c \u0dc3\u0db8\u0d9c convolution \u0dc3\u0dca\u0dae\u0dbb\u0dba \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1. \u0db8\u0dd9\u0dba \u0dbb\u0dd6\u0db4\u0dba \u0db4\u0dd0\u0da0\u0dca \u0dc0\u0dbd\u0da7 \u0db6\u0dd9\u0daf\u0dd3\u0db8\u0da7 \u0dc4\u0dcf \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db4\u0dd0\u0da0\u0dca \u0d91\u0d9a\u0dda \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0d9a\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0dc3\u0db8\u0dcf\u0db1 \u0dc0\u0dda. </p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a copy of a single <a href=\"#ConvMixerLayer\">ConvMixer layer</a>.  We make copies of it to make ConvMixer with <span translate=no>_^_1_^_</span>. </li>\n<li><span translate=no>_^_2_^_</span> is the number of ConvMixer layers (or depth), <span translate=no>_^_3_^_</span>. </li>\n<li><span translate=no>_^_4_^_</span> is the <a href=\"#PatchEmbeddings\">patch embeddings layer</a>. </li>\n<li><span translate=no>_^_5_^_</span> is the <a href=\"#ClassificationHead\">classification head</a>.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0dad\u0db1\u0dd2 <a href=\"#ConvMixerLayer\">ConvMixer \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda</a>\u0db4\u0dd2\u0da7\u0db4\u0dad\u0d9a\u0dca \u0dc0\u0dda. ConvMixer \u0dc3\u0db8\u0d9f \u0d91\u0dba \u0dc3\u0dd1\u0daf\u0dd3\u0db8\u0da7 \u0d85\u0db4\u0dd2 \u0d91\u0dc4\u0dd2 \u0db4\u0dd2\u0da7\u0db4\u0dad\u0dca \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1\u0dd9\u0db8\u0dd4 <span translate=no>_^_1_^_</span>. </li>\n<li><span translate=no>_^_2_^_</span> ConVMixer \u0dc3\u0dca\u0dae\u0dbb (\u0dc4\u0ddd \u0d9c\u0dd0\u0db9\u0dd4\u0dbb) \u0dc3\u0d82\u0d9b\u0dca\u0dba\u0dcf\u0dc0 <span translate=no>_^_3_^_</span>\u0dc0\u0dda. </li>\n<li><span translate=no>_^_4_^_</span> <a href=\"#PatchEmbeddings\">\u0db4\u0dd0\u0da0\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc3\u0dca\u0dae\u0dbb\u0dba</a>\u0dc0\u0dda. </li>\n<li><span translate=no>_^_5_^_</span> <a href=\"#ClassificationHead\">\u0dc0\u0dbb\u0dca\u0d9c\u0dd3\u0d9a\u0dbb\u0dab \u0dc4\u0dd2\u0dc3</a>\u0dc0\u0dda. </li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input image of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0dba\u0db1\u0dd4 \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d86\u0daf\u0dcf\u0db1 \u0dbb\u0dd6\u0db4\u0dba\u0dba\u0dd2 <span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of channels in patch embeddings <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the size of the patch, <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> is the number of channels in the input image (3 for rgb)</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0db4\u0dd0\u0da0\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc0\u0dbd \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0d9c\u0dab\u0db1 <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> \u0db4\u0dd0\u0da0\u0dca \u0dc0\u0dbd \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba, <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> \u0d86\u0daf\u0dcf\u0db1 \u0dbb\u0dd6\u0db4\u0dba\u0dda \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0d9c\u0dab\u0db1 (rgb \u0dc3\u0db3\u0dc4\u0dcf 3)</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of channels in patch embeddings, <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the number of classes in the classification task</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0db4\u0dd0\u0da0\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc0\u0dbd \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0d9c\u0dab\u0db1, <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> \u0dc0\u0dbb\u0dca\u0d9c\u0dd3\u0d9a\u0dbb\u0dab \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0dba\u0dda \u0db4\u0db1\u0dca\u0dad\u0dd2 \u0d9c\u0dab\u0db1 \u0dc0\u0dda</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of channels in patch embeddings, <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the size of the kernel of spatial convolution, <span translate=no>_^_3_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0db4\u0dd0\u0da0\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc0\u0dbd \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0d9c\u0dab\u0db1, <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0dc3\u0d82\u0dc0\u0dc4\u0db1\u0dba\u0dda \u0d9a\u0dbb\u0dca\u0db1\u0dbd\u0dba\u0dda \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba, <span translate=no>_^_3_^_</span></li></ul>\n",
 "A PyTorch implementation/tutorial of the paper \"Patches Are All You Need?\"": "PyTorch \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8/\u0db1\u0dd2\u0db6\u0db1\u0dca\u0db0\u0db1\u0dba \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u201c\u0db4\u0dd0\u0da0\u0dca \u0d94\u0db6\u0da7 \u0d85\u0dc0\u0dc1\u0dca\u0dba\u0daf?\u201d",
 "Patches Are All You Need? (ConvMixer)": "\u0db4\u0dd0\u0da0\u0dca \u0d94\u0db6\u0da7 \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0dc3\u0dd2\u0dba\u0dbd\u0dd4\u0db8 \u0daf? (\u0d9a\u0ddc\u0db1\u0dca\u0dc0\u0dd3 \u0db8\u0dd2\u0d9a\u0dca\u0dc3\u0dbb\u0dca)"
}