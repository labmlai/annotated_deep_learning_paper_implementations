{
 "<h1>Capsule Networks</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation/tutorial of <a href=\"https://arxiv.org/abs/1710.09829\">Dynamic Routing Between Capsules</a>.</p>\n<p>Capsule network is a neural network architecture that embeds features as capsules and routes them with a voting mechanism to next layer of capsules.</p>\n<p>Unlike in other implementations of models, we&#x27;ve included a sample, because it is difficult to understand some concepts with just the modules. <a href=\"mnist.html\">This is the annotated code for a model that uses capsules to classify MNIST dataset</a></p>\n<p>This file holds the implementations of the core modules of Capsule Networks.</p>\n<p>I used <a href=\"https://github.com/jindongwang/Pytorch-CapsuleNet\">jindongwang/Pytorch-CapsuleNet</a> to clarify some confusions I had with the paper.</p>\n<p>Here&#x27;s a notebook for training a Capsule Network on MNIST dataset.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/capsule_networks/mnist.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n": "<h1>\u80f6\u56ca\u7f51\u7edc</h1>\n<p>\u8fd9\u662f<a href=\"https://arxiv.org/abs/1710.09829\">\u80f6\u56ca\u95f4\u52a8\u6001\u8def\u7531</a>\u7684 <a href=\"https://pytorch.org\">PyTorch</a> \u5b9e\u73b0/\u6559\u7a0b\u3002</p>\n<p>Capsule \u7f51\u7edc\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5b83\u4ee5\u80f6\u56ca\u7684\u5f62\u5f0f\u5d4c\u5165\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6295\u7968\u673a\u5236\u5c06\u5b83\u4eec\u8def\u7531\u5230\u4e0b\u4e00\u5c42\u80f6\u56ca\u3002</p>\n<p>\u4e0e\u5176\u4ed6\u6a21\u578b\u5b9e\u73b0\u4e0d\u540c\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u4f8b\uff0c\u56e0\u4e3a\u4ec5\u4f7f\u7528\u6a21\u5757\u5f88\u96be\u7406\u89e3\u67d0\u4e9b\u6982\u5ff5\u3002<a href=\"mnist.html\">\u8fd9\u662f\u4f7f\u7528\u80f6\u56ca\u5bf9 MNIST \u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\u7684\u6a21\u578b\u7684\u5e26\u6ce8\u91ca\u7684\u4ee3\u7801</a></p>\n<p>\u8be5\u6587\u4ef6\u5305\u542b\u4e86 Capsule Networks \u6838\u5fc3\u6a21\u5757\u7684\u5b9e\u73b0\u3002</p>\n<p>\u6211\u7528 <a href=\"https://github.com/jindongwang/Pytorch-CapsuleNet\">jindongwang/pytorch-CapsuleNet</a> \u6765\u6f84\u6e05\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u7684\u4e00\u4e9b\u56f0\u60d1\u3002</p>\n<p>\u8fd9\u662f\u4e00\u672c\u5728 MNIST \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3 Capsule \u7f51\u7edc\u7684\u7b14\u8bb0\u672c\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/capsule_networks/mnist.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n",
 "<h2>Margin loss for class existence</h2>\n<p>A separate margin loss is used for each output capsule and the total loss is the sum of them. The length of each output capsule is the probability that class is present in the input.</p>\n<p>Loss for each output capsule or class <span translate=no>_^_0_^_</span> is, <span translate=no>_^_1_^_</span></p>\n<p><span translate=no>_^_2_^_</span> is <span translate=no>_^_3_^_</span> if the class <span translate=no>_^_4_^_</span> is present and <span translate=no>_^_5_^_</span> otherwise. The first component of the loss is <span translate=no>_^_6_^_</span> when the class is not present, and the second component is <span translate=no>_^_7_^_</span> if the class is present. The <span translate=no>_^_8_^_</span> is used to avoid predictions going to extremes. <span translate=no>_^_9_^_</span> is set to be <span translate=no>_^_10_^_</span> and <span translate=no>_^_11_^_</span> to be <span translate=no>_^_12_^_</span> in the paper.</p>\n<p>The <span translate=no>_^_13_^_</span> down-weighting is used to stop the length of all capsules from falling during the initial phase of training.</p>\n": "<h2>\u9636\u7ea7\u5b58\u5728\u7684\u4fdd\u8bc1\u91d1\u635f\u5931</h2>\n<p>\u6bcf\u4e2a\u8f93\u51fa\u80f6\u56ca\u4f7f\u7528\u5355\u72ec\u7684\u4fdd\u8bc1\u91d1\u635f\u5931\uff0c\u603b\u4e8f\u635f\u662f\u5b83\u4eec\u7684\u603b\u548c\u3002\u6bcf\u4e2a\u8f93\u51fa\u80f6\u56ca\u7684\u957f\u5ea6\u662f\u8f93\u5165\u4e2d\u5b58\u5728\u7c7b\u7684\u6982\u7387\u3002</p>\n<p>\u6bcf\u4e2a\u8f93\u51fa\u80f6\u56ca\u6216\u7c7b\u7684\u635f\u5931<span translate=no>_^_0_^_</span>\u4e3a\uff0c<span translate=no>_^_1_^_</span></p>\n<p><span translate=no>_^_2_^_</span><span translate=no>_^_4_^_</span>\u662f\u7c7b<span translate=no>_^_3_^_</span>\u662f\u5426\u5b58\u5728\uff0c<span translate=no>_^_5_^_</span>\u5426\u5219\u3002\u635f\u5931\u7684\u7b2c\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\u662f<span translate=no>_^_6_^_</span>\u5f53\u7c7b\u4e0d\u5b58\u5728\u65f6\uff0c\u7b2c\u4e8c\u4e2a\u7ec4\u6210\u90e8\u5206\u662f\u7c7b<span translate=no>_^_7_^_</span>\u662f\u5426\u5b58\u5728\u3002<span translate=no>_^_8_^_</span>\u7528\u4e8e\u907f\u514d\u9884\u6d4b\u8d70\u5411\u6781\u7aef\u3002<span translate=no>_^_9_^_</span>\u88ab\u8bbe\u7f6e<span translate=no>_^_11_^_</span>\u4e3a<span translate=no>_^_10_^_</span>\u548c\u5c06\u5728<span translate=no>_^_12_^_</span>\u62a5\u7eb8\u4e0a\u3002</p>\n<p>\u5728\u8bad\u7ec3<span translate=no>_^_13_^_</span>\u7684\u521d\u59cb\u9636\u6bb5\uff0c\u51cf\u91cd\u7528\u4e8e\u9632\u6b62\u6240\u6709\u80f6\u56ca\u7684\u957f\u5ea6\u6389\u843d\u3002</p>\n",
 "<h2>Routing Algorithm</h2>\n<p>This is the routing mechanism described in the paper. You can use multiple routing layers in your models.</p>\n<p>This combines calculating <span translate=no>_^_0_^_</span> for this layer and the routing algorithm described in <em>Procedure 1</em>.</p>\n": "<h2>\u8def\u7531\u7b97\u6cd5</h2>\n<p>\u8fd9\u662f\u767d\u76ae\u4e66\u4e2d\u63cf\u8ff0\u7684\u8def\u7531\u673a\u5236\u3002\u53ef\u4ee5\u5728\u6a21\u578b\u4e2d\u4f7f\u7528\u591a\u4e2a\u5e03\u7ebf\u5c42\u3002</p>\n<p>\u8fd9\u7ed3\u5408\u4e86\u6b64\u5c42<span translate=no>_^_0_^_</span>\u7684\u8ba1\u7b97\u548c<em>\u8fc7\u7a0b 1</em> \u4e2d\u63cf\u8ff0\u7684\u8def\u7531\u7b97\u6cd5\u3002</p>\n",
 "<h2>Squash</h2>\n<p>This is <strong>squashing</strong> function from paper, given by equation <span translate=no>_^_0_^_</span>.</p>\n<p><span translate=no>_^_1_^_</span></p>\n<p><span translate=no>_^_2_^_</span> normalizes the length of all the capsules, whilst <span translate=no>_^_3_^_</span> shrinks the capsules that have a length smaller than one .</p>\n": "<h2>\u58c1\u7403</h2>\n<p>\u8fd9\u662f\u6765\u81ea\u7eb8\u5f20\u7684<strong>\u6324\u538b</strong>\u51fd\u6570\uff0c\u7531\u65b9\u7a0b\u7ed9\u51fa<span translate=no>_^_0_^_</span>\u3002</p>\n<p><span translate=no>_^_1_^_</span></p>\n<p><span translate=no>_^_2_^_</span>\u6807\u51c6\u5316\u6240\u6709\u80f6\u56ca\u7684\u957f\u5ea6\uff0c\u540c\u65f6<span translate=no>_^_3_^_</span>\u7f29\u5c0f\u957f\u5ea6\u5c0f\u4e8e\u4e00\u4e2a\u7684\u80f6\u56ca\u3002</p>\n",
 "<p> <span translate=no>_^_0_^_</span> is the number of capsules, and <span translate=no>_^_1_^_</span> is the number of features per capsule from the layer below. <span translate=no>_^_2_^_</span> and <span translate=no>_^_3_^_</span> are the same for this layer.</p>\n<p><span translate=no>_^_4_^_</span> is the number of routing iterations, symbolized by <span translate=no>_^_5_^_</span> in the paper.</p>\n": "<p><span translate=no>_^_0_^_</span>\u662f\u80f6\u56ca\u7684\u6570\u91cf\uff0c<span translate=no>_^_1_^_</span>\u662f\u4e0b\u65b9\u56fe\u5c42\u4e2d\u6bcf\u4e2a\u80f6\u56ca\u7684\u7279\u5f81\u6570\u3002<span translate=no>_^_2_^_</span>\u5bf9\u4e8e\u8fd9\u4e2a\u5c42\u6765\u8bf4<span translate=no>_^_3_^_</span>\u662f\u76f8\u540c\u7684\u3002</p>\n<p><span translate=no>_^_4_^_</span>\u662f\u8def\u7531\u8fed\u4ee3\u6b21\u6570\uff0c\u5728\u8bba\u6587<span translate=no>_^_5_^_</span>\u4e2d\u7528\u7b26\u53f7\u8868\u793a\u3002</p>\n",
 "<p> <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> are the squashed output capsules. This has shape <span translate=no>_^_2_^_</span>; that is, there is a capsule for each label.</p>\n<p><span translate=no>_^_3_^_</span> are the labels, and has shape <span translate=no>_^_4_^_</span>.</p>\n": "<p><span translate=no>_^_0_^_</span>\uff0c<span translate=no>_^_1_^_</span>\u662f\u538b\u6241\u7684\u8f93\u51fa\u80f6\u56ca\u3002\u5b83\u6709\u5f62\u72b6<span translate=no>_^_2_^_</span>\uff1b\u4e5f\u5c31\u662f\u8bf4\uff0c\u6bcf\u4e2a\u6807\u7b7e\u90fd\u6709\u4e00\u4e2a\u80f6\u56ca\u3002</p>\n<p><span translate=no>_^_3_^_</span>\u662f\u6807\u7b7e\uff0c\u6709\u5f62\u72b6<span translate=no>_^_4_^_</span>\u3002</p>\n",
 "<p> The shape of <span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span>. These are the capsules from the lower layer.</p>\n": "<p>\u7684\u5f62\u72b6<span translate=no>_^_0_^_</span>\u662f<span translate=no>_^_1_^_</span>\u3002\u8fd9\u4e9b\u662f\u4e0b\u5c42\u7684\u80f6\u56ca\u3002</p>\n",
 "<p> The shape of <span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span></p>\n": "<p>\u7684\u5f62\u72b6<span translate=no>_^_0_^_</span>\u662f<span translate=no>_^_1_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> has shape <span translate=no>_^_2_^_</span>. We have parallelized the computation of <span translate=no>_^_3_^_</span> for for all <span translate=no>_^_4_^_</span>. </p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u6709\u5f62\u72b6<span translate=no>_^_2_^_</span>\u3002\u6211\u4eec\u5df2\u7ecf\u5e76\u884c\u5316\u4e86 for all<span translate=no>_^_3_^_</span> \u7684\u8ba1\u7b97<span translate=no>_^_4_^_</span>\u3002</p>\n",
 "<p><span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> is one-hot encoded labels of shape <span translate=no>_^_2_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u662f\u5f62\u72b6\u7684\u4e00\u70ed\u7f16\u7801\u6807\u7b7e<span translate=no>_^_2_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> Here <span translate=no>_^_1_^_</span> is used to index capsules in this layer, whilst <span translate=no>_^_2_^_</span> is used to index capsules in the layer below (previous). </p>\n": "<p><span translate=no>_^_0_^_</span>\u8fd9\u91cc<span translate=no>_^_1_^_</span>\u7528\u4e8e\u7d22\u5f15\u8be5\u5c42\u4e2d\u7684\u80f6\u56ca\uff0c\u800c<span translate=no>_^_2_^_</span>\u7528\u4e8e\u7d22\u5f15\u4e0b\u5c42\uff08\u4e0a\u4e00\u5c42\uff09\u4e2d\u7684\u80f6\u56ca\u3002</p>\n",
 "<p>Initial logits <span translate=no>_^_0_^_</span> are the log prior probabilities that capsule <span translate=no>_^_1_^_</span> should be coupled with <span translate=no>_^_2_^_</span>. We initialize these at zero </p>\n": "<p>\u521d\u59cb\u5bf9\u6570<span translate=no>_^_0_^_</span>\u662f\u80f6\u56ca<span translate=no>_^_1_^_</span>\u5e94\u4e0e\u4e4b\u76f8\u7ed3\u5408\u7684\u5bf9\u6570\u5148\u9a8c\u6982\u7387<span translate=no>_^_2_^_</span>\u3002\u6211\u4eec\u5c06\u5b83\u4eec\u521d\u59cb\u5316\u4e3a\u96f6</p>\n",
 "<p>Iterate </p>\n": "<p>\u8fed\u4ee3</p>\n",
 "<p>This is the weight matrix <span translate=no>_^_0_^_</span>. It maps each capsule in the lower layer to each capsule in this layer </p>\n": "<p>\u8fd9\u662f\u6743\u91cd\u77e9\u9635<span translate=no>_^_0_^_</span>\u3002\u5b83\u5c06\u4e0b\u5c42\u4e2d\u7684\u6bcf\u4e2a\u80f6\u56ca\u6620\u5c04\u5230\u8be5\u5c42\u4e2d\u7684\u6bcf\u4e2a\u80f6\u56ca\u4f53</p>\n",
 "<p>We add an epsilon when calculating <span translate=no>_^_0_^_</span> to make sure it doesn&#x27;t become zero. If this becomes zero it starts giving out <span translate=no>_^_1_^_</span> values and training fails. <span translate=no>_^_2_^_</span> </p>\n": "<p>\u6211\u4eec\u5728\u8ba1\u7b97\u65f6\u6dfb\u52a0\u4e00\u4e2a epsilon<span translate=no>_^_0_^_</span>\uff0c\u4ee5\u786e\u4fdd\u5b83\u4e0d\u4f1a\u53d8\u4e3a\u96f6\u3002\u5982\u679c\u8be5\u503c\u53d8\u4e3a\u96f6\uff0c\u5219\u5f00\u59cb\u7ed9\u51fa<span translate=no>_^_1_^_</span>\u503c\uff0c\u5e76\u4e14\u8bad\u7ec3\u5931\u8d25\u3002<span translate=no>_^_2_^_</span></p>\n",
 "<p>routing softmax <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8def\u7531\u8f6f\u6700\u5927<span translate=no>_^_0_^_</span></p>\n",
 "Capsule Networks": "\u80f6\u56ca\u7f51\u7edc",
 "PyTorch implementation and tutorial of Capsule Networks. Capsule network is a neural network architecture that embeds features as capsules and routes them with a voting mechanism to next layer of capsules.": "PyTorch \u5b9e\u73b0\u548c\u80f6\u56ca\u7f51\u7edc\u6559\u7a0b\u3002\u80f6\u56ca\u7f51\u7edc\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5b83\u4ee5\u80f6\u56ca\u7684\u5f62\u5f0f\u5d4c\u5165\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6295\u7968\u673a\u5236\u5c06\u5b83\u4eec\u8def\u7531\u5230\u4e0b\u4e00\u5c42\u80f6\u56ca\u3002"
}