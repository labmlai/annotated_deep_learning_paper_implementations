{
 "<h1>Optimizers</h1>\n<h2>Optimizer Implementations</h2>\n<ul><li><a href=\"adam.html\">Adam Optimizer</a> </li>\n<li><a href=\"amsgrad.html\">AMSGrad Optimizer</a> </li>\n<li><a href=\"adam_warmup.html\">Adam Optimizer with warmup</a> </li>\n<li><a href=\"noam.html\">Noam Optimizer</a> </li>\n<li><a href=\"radam.html\">Rectified Adam Optimizer</a> </li>\n<li><a href=\"ada_belief.html\">AdaBelief Optimizer</a> </li>\n<li><a href=\"sophia.html\">Sophia-G Optimizer</a></li></ul>\n<p>This <a href=\"mnist_experiment.html\">MNIST example</a> uses these optimizers.</p>\n<h2>Generic Adaptive Optimizer Base class and Weight Decay</h2>\n<p>This file defines a common base class for <em>Adam</em> and extensions of it. The base class helps use implement other optimizers with minimal code because of re-usability.</p>\n<p>We also define a special class for L2 weight decay, so that we don&#x27;t have to implement it inside each of the optimizers, and can easily extend to other weight decays like L1 without changing the optimizers.</p>\n<p>Here are some concepts on PyTorch optimizers:</p>\n<h3>Parameter groups</h3>\n<p>PyTorch optimizers group parameters into sets called groups. Each group can have its own hyper-parameters like learning rates.</p>\n<p>In most common cases there will be only one group. This is when you initialize your optimizer with,</p>\n<span translate=no>_^_0_^_</span><p>You can define multiple parameter groups when initializing the optimizer:</p>\n<span translate=no>_^_1_^_</span><p>Here we pass a list of groups. Each group is a dictionary with its parameters under the key &#x27;params&#x27;. You specify any hyper-parameters as well. If the hyper parameters are not defined they will default to the optimizer level defaults.</p>\n<p>You can access (and even change) these groups, and their hyper-parameters with <span translate=no>_^_2_^_</span>. Most learning rate schedule implementations I&#x27;ve come across do access this and change &#x27;lr&#x27;.</p>\n<h3>States</h3>\n<p>Optimizer maintains states (a dictionary) for each parameter (a tensor), in a dictionary <span translate=no>_^_3_^_</span>. This is where the optimizer maintains things like exponential averages.</p>\n": "<h1>Optimizers</h1>\n<h2>Optimizer Implementations</h2>\n<ul><li><a href=\"adam.html\">Adam Optimizer</a> </li>\n<li><a href=\"amsgrad.html\">AMSGrad Optimizer</a> </li>\n<li><a href=\"adam_warmup.html\">Adam Optimizer with warmup</a> </li>\n<li><a href=\"noam.html\">Noam Optimizer</a> </li>\n<li><a href=\"radam.html\">Rectified Adam Optimizer</a> </li>\n<li><a href=\"ada_belief.html\">AdaBelief Optimizer</a> </li>\n<li><a href=\"sophia.html\">Sophia-G Optimizer</a></li></ul>\n<p>This <a href=\"mnist_experiment.html\">MNIST example</a> uses these optimizers.</p>\n<h2>Generic Adaptive Optimizer Base class and Weight Decay</h2>\n<p>This file defines a common base class for <em>Adam</em> and extensions of it. The base class helps use implement other optimizers with minimal code because of re-usability.</p>\n<p>We also define a special class for L2 weight decay, so that we don&#x27;t have to implement it inside each of the optimizers, and can easily extend to other weight decays like L1 without changing the optimizers.</p>\n<p>Here are some concepts on PyTorch optimizers:</p>\n<h3>Parameter groups</h3>\n<p>PyTorch optimizers group parameters into sets called groups. Each group can have its own hyper-parameters like learning rates.</p>\n<p>In most common cases there will be only one group. This is when you initialize your optimizer with,</p>\n<span translate=no>_^_0_^_</span><p>You can define multiple parameter groups when initializing the optimizer:</p>\n<span translate=no>_^_1_^_</span><p>Here we pass a list of groups. Each group is a dictionary with its parameters under the key &#x27;params&#x27;. You specify any hyper-parameters as well. If the hyper parameters are not defined they will default to the optimizer level defaults.</p>\n<p>You can access (and even change) these groups, and their hyper-parameters with <span translate=no>_^_2_^_</span>. Most learning rate schedule implementations I&#x27;ve come across do access this and change &#x27;lr&#x27;.</p>\n<h3>States</h3>\n<p>Optimizer maintains states (a dictionary) for each parameter (a tensor), in a dictionary <span translate=no>_^_3_^_</span>. This is where the optimizer maintains things like exponential averages.</p>\n",
 "<h2>Base class for <em>Adam</em> and extensions</h2>\n": "<h2><em>Adam</em> \u548c\u6269\u5c55\u7684\u57fa\u7c7b</h2>\n",
 "<h2>L2 Weight decay</h2>\n": "<h2>L2 \u91cd\u91cf\u8870\u51cf</h2>\n",
 "<h3>Initialize state for a given parameter tensor</h3>\n<p>This should be overridden with code to initialize <span translate=no>_^_0_^_</span> for parameters <span translate=no>_^_1_^_</span>. <span translate=no>_^_2_^_</span> is the parameter group dictionary to which <span translate=no>_^_3_^_</span> belongs.</p>\n": "<h3>\u521d\u59cb\u5316\u7ed9\u5b9a\u53c2\u6570\u5f20\u91cf\u7684\u72b6\u6001</h3>\n<p>\u8fd9\u5e94\u8be5\u88ab\u4ee3\u7801\u8986\u76d6\uff0c\u4ee5\u4fbf\u521d\u59cb<span translate=no>_^_0_^_</span>\u5316\u53c2\u6570<span translate=no>_^_1_^_</span>\u3002<span translate=no>_^_2_^_</span>\u662f\u6240<span translate=no>_^_3_^_</span>\u5c5e\u7684\u53c2\u6570\u7ec4\u5b57\u5178\u3002</p>\n",
 "<h3>Initialize weight decay</h3>\n<ul><li><span translate=no>_^_0_^_</span> is the decay coefficient </li>\n<li><span translate=no>_^_1_^_</span> is a flag indicating whether to add the weight decay to the gradient or directly decay from the parameter. If added to the gradient it will go through the normal optimizer update. </li>\n<li><span translate=no>_^_2_^_</span> this flag indicates whether the weight decay coefficient is absolute. This is applicable when the decay is performed directly on the parameter. If this is false the actual decay is <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span>.</li></ul>\n": "<h3>\u521d\u59cb\u5316\u6743\u91cd\u8870\u51cf</h3>\n<ul><li><span translate=no>_^_0_^_</span>\u662f\u8870\u51cf\u7cfb\u6570</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u4e00\u4e2a\u6807\u5fd7\uff0c\u6307\u793a\u662f\u5c06\u6743\u91cd\u8870\u51cf\u6dfb\u52a0\u5230\u68af\u5ea6\u8fd8\u662f\u76f4\u63a5\u4ece\u53c2\u6570\u4e2d\u8870\u51cf\u3002\u5982\u679c\u6dfb\u52a0\u5230\u6e10\u53d8\u4e2d\uff0c\u5b83\u5c06\u901a\u8fc7\u666e\u901a\u7684\u4f18\u5316\u5668\u66f4\u65b0\u3002</li>\n<li><span translate=no>_^_2_^_</span>\u6b64\u6807\u5fd7\u6307\u793a\u6743\u91cd\u8870\u51cf\u7cfb\u6570\u662f\u5426\u4e3a\u7edd\u5bf9\u503c\u3002\u5f53\u76f4\u63a5\u5bf9\u53c2\u6570\u6267\u884c\u8870\u51cf\u65f6\uff0c\u8fd9\u9002\u7528\u3002\u5982\u679c\u6b64\u503c\u4e3a\u5047\uff0c\u5219\u5b9e\u9645\u8870\u51cf\u4e3a<span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span>\u3002</li></ul>\n",
 "<h3>Initialize</h3>\n<ul><li><span translate=no>_^_0_^_</span> is the collection of parameters or set of parameter groups. </li>\n<li><span translate=no>_^_1_^_</span> a dictionary of default hyper-parameters </li>\n<li><span translate=no>_^_2_^_</span> is the learning rate, <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> is the tuple <span translate=no>_^_5_^_</span> </li>\n<li><span translate=no>_^_6_^_</span> is <span translate=no>_^_7_^_</span></li></ul>\n": "<h3>\u521d\u59cb\u5316</h3>\n<ul><li><span translate=no>_^_0_^_</span>\u662f\u53c2\u6570\u7684\u96c6\u5408\u6216\u4e00\u7ec4\u53c2\u6570\u7ec4\u3002</li>\n<li><span translate=no>_^_1_^_</span>\u9ed8\u8ba4\u8d85\u53c2\u6570\u7684\u5b57\u5178</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u5b66\u4e60\u7387\uff0c<span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span>\u662f\u5143\u7ec4<span translate=no>_^_5_^_</span></li>\n</ul><li><span translate=no>_^_6_^_</span>\u662f<span translate=no>_^_7_^_</span></li>\n",
 "<h3>Optimizer step</h3>\n<p>We have created a template method that does the common stuff every <em>Adam</em> based optimizer needs.</p>\n": "<h3>\u4f18\u5316\u5668\u6b65\u9aa4</h3>\n<p>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u6a21\u677f\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u5b8c\u6210\u6bcf\u4e2a\u57fa\u4e8e <em>Adam</em> \u7684\u4f18\u5316\u5668\u6240\u9700\u8981\u7684\u5e38\u7528\u5185\u5bb9\u3002</p>\n",
 "<h3>Perform weight decay and return the gradient</h3>\n": "<h3>\u6267\u884c\u6743\u91cd\u8870\u51cf\u5e76\u8fd4\u56de\u68af\u5ea6</h3>\n",
 "<h3>Take optimizer step on a parameter tensor</h3>\n<p>This should be overridden and take the optimization step on <span translate=no>_^_0_^_</span> tensor <span translate=no>_^_1_^_</span>, where <span translate=no>_^_2_^_</span> is the gradient for that parameter, <span translate=no>_^_3_^_</span>, <span translate=no>_^_4_^_</span> is the optimizer state dictionary for that parameter, and <span translate=no>_^_5_^_</span> is the parameter group dictionary <span translate=no>_^_6_^_</span> belongs to.</p>\n": "<h3>\u5728\u53c2\u6570\u5f20\u91cf\u4e0a\u91c7\u53d6\u4f18\u5316\u5668\u6b65\u9aa4</h3>\n<p>\u8fd9\u5e94\u8be5\u88ab\u91cd\u5199\u5e76\u5bf9<span translate=no>_^_0_^_</span>\u5f20\u91cf\u91c7\u53d6\u4f18\u5316\u6b65\u9aa4<span translate=no>_^_1_^_</span>\uff0c\u5176\u4e2d<span translate=no>_^_2_^_</span>\u662f\u8be5\u53c2\u6570\u7684\u68af\u5ea6<span translate=no>_^_3_^_</span>\uff0c<span translate=no>_^_4_^_</span>\u662f\u8be5\u53c2\u6570\u7684\u4f18\u5316\u5668\u72b6\u6001\u5b57\u5178\uff0c<span translate=no>_^_5_^_</span>\u4e5f\u662f\u53c2\u6570\u7ec4\u5b57\u5178<span translate=no>_^_6_^_</span>\u6240\u5c5e\u7684\u3002</p>\n",
 "<p> Return defaults for parameter groups</p>\n": "<p>\u8fd4\u56de\u53c2\u6570\u7ec4\u7684\u9ed8\u8ba4\u503c</p>\n",
 "<p>Add the hyper-parameters to the defaults </p>\n": "<p>\u5c06\u8d85\u53c2\u6570\u6dfb\u52a0\u5230\u9ed8\u8ba4\u503c</p>\n",
 "<p>Add the weight decay to the gradient and return the modified gradient </p>\n": "<p>\u5c06\u6743\u91cd\u8870\u51cf\u6dfb\u52a0\u5230\u6e10\u53d8\u5e76\u8fd4\u56de\u4fee\u6539\u540e\u7684\u6e10\u53d8</p>\n",
 "<p>Calculate loss.</p>\n<p>\ud83e\udd14 I&#x27;m not sure when you need this. I guess it&#x27;s if you define a function that calculates the loss, does <span translate=no>_^_0_^_</span> and return the loss, instead of calling it on your own you could pass it to <span translate=no>_^_1_^_</span>. \ud83e\udd37\u200d\u2642\ufe0f </p>\n": "<p>\u8ba1\u7b97\u635f\u5931\u3002</p>\n<p>\ud83e\udd14 \u6211\u4e0d\u786e\u5b9a\u4f60\u4ec0\u4e48\u65f6\u5019\u9700\u8981\u8fd9\u4e2a\u3002\u6211\u60f3\u5982\u679c\u4f60\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\u6765\u8ba1\u7b97\u635f\u5931\uff0c\u505a<span translate=no>_^_0_^_</span>\u548c\u8fd4\u56de\u635f\u5931\uff0c\u800c\u4e0d\u662f\u81ea\u5df1\u8c03\u7528\u5b83\uff0c\u4f60\u53ef\u4ee5\u4f20\u9012\u7ed9\u5b83<span translate=no>_^_1_^_</span>\u3002\ud83e\udd37\u200d\u2642\ufe0f</p>\n",
 "<p>Check hyper-parameters </p>\n": "<p>\u68c0\u67e5\u8d85\u53c2\u6570</p>\n",
 "<p>Check the hyper-parameters </p>\n": "<p>\u68c0\u67e5\u8d85\u53c2\u6570</p>\n",
 "<p>Get the gradient tensor </p>\n": "<p>\u83b7\u53d6\u68af\u5ea6\u5f20\u91cf</p>\n",
 "<p>Get the state for the parameter </p>\n": "<p>\u83b7\u53d6\u53c2\u6570\u7684\u72b6\u6001</p>\n",
 "<p>If the weight decay coefficient is absolute </p>\n": "<p>\u5982\u679c\u6743\u91cd\u8870\u51cf\u7cfb\u6570\u4e3a\u7edd\u5bf9\u503c</p>\n",
 "<p>If we are doing the decay on the parameter directly </p>\n": "<p>\u5982\u679c\u6211\u4eec\u76f4\u63a5\u5bf9\u53c2\u6570\u8fdb\u884c\u8870\u51cf</p>\n",
 "<p>Initialize the PyTorch optimizer. This will create parameter groups with the default hyper-parameters </p>\n": "<p>\u521d\u59cb\u5316 PyTorch \u4f18\u5316\u5668\u3002\u8fd9\u5c06\u4f7f\u7528\u9ed8\u8ba4\u7684\u8d85\u53c2\u6570\u521b\u5efa\u53c2\u6570\u7ec4</p>\n",
 "<p>Initialize the state if state is uninitialized </p>\n": "<p>\u5982\u679c\u72b6\u6001\u672a\u521d\u59cb\u5316\uff0c\u5219\u521d\u59cb\u5316\u72b6\u6001</p>\n",
 "<p>Iterate through the parameter groups </p>\n": "<p>\u904d\u5386\u53c2\u6570\u7ec4</p>\n",
 "<p>Iterate through the parameters in the parameter group </p>\n": "<p>\u904d\u5386\u53c2\u6570\u7ec4\u4e2d\u7684\u53c2\u6570</p>\n",
 "<p>Otherwise, </p>\n": "<p>\u5426\u5219\uff0c</p>\n",
 "<p>Return the loss, calculated from closure </p>\n": "<p>\u8fd4\u56de\u4ece\u95ed\u5305\u8ba1\u7b97\u5f97\u51fa\u7684\u635f\u5931</p>\n",
 "<p>Return the unmodified gradient </p>\n": "<p>\u8fd4\u56de\u672a\u4fee\u6539\u7684\u6e10\u53d8</p>\n",
 "<p>Skip if the parameter has no gradient </p>\n": "<p>\u5982\u679c\u53c2\u6570\u6ca1\u6709\u6e10\u53d8\uff0c\u5219\u8df3\u8fc7</p>\n",
 "<p>Take the optimization step on the parameter </p>\n": "<p>\u5bf9\u53c2\u6570\u91c7\u53d6\u4f18\u5316\u6b65\u9aa4</p>\n",
 "<p>We don&#x27;t handle sparse gradients </p>\n": "<p>\u6211\u4eec\u4e0d\u5904\u7406\u7a00\u758f\u6e10\u53d8</p>\n",
 "A set of PyTorch implementations/tutorials of popular gradient descent based optimizers. Currently includes Adam, AMSGrad and RAdam optimizers.": "\u4e00\u7ec4\u6d41\u884c\u7684\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u4f18\u5316\u5668\u7684 PyTorch \u5b9e\u73b0/\u6559\u7a0b\u3002\u76ee\u524d\u5305\u62ec Adam\u3001AmsGrad \u548c RadAM \u4f18\u5316\u5668\u3002",
 "Optimizers": "\u4f18\u5316\u5668"
}