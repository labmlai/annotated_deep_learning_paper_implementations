{
 "<h1>Graph Attention Networks (GAT)</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://arxiv.org/abs/1710.10903\">Graph Attention Networks</a>.</p>\n<p>GATs work on graph data. A graph consists of nodes and edges connecting nodes. For example, in Cora dataset the nodes are research papers and the edges are citations that connect the papers.</p>\n<p>GAT uses masked self-attention, kind of similar to <a href=\"../../transformers/mha.html\">transformers</a>. GAT consists of graph attention layers stacked on top of each other. Each graph attention layer gets node embeddings as inputs and outputs transformed embeddings. The node embeddings pay attention to the embeddings of other nodes it&#x27;s connected to. The details of graph attention layers are included alongside the implementation.</p>\n<p>Here is <a href=\"experiment.html\">the training code</a> for training a two-layer GAT on Cora dataset.</p>\n": "<h1>\u56fe\u8868\u6ce8\u610f\u529b\u7f51\u7edc (GAT)</h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch</a> \u5bf9\u300a<a href=\"https://arxiv.org/abs/1710.10903\">\u56fe\u5f62\u6ce8\u610f\u529b\u7f51\u7edc</a>\u300b\u8bba\u6587\u7684\u5b9e\u73b0\u3002</p>\n<p>GAT \u5904\u7406\u56fe\u8868\u6570\u636e\u3002\u56fe\u7531\u8282\u70b9\u548c\u8fde\u63a5\u8282\u70b9\u7684\u8fb9\u7ec4\u6210\u3002\u4f8b\u5982\uff0c\u5728 Cora \u6570\u636e\u96c6\u4e2d\uff0c\u8282\u70b9\u662f\u7814\u7a76\u8bba\u6587\uff0c\u8fb9\u7f18\u662f\u8fde\u63a5\u8bba\u6587\u7684\u5f15\u6587\u3002</p>\n<p>GAT \u4f7f\u7528\u8499\u9762\u81ea\u6ce8\u610f\u529b\uff0c\u6709\u70b9\u7c7b\u4f3c\u4e8e<a href=\"../../transformers/mha.html\">\u53d8\u5f62\u91d1\u521a</a>\u3002GAT \u7531\u76f8\u4e92\u5806\u53e0\u7684\u56fe\u8868\u6ce8\u610f\u529b\u5c42\u7ec4\u6210\u3002\u6bcf\u4e2a\u56fe\u6ce8\u610f\u529b\u5c42\u90fd\u5c06\u8282\u70b9\u5d4c\u5165\u4f5c\u4e3a\u8f6c\u6362\u540e\u7684\u5d4c\u5165\u7684\u8f93\u5165\u548c\u8f93\u51fa\u83b7\u5f97\u8282\u70b9\u3002\u8282\u70b9\u5d4c\u5165\u4f1a\u6ce8\u610f\u5b83\u6240\u8fde\u63a5\u7684\u5176\u4ed6\u8282\u70b9\u7684\u5d4c\u5165\u3002\u56fe\u5f62\u6ce8\u610f\u529b\u5c42\u7684\u8be6\u7ec6\u4fe1\u606f\u4e0e\u5b9e\u73b0\u4e00\u8d77\u5305\u62ec\u5728\u5185\u3002</p>\n<p>\u4ee5\u4e0b\u662f<a href=\"experiment.html\">\u5728 Cora \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e24\u5c42 GAT \u7684\u8bad\u7ec3\u4ee3\u7801</a>\u3002</p>\n",
 "<h2>Graph attention layer</h2>\n<p>This is a single graph attention layer. A GAT is made up of multiple such layers.</p>\n<p>It takes <span translate=no>_^_0_^_</span>, where <span translate=no>_^_1_^_</span> as input and outputs <span translate=no>_^_2_^_</span>, where <span translate=no>_^_3_^_</span>.</p>\n": "<h2>\u56fe\u5f62\u5173\u6ce8\u5c42</h2>\n<p>\u8fd9\u662f\u4e00\u4e2a\u5355\u4e00\u7684\u56fe\u5f62\u5173\u6ce8\u5c42\u3002\u4e00\u4e2a GAT \u7531\u591a\u4e2a\u8fd9\u6837\u7684\u5c42\u7ec4\u6210\u3002</p>\n<p>\u5b83\u9700\u8981<span translate=no>_^_0_^_</span>\uff0c\u5176\u4e2d<span translate=no>_^_1_^_</span>\u4f5c\u4e3a\u8f93\u5165\u548c\u8f93\u51fa<span translate=no>_^_2_^_</span>\uff0c\u5728\u54ea\u91cc<span translate=no>_^_3_^_</span>\u3002</p>\n",
 "<h4>Calculate attention score</h4>\n<p>We calculate these for each head <span translate=no>_^_0_^_</span>. <em>We have omitted <span translate=no>_^_1_^_</span> for simplicity</em>.</p>\n<p><span translate=no>_^_2_^_</span></p>\n<p><span translate=no>_^_3_^_</span> is the attention score (importance) from node <span translate=no>_^_4_^_</span> to node <span translate=no>_^_5_^_</span>. We calculate this for each head.</p>\n<p><span translate=no>_^_6_^_</span> is the attention mechanism, that calculates the attention score. The paper concatenates <span translate=no>_^_7_^_</span>, <span translate=no>_^_8_^_</span> and does a linear transformation with a weight vector <span translate=no>_^_9_^_</span> followed by a <span translate=no>_^_10_^_</span>.</p>\n<p><span translate=no>_^_11_^_</span> </p>\n": "<h4>\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570</h4>\n<p>\u6211\u4eec\u4e3a\u6bcf\u4e2a\u5934\u90e8\u8ba1\u7b97\u8fd9\u4e9b<span translate=no>_^_0_^_</span>\u3002<em><span translate=no>_^_1_^_</span>\u4e3a\u7b80\u5355\u8d77\u89c1\uff0c\u6211\u4eec\u7701\u7565\u4e86</em>\u3002</p>\n<p><span translate=no>_^_2_^_</span></p>\n<p><span translate=no>_^_3_^_</span>\u662f\u4ece\u4e00\u4e2a\u8282\u70b9\u5230\u53e6\u4e00\u4e2a\u8282\u70b9\u7684<span translate=no>_^_4_^_</span>\u6ce8\u610f\u529b\u5206\u6570\uff08\u91cd\u8981\u6027\uff09<span translate=no>_^_5_^_</span>\u3002\u6211\u4eec\u4e3a\u6bcf\u4e2a\u5934\u90e8\u8ba1\u7b97\u8fd9\u4e2a\u503c\u3002</p>\n<p><span translate=no>_^_6_^_</span>\u662f\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002\u672c\u6587\u8fde\u63a5\u8d77\u6765<span translate=no>_^_7_^_</span>\uff0c<span translate=no>_^_8_^_</span>\u7136\u540e\u4f7f\u7528\u6743\u91cd\u5411\u91cf<span translate=no>_^_9_^_</span>\u540e\u8ddf a \u8fdb\u884c\u7ebf\u6027\u53d8\u6362<span translate=no>_^_10_^_</span>\u3002</p>\n<p><span translate=no>_^_11_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> gets <span translate=no>_^_1_^_</span> where each node embedding is repeated <span translate=no>_^_2_^_</span> times. </p>\n": "<p><span translate=no>_^_0_^_</span>\u83b7\u53d6\u6bcf\u4e2a\u8282\u70b9\u5d4c\u5165\u91cd\u590d<span translate=no>_^_2_^_</span>\u6b21\u6570<span translate=no>_^_1_^_</span>\u7684\u4f4d\u7f6e\u3002</p>\n",
 "<p>Apply dropout regularization </p>\n": "<p>\u5e94\u7528\u8f8d\u5b66\u6b63\u5219\u5316</p>\n",
 "<p>Calculate <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> is of shape <span translate=no>_^_2_^_</span> </p>\n": "<p>\u8ba1\u7b97<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u662f\u5f62\u72b6\u7684<span translate=no>_^_2_^_</span></p>\n",
 "<p>Calculate final output for each head <span translate=no>_^_0_^_</span></p>\n<p><em>Note:</em> The paper includes the final activation <span translate=no>_^_1_^_</span> in <span translate=no>_^_2_^_</span> We have omitted this from the Graph Attention Layer implementation and use it on the GAT model to match with how other PyTorch modules are defined - activation as a separate layer. </p>\n": "<p>\u8ba1\u7b97\u6bcf\u4e2a\u5934\u7684\u6700\u7ec8\u8f93\u51fa<span translate=no>_^_0_^_</span></p>\n<p><em>\u6ce8\u610f\uff1a</em>\u672c\u6587\u5305\u542b\u4e86\u6700\u540e\u7684\u6fc0\u6d3b\u3002<span translate=no>_^_2_^_</span>\u6211\u4eec\u5728Graph Attention Layer\u5b9e\u73b0<span translate=no>_^_1_^_</span>\u4e2d\u7701\u7565\u4e86\u8fd9\u4e00\u70b9\uff0c\u5e76\u5c06\u5176\u7528\u4e8eGAT\u6a21\u578b\u4ee5\u5339\u914d\u5176\u4ed6 PyTorch \u6a21\u5757\u7684\u5b9a\u4e49\u65b9\u5f0f\u2014\u2014\u6fc0\u6d3b\u4f5c\u4e3a\u5355\u72ec\u7684\u56fe\u5c42\u3002</p>\n",
 "<p>Calculate the number of dimensions per head </p>\n": "<p>\u8ba1\u7b97\u6bcf\u5934\u7684\u5c3a\u5bf8\u6570</p>\n",
 "<p>Concatenate the heads </p>\n": "<p>\u8fde\u63a5\u5934\u90e8</p>\n",
 "<p>Dropout layer to be applied for attention </p>\n": "<p>\u8981\u5e94\u7528\u7684\u6389\u843d\u5c42\u4ee5\u5f15\u8d77\u6ce8\u610f</p>\n",
 "<p>First we calculate <span translate=no>_^_0_^_</span> for all pairs of <span translate=no>_^_1_^_</span>.</p>\n<p><span translate=no>_^_2_^_</span> gets <span translate=no>_^_3_^_</span> where each node embedding is repeated <span translate=no>_^_4_^_</span> times. </p>\n": "<p>\u9996\u5148\uff0c\u6211\u4eec\u8ba1\u7b97<span translate=no>_^_0_^_</span>\u6240\u6709\u5bf9<span translate=no>_^_1_^_</span>.</p>\n<p><span translate=no>_^_2_^_</span>\u83b7\u53d6\u6bcf\u4e2a\u8282\u70b9\u5d4c\u5165\u91cd\u590d<span translate=no>_^_4_^_</span>\u6b21\u6570<span translate=no>_^_3_^_</span>\u7684\u4f4d\u7f6e\u3002</p>\n",
 "<p>If we are averaging the multiple heads </p>\n": "<p>\u5982\u679c\u6211\u4eec\u5e73\u5747\u591a\u5934</p>\n",
 "<p>If we are concatenating the multiple heads </p>\n": "<p>\u5982\u679c\u6211\u4eec\u8981\u8fde\u63a5\u591a\u4e2a\u5934</p>\n",
 "<p>Linear layer for initial transformation; i.e. to transform the node embeddings before self-attention </p>\n": "<p>\u7528\u4e8e\u521d\u59cb\u53d8\u6362\u7684\u7ebf\u6027\u5c42\uff1b\u5373\u5728\u81ea\u6211\u5173\u6ce8\u4e4b\u524d\u8f6c\u6362\u8282\u70b9\u5d4c\u5165</p>\n",
 "<p>Linear layer to compute attention score <span translate=no>_^_0_^_</span> </p>\n": "<p>\u7528\u4e8e\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\u7684\u7ebf\u6027\u56fe\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>Mask <span translate=no>_^_0_^_</span> based on adjacency matrix. <span translate=no>_^_1_^_</span> is set to <span translate=no>_^_2_^_</span> if there is no edge from <span translate=no>_^_3_^_</span> to <span translate=no>_^_4_^_</span>. </p>\n": "<p><span translate=no>_^_0_^_</span>\u57fa\u4e8e\u90bb\u63a5\u77e9\u9635\u7684\u63a9\u7801\u3002<span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\u5982\u679c\u6ca1\u6709\u4ece\u5230\u7684\u8fb9\u7f18\uff0c\u5219\u8bbe\u7f6e<span translate=no>_^_3_^_</span>\u4e3a<span translate=no>_^_4_^_</span>\u3002</p>\n",
 "<p>Now we concatenate to get <span translate=no>_^_0_^_</span> </p>\n": "<p>\u73b0\u5728\u6211\u4eec\u8fde\u63a5\u6765\u83b7\u5f97<span translate=no>_^_0_^_</span></p>\n",
 "<p>Number of nodes </p>\n": "<p>\u8282\u70b9\u6570\u91cf</p>\n",
 "<p>Remove the last dimension of size <span translate=no>_^_0_^_</span> </p>\n": "<p>\u79fb\u9664\u5927\u5c0f\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6<span translate=no>_^_0_^_</span></p>\n",
 "<p>Reshape so that <span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> </p>\n": "<p>\u91cd\u5851<span translate=no>_^_0_^_</span>\u5c31\u662f\u8fd9\u6837<span translate=no>_^_1_^_</span></p>\n",
 "<p>Softmax to compute attention <span translate=no>_^_0_^_</span> </p>\n": "<p>Softmax \u9700\u8981\u8ba1\u7b97\u6ce8\u610f\u529b<span translate=no>_^_0_^_</span></p>\n",
 "<p>Take the mean of the heads </p>\n": "<p>\u4ee5\u5934\u8111\u7684\u610f\u601d\u4e3a\u4f8b</p>\n",
 "<p>The activation for attention score <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6fc0\u6d3b\u6ce8\u610f\u529b\u5206\u6570<span translate=no>_^_0_^_</span></p>\n",
 "<p>The adjacency matrix should have shape <span translate=no>_^_0_^_</span> or<span translate=no>_^_1_^_</span> </p>\n": "<p>\u90bb\u63a5\u77e9\u9635\u7684\u5f62\u72b6\u5e94<span translate=no>_^_0_^_</span>\u4e3a<span translate=no>_^_1_^_</span></p>\n",
 "<p>The initial transformation, <span translate=no>_^_0_^_</span> for each head. We do single linear transformation and then split it up for each head. </p>\n": "<p>\u6bcf\u4e2a\u5934\u90e8\u7684\u521d\u59cb\u53d8\u6362\u3002<span translate=no>_^_0_^_</span>\u6211\u4eec\u505a\u5355\u4e2a\u7ebf\u6027\u53d8\u6362\uff0c\u7136\u540e\u5c06\u5176\u62c6\u5206\u4e3a\u6bcf\u4e2a\u5934\u90e8\u3002</p>\n",
 "<p>We then normalize attention scores (or coefficients) <span translate=no>_^_0_^_</span></p>\n<p>where <span translate=no>_^_1_^_</span> is the set of nodes connected to <span translate=no>_^_2_^_</span>.</p>\n<p>We do this by setting unconnected <span translate=no>_^_3_^_</span> to <span translate=no>_^_4_^_</span> which makes <span translate=no>_^_5_^_</span> for unconnected pairs. </p>\n": "<p>\u7136\u540e\uff0c\u6211\u4eec\u5c06\u6ce8\u610f\u529b\u5206\u6570\uff08\u6216\u7cfb\u6570\uff09\u5f52\u4e00\u5316<span translate=no>_^_0_^_</span></p>\n<p>\u5176\u4e2d<span translate=no>_^_1_^_</span>\u662f\u8fde\u63a5\u5230\u7684\u8282\u70b9\u96c6<span translate=no>_^_2_^_</span>\u3002</p>\n<p>\u6211\u4eec\u901a\u8fc7<span translate=no>_^_3_^_</span>\u5c06\u672a\u8fde\u63a5\u7684\u914d\u5bf9\u8bbe\u7f6e<span translate=no>_^_5_^_</span>\u4e3a\u672a\u8fde\u63a5<span translate=no>_^_4_^_</span>\u7684\u914d\u5bf9\u6765\u5b9e\u73b0\u6b64\u76ee\u7684\u3002</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> is the input node embeddings of shape <span translate=no>_^_2_^_</span>. </li>\n<li><span translate=no>_^_3_^_</span> is the adjacency matrix of shape <span translate=no>_^_4_^_</span>. We use shape <span translate=no>_^_5_^_</span> since the adjacency is the same for each head.</li></ul>\n<p>Adjacency matrix represent the edges (or connections) among nodes. <span translate=no>_^_6_^_</span> is <span translate=no>_^_7_^_</span> if there is an edge from node <span translate=no>_^_8_^_</span> to node <span translate=no>_^_9_^_</span>.</p>\n": "<ul><li><span translate=no>_^_0_^_</span>\uff0c<span translate=no>_^_1_^_</span>\u662f shape \u7684\u8f93\u5165\u8282\u70b9\u5d4c\u5165<span translate=no>_^_2_^_</span>\u3002</li>\n<li><span translate=no>_^_3_^_</span>\u662f\u5f62\u72b6\u7684\u90bb\u63a5\u77e9\u9635<span translate=no>_^_4_^_</span>\u3002\u6211\u4eec\u4f7f\u7528\u5f62\u72b6\uff0c<span translate=no>_^_5_^_</span>\u56e0\u4e3a\u6bcf\u4e2a\u5934\u90e8\u7684\u90bb\u63a5\u662f\u76f8\u540c\u7684\u3002</li></ul>\n<p>\u90bb\u63a5\u77e9\u9635\u8868\u793a\u8282\u70b9\u4e4b\u95f4\u7684\u8fb9\uff08\u6216\u8fde\u63a5\uff09\u3002<span translate=no>_^_6_^_</span><span translate=no>_^_7_^_</span>\u5982\u679c\u8282\u70b9\u4e0e\u8282<span translate=no>_^_8_^_</span>\u70b9\u4e4b\u95f4\u5b58\u5728\u8fb9\u7f18<span translate=no>_^_9_^_</span>\u3002</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span>, is the number of input features per node </li>\n<li><span translate=no>_^_2_^_</span>, <span translate=no>_^_3_^_</span>, is the number of output features per node </li>\n<li><span translate=no>_^_4_^_</span>, <span translate=no>_^_5_^_</span>, is the number of attention heads </li>\n<li><span translate=no>_^_6_^_</span> whether the multi-head results should be concatenated or averaged </li>\n<li><span translate=no>_^_7_^_</span> is the dropout probability </li>\n<li><span translate=no>_^_8_^_</span> is the negative slope for leaky relu activation</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\uff0c\u662f\u6bcf\u4e2a\u8282\u70b9\u7684\u8f93\u5165\u8981\u7d20\u6570</li>\n<li><span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span>\uff0c\u662f\u6bcf\u4e2a\u8282\u70b9\u7684\u8f93\u51fa\u8981\u7d20\u6570</li>\n<li><span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\uff0c\u662f\u6ce8\u610f\u5934\u7684\u6570\u91cf</li>\n<li><span translate=no>_^_6_^_</span>\u591a\u5934\u7ed3\u679c\u5e94\u8be5\u662f\u4e32\u8054\u8fd8\u662f\u6c42\u5e73\u5747\u503c</li>\n<li><span translate=no>_^_7_^_</span>\u662f\u8f8d\u5b66\u6982\u7387</li>\n<li><span translate=no>_^_8_^_</span>\u662f\u6cc4\u6f0f\u7684 relu \u6fc0\u6d3b\u7684\u8d1f\u659c\u7387</li></ul>\n",
 "A PyTorch implementation/tutorial of Graph Attention Networks.": "Graph \u6ce8\u610f\u529b\u7f51\u7edc\u7684 PyTorch \u5b9e\u73b0/\u6559\u7a0b\u3002",
 "Graph Attention Networks (GAT)": "\u56fe\u5173\u6ce8\u7f51\u7edc (GAT)"
}