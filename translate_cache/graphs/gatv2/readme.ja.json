{
 "<h1><a href=\"https://nn.labml.ai/graphs/gatv2/index.html\">Graph Attention Networks v2 (GATv2)</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the GATv2 operator from the paper <a href=\"https://arxiv.org/abs/2105.14491\">How Attentive are Graph Attention Networks?</a>.</p>\n<p>GATv2s work on graph data. A graph consists of nodes and edges connecting nodes. For example, in Cora dataset the nodes are research papers and the edges are citations that connect the papers.</p>\n<p>The GATv2 operator fixes the static attention problem of the standard GAT: since the linear layers in the standard GAT are applied right after each other, the ranking of attended nodes is unconditioned on the query node. In contrast, in GATv2, every node can attend to any other node.</p>\n<p>Here is <a href=\"https://nn.labml.ai/graphs/gatv2/experiment.html\">the training code</a> for training a two-layer GATv2 on Cora dataset. </p>\n": "<h1><a href=\"https://nn.labml.ai/graphs/gatv2/index.html\">\u30b0\u30e9\u30d5\u30fb\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30b9 v2 (GATv2)</a></h1>\n<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://arxiv.org/abs/2105.14491\">\u30b0\u30e9\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u3069\u306e\u7a0b\u5ea6\u6ce8\u610f\u6df1\u3044\u306e\u304b</a>\uff1f\u300d<a href=\"https://pytorch.org\">\u3068\u3044\u3046\u8ad6\u6587\u306eGATv2\u6f14\u7b97\u5b50\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a>\u3002</p>\u3002\n<p>GATv2\u306f\u30b0\u30e9\u30d5\u30c7\u30fc\u30bf\u3092\u51e6\u7406\u3057\u307e\u3059\u3002\u30b0\u30e9\u30d5\u306f\u3001\u30ce\u30fc\u30c9\u3068\u30ce\u30fc\u30c9\u3092\u63a5\u7d9a\u3059\u308b\u30a8\u30c3\u30b8\u3067\u69cb\u6210\u3055\u308c\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001Cora\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u306f\u3001\u30ce\u30fc\u30c9\u306f\u7814\u7a76\u8ad6\u6587\u3067\u3001\u7aef\u306f\u8ad6\u6587\u3092\u3064\u306a\u3050\u5f15\u7528\u3067\u3059</p>\u3002\n<p>GATv2 \u6f14\u7b97\u5b50\u306f\u3001\u6a19\u6e96 GAT \u306e\u9759\u7684\u6ce8\u610f\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3057\u307e\u3059\u3002\u6a19\u6e96 GAT \u306e\u7dda\u5f62\u30ec\u30a4\u30e4\u30fc\u306f\u6b21\u3005\u306b\u9069\u7528\u3055\u308c\u308b\u305f\u3081\u3001\u53c2\u52a0\u30ce\u30fc\u30c9\u306e\u30e9\u30f3\u30af\u4ed8\u3051\u306f\u30af\u30a8\u30ea\u30ce\u30fc\u30c9\u3067\u6761\u4ef6\u4ed8\u3051\u3055\u308c\u307e\u305b\u3093\u3002\u5bfe\u7167\u7684\u306b\u3001GATv2\u3067\u306f\u3001\u3059\u3079\u3066\u306e\u30ce\u30fc\u30c9\u304c\u4ed6\u306e\u30ce\u30fc\u30c9\u306b\u63a5\u7d9a\u3067\u304d\u307e\u3059</p>\u3002\n<p>\u3053\u308c\u306f\u3001<a href=\"https://nn.labml.ai/graphs/gatv2/experiment.html\">Cora\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30672\u5c64GATv2\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u3067\u3059</a>\u3002</p>\n",
 "Graph Attention Networks v2 (GATv2)": "\u30b0\u30e9\u30d5\u30fb\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30b9 v2 (GATv2)"
}