{
 "<h1>Graph Attention Networks v2 (GATv2)</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the GATv2 operator from the paper <a href=\"https://arxiv.org/abs/2105.14491\">How Attentive are Graph Attention Networks?</a>.</p>\n<p>GATv2s work on graph data similar to <a href=\"../gat/index.html\">GAT</a>. A graph consists of nodes and edges connecting nodes. For example, in Cora dataset the nodes are research papers and the edges are citations that connect the papers.</p>\n<p>The GATv2 operator fixes the static attention problem of the standard <a href=\"../gat/index.html\">GAT</a>. Static attention is when the attention to the key nodes has the same rank (order) for any query node. <a href=\"../gat/index.html\">GAT</a> computes attention from query node <span translate=no>_^_0_^_</span> to key node <span translate=no>_^_1_^_</span> as,</p>\n<span translate=no>_^_2_^_</span><p>Note that for any query node <span translate=no>_^_3_^_</span>, the attention rank (<span translate=no>_^_4_^_</span>) of keys depends only on <span translate=no>_^_5_^_</span>. Therefore the attention rank of keys remains the same (<em>static</em>) for all queries.</p>\n<p>GATv2 allows dynamic attention by changing the attention mechanism,</p>\n<span translate=no>_^_6_^_</span><p>The paper shows that GATs static attention mechanism fails on some graph problems with a synthetic dictionary lookup dataset. It&#x27;s a fully connected bipartite graph where one set of nodes (query nodes) have a key associated with it and the other set of nodes have both a key and a value associated with it. The goal is to predict the values of query nodes. GAT fails on this task because of its limited static attention.</p>\n<p>Here is <a href=\"experiment.html\">the training code</a> for training a two-layer GATv2 on Cora dataset.</p>\n<p><a href=\"https://app.labml.ai/run/34b1e2f6ed6f11ebb860997901a2d1e3\"><span translate=no>_^_7_^_</span></a></p>\n": "<h1>\u0db4\u0dca\u0dbb\u0dc3\u0dca\u0dad\u0dcf\u0dbb\u0dba\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0da2\u0dcf\u0dbd v2 (GATV2)</h1>\n<p>\u0db8\u0dd9\u0dbaGATV2 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0d9a\u0dbb\u0dd4\u0d9c\u0dda <a href=\"https://pytorch.org\">PyTorch</a> \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0d9a\u0dd2 <a href=\"https://arxiv.org/abs/2105.14491\">\u0db4\u0dca\u0dbb\u0dc3\u0dca\u0dae\u0dcf\u0dbb \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dbb\u0db1 \u0da2\u0dcf\u0dbd\u0dba\u0db1\u0dca \u0d9a\u0dd9\u0dad\u0dbb\u0db8\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dd9\u0db1\u0dca \u0dc3\u0dd2\u0da7\u0dd2\u0db1\u0dc0\u0dcf\u0daf? </a>. </p>\n<p>GATV2s <a href=\"../gat/index.html\">GAT</a>\u0dc4\u0dcf \u0dc3\u0db8\u0dcf\u0db1 \u0db4\u0dca\u0dbb\u0dc3\u0dca\u0dad\u0dcf\u0dbb \u0daf\u0dad\u0dca\u0dad \u0db8\u0dad \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. \u0db4\u0dca\u0dbb\u0dc3\u0dca\u0dae\u0dcf\u0dbb\u0dba\u0d9a\u0dca \u0db1\u0ddd\u0da9\u0dca \u0dc3\u0dc4 \u0daf\u0dcf\u0dbb \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0 \u0d9a\u0dbb\u0db1 \u0db1\u0ddd\u0da9\u0dca \u0dc0\u0dbd\u0dd2\u0db1\u0dca \u0dc3\u0db8\u0db1\u0dca\u0dc0\u0dd2\u0dad \u0dc0\u0dda. \u0d8b\u0daf\u0dcf\u0dc4\u0dbb\u0dab\u0dba\u0d9a\u0dca \u0dbd\u0dd9\u0dc3, \u0d9a\u0ddd\u0dbb\u0dcf \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba\u0dda \u0db1\u0ddd\u0da9\u0dca \u0db4\u0dbb\u0dca\u0dba\u0dda\u0dc2\u0dab \u0db4\u0dad\u0dca\u0dbb\u0dd2\u0d9a\u0dcf \u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0daf\u0dcf\u0dbb \u0dba\u0db1\u0dd4 \u0db4\u0dad\u0dca\u0dbb\u0dd2\u0d9a\u0dcf \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0 \u0d9a\u0dbb\u0db1 \u0d8b\u0db4\u0dd4\u0da7\u0dcf \u0daf\u0dd0\u0d9a\u0dca\u0dc0\u0dd3\u0db8\u0dca \u0dc0\u0dda. </p>\n<p>GATV2\u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0d9a\u0dbb\u0dd4 \u0dc3\u0db8\u0dca\u0db8\u0dad <a href=\"../gat/index.html\">GAT \u0dc4\u0dd2</a>\u0dc3\u0dca\u0dae\u0dd2\u0dad\u0dd2\u0d9a \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d9c\u0dd0\u0da7\u0dc5\u0dd4\u0dc0 \u0dc0\u0dd2\u0dc3\u0db3\u0dba\u0dd2. \u0dc3\u0dca\u0dae\u0dd2\u0dad\u0dd2\u0d9a \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0db1\u0dd4 \u0dba\u0dad\u0dd4\u0dbb\u0dd4 \u0db1\u0ddd\u0da9\u0dca \u0dc0\u0dd9\u0dad \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0d95\u0db1\u0dd1\u0db8 \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca \u0db1\u0ddd\u0da9\u0dba\u0d9a\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0d91\u0d9a\u0db8 \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba\u0d9a\u0dca (\u0d87\u0dab\u0dc0\u0dd4\u0db8\u0d9a\u0dca) \u0d87\u0dad\u0dd2 \u0dc0\u0dd2\u0da7\u0dba. <a href=\"../gat/index.html\">\u0d9c\u0dd0\u0da7\u0dca</a> \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8 node \u0d91\u0d9a\u0d9a\u0dca \u0db8\u0dad\u0db8 \u0d8a\u0da7 \u0d85\u0daf\u0dcf\u0dbd \u0dc3\u0dd2\u0da7 \u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1 node <span translate=no>_^_0_^_</span> \u0d91\u0d9a\u0d9a\u0dca \u0db8\u0dad\u0db8 \u0d8a\u0da7 \u0d85\u0daf\u0dcf\u0dbd <span translate=no>_^_1_^_</span> \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0d9c\u0dab\u0db1\u0dba</p>\n<span translate=no>_^_2_^_</span><p>\u0d95\u0db1\u0dd1\u0db8\u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca \u0db1\u0ddd\u0da9\u0dba\u0d9a\u0dca \u0dc3\u0db3\u0dc4\u0dcf <span translate=no>_^_3_^_</span>, \u0dba\u0dad\u0dd4\u0dbb\u0dd4 \u0dc0\u0dbd \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba (<span translate=no>_^_4_^_</span>) \u0db8\u0dad \u0db4\u0db8\u0dab\u0d9a\u0dca \u0dbb\u0db3\u0dcf \u0db4\u0dc0\u0dad\u0dd2\u0db1 \u0db6\u0dc0 \u0dc3\u0dbd\u0d9a\u0db1\u0dca\u0db1 <span translate=no>_^_5_^_</span>. \u0d91\u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca \u0dc3\u0dd2\u0dba\u0dbd\u0dd4 \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0dba\u0dad\u0dd4\u0dbb\u0dd4 \u0dc0\u0dbd \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0d91\u0d9a\u0db8 (<em>\u0dc3\u0dca\u0dae\u0dd2\u0dad\u0dd2\u0d9a</em>) \u0db4\u0dc0\u0dad\u0dd3. </p>\n<p>GATV2\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dba\u0dcf\u0db1\u0dca\u0dad\u0dca\u0dbb\u0dab\u0dba \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0d9c\u0dad\u0dd2\u0d9a \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0da7 \u0d89\u0da9 \u0daf\u0dd9\u0dba\u0dd2,</p>\n<span translate=no>_^_6_^_</span><p>\u0d9a\u0dd8\u0dad\u0dd2\u0db8\u0dc1\u0db6\u0dca\u0daf \u0d9a\u0ddd\u0dc2 \u0db6\u0dd0\u0dbd\u0dd3\u0db8\u0dda \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba\u0d9a\u0dca \u0dc3\u0db8\u0d9f \u0dc3\u0db8\u0dc4\u0dbb \u0db4\u0dca\u0dbb\u0dc3\u0dca\u0dae\u0dcf\u0dbb \u0d9c\u0dd0\u0da7\u0dc5\u0dd4 \u0dc0\u0dbd\u0da7 GATs \u0dc3\u0dca\u0dae\u0dd2\u0dad\u0dd2\u0d9a \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dba\u0dcf\u0db1\u0dca\u0dad\u0dca\u0dbb\u0dab\u0dba \u0d85\u0dc3\u0db8\u0dad\u0dca \u0dc0\u0db1 \u0db6\u0dc0 \u0db4\u0dad\u0dca\u0dbb\u0dd2\u0d9a\u0dcf\u0dc0 \u0db4\u0dd9\u0db1\u0dca\u0dc0\u0dba\u0dd2. \u0d91\u0dba \u0dc3\u0db8\u0dca\u0db4\u0dd6\u0dbb\u0dca\u0dab \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dd2\u0dad \u0daf\u0dca\u0dc0\u0dd2\u0db4\u0dcf\u0dbb\u0dca\u0dc1\u0dc0\u0dd3\u0dba \u0db4\u0dca\u0dbb\u0dc3\u0dca\u0dae\u0dcf\u0dbb\u0dba\u0d9a\u0dca \u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0d91\u0dc4\u0dd2\u0daf\u0dd3 \u0d91\u0d9a\u0dca \u0db1\u0ddd\u0da9\u0dca \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba\u0d9a\u0dca (\u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca \u0db1\u0ddd\u0da9\u0dca) \u0d91\u0dba\u0da7 \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0 \u0dba\u0dad\u0dd4\u0dbb\u0d9a\u0dca \u0d87\u0dad\u0dd2 \u0d85\u0dad\u0dbb \u0d85\u0db1\u0dd9\u0d9a\u0dca \u0db1\u0ddd\u0da9\u0dca \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba\u0da7 \u0dba\u0dad\u0dd4\u0dbb\u0d9a\u0dca \u0dc3\u0dc4 \u0d92 \u0dc4\u0dcf \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0 \u0dc0\u0da7\u0dd2\u0db1\u0dcf\u0d9a\u0db8\u0d9a\u0dca \u0d87\u0dad. \u0d89\u0dbd\u0d9a\u0dca\u0d9a\u0dba \u0dc0\u0db1\u0dca\u0db1\u0dda \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca \u0db1\u0ddd\u0da9\u0dca \u0dc0\u0dbd \u0d85\u0d9c\u0dba\u0db1\u0dca \u0db4\u0dd4\u0dbb\u0ddd\u0d9a\u0dae\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dba\u0dd2. GAT \u0db1\u0dd2\u0dc3\u0dcf \u0d91\u0dc4\u0dd2 \u0dc3\u0dd3\u0db8\u0dd2\u0dad \u0dc3\u0dca\u0dae\u0dd2\u0dad\u0dd2\u0d9a \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db8\u0dd9\u0db8 \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0dc3\u0db8\u0dad\u0dca. </p>\n<p><a href=\"experiment.html\">\u0d9a\u0ddd\u0dbb\u0dcf \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba\u0dda \u0dc3\u0dca\u0dae\u0dbb \u0daf\u0dd9\u0d9a\u0d9a GATV2 \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dda\u0dad\u0dba</a> \u0db8\u0dd9\u0db1\u0dca\u0db1. </p>\n<p><a href=\"https://app.labml.ai/run/34b1e2f6ed6f11ebb860997901a2d1e3\"><span translate=no>_^_7_^_</span></a></p>\n",
 "<h2>Graph attention v2 layer</h2>\n<p>This is a single graph attention v2 layer. A GATv2 is made up of multiple such layers. It takes <span translate=no>_^_0_^_</span>, where <span translate=no>_^_1_^_</span> as input and outputs <span translate=no>_^_2_^_</span>, where <span translate=no>_^_3_^_</span>.</p>\n": "<h2>\u0db4\u0dca\u0dbb\u0dc3\u0dca\u0dad\u0dcf\u0dbb\u0dba\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba v2 \u0dc3\u0dca\u0dae\u0dbb\u0dba</h2>\n<p>\u0db8\u0dd9\u0dba\u0dad\u0db1\u0dd2 \u0db4\u0dca\u0dbb\u0dc3\u0dca\u0dae\u0dcf\u0dbb \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba v2 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0dd2. GATV2 \u0d91\u0dc0\u0dd0\u0db1\u0dd2 \u0dc3\u0dca\u0dae\u0dbb \u0d9a\u0dd2\u0dc4\u0dd2\u0db4\u0dba\u0d9a\u0dd2\u0db1\u0dca \u0dc3\u0dd1\u0daf\u0dd3 \u0d87\u0dad. \u0d86\u0daf\u0dcf\u0db1 \u0dc3\u0dc4 \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba\u0db1\u0dca <span translate=no>_^_1_^_</span> \u0dbd\u0dd9\u0dc3 \u0d9a\u0ddc\u0dad\u0dd0\u0db1\u0daf <span translate=no>_^_2_^_</span>, \u0d91\u0dba \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0dc0\u0dda <span translate=no>_^_0_^_</span><span translate=no>_^_3_^_</span>. </p>\n",
 "<h4>Calculate attention score</h4>\n<p>We calculate these for each head <span translate=no>_^_0_^_</span>. <em>We have omitted <span translate=no>_^_1_^_</span> for simplicity</em>.</p>\n<p><span translate=no>_^_2_^_</span></p>\n<p><span translate=no>_^_3_^_</span> is the attention score (importance) from node <span translate=no>_^_4_^_</span> to node <span translate=no>_^_5_^_</span>. We calculate this for each head.</p>\n<p><span translate=no>_^_6_^_</span> is the attention mechanism, that calculates the attention score. The paper sums <span translate=no>_^_7_^_</span>, <span translate=no>_^_8_^_</span> followed by a <span translate=no>_^_9_^_</span> and does a linear transformation with a weight vector <span translate=no>_^_10_^_</span></p>\n<p><span translate=no>_^_11_^_</span> Note: The paper desrcibes <span translate=no>_^_12_^_</span> as <span translate=no>_^_13_^_</span> which is equivalent to the definition we use here. </p>\n": "<h4>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dbd\u0d9a\u0dd4\u0dab\u0dd4 \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1</h4>\n<p>\u0d91\u0d9a\u0dca\u0d91\u0d9a\u0dca \u0dc4\u0dd2\u0dc3 \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0db4\u0dd2 \u0db8\u0dda\u0dc0\u0dcf \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db8\u0dd4 <span translate=no>_^_0_^_</span>. <em>\u0dc3\u0dbb\u0dbd\u0db6\u0dc0 <span translate=no>_^_1_^_</span> \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0db4\u0dd2 \u0db8\u0d9f \u0dc4\u0dbb\u0dc0\u0dcf \u0d87\u0dad\u0dca\u0dad\u0dd9\u0db8\u0dd4</em>. </p>\n<p><span translate=no>_^_2_^_</span></p>\n<p><span translate=no>_^_3_^_</span> \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dbd\u0d9a\u0dd4\u0dab\u0dd4 (\u0dc0\u0dd0\u0daf\u0d9c\u0dad\u0dca\u0d9a\u0db8) node \u0d91\u0d9a\u0d9a\u0dca \u0db8\u0dad\u0db8 \u0d8a\u0da7 \u0d85\u0daf\u0dcf\u0dbd \u0dc3\u0dd2\u0da7 node <span translate=no>_^_4_^_</span> \u0d91\u0d9a\u0d9a\u0dca <span translate=no>_^_5_^_</span>\u0db8\u0dad\u0db8 \u0d8a\u0da7 \u0d85\u0daf\u0dcf\u0dbd \u0d85\u0db4\u0dd2 \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0dc4\u0dd2\u0dc3 \u0dc3\u0db3\u0dc4\u0dcf \u0db8\u0dd9\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db8\u0dd4. </p>\n<p><span translate=no>_^_6_^_</span> \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dba\u0dcf\u0db1\u0dca\u0dad\u0dca\u0dbb\u0dab\u0dba, \u0d91\u0dba \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dbd\u0d9a\u0dd4\u0dab\u0dd4 \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0dba\u0dd2. \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dc3\u0dcf\u0dbb\u0dcf\u0d82\u0dc1 <span translate=no>_^_7_^_</span>, a \u0dc0\u0dd2\u0dc3\u0dd2\u0db1\u0dca <span translate=no>_^_8_^_</span> \u0d85\u0db1\u0dd4\u0d9c\u0db8\u0db1\u0dba \u0d9a\u0dbb\u0db1 <span translate=no>_^_9_^_</span> \u0d85\u0dad\u0dbb \u0db6\u0dbb \u0daf\u0ddb\u0dc1\u0dd2\u0d9a\u0dba\u0d9a\u0dca \u0dc3\u0db8\u0d9f \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0d9a\u0dca \u0dc3\u0dd2\u0daf\u0dd4 \u0d9a\u0dbb\u0dba\u0dd2 <span translate=no>_^_10_^_</span></p>\n<p><span translate=no>_^_11_^_</span> \u0dc3\u0da7\u0dc4\u0db1: \u0d85\u0db4 \u0db8\u0dd9\u0dc4\u0dd2 \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1 \u0d85\u0dbb\u0dca\u0dae \u0daf\u0dd0\u0d9a\u0dca\u0dc0\u0dd3\u0db8\u0da7 \u0dc3\u0db8\u0dcf\u0db1 <span translate=no>_^_13_^_</span> \u0dc0\u0db1 \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0d85\u0dc0\u0dbd\u0d82\u0d9c\u0dd4 \u0d9a\u0dbb\u0dba\u0dd2 <span translate=no>_^_12_^_</span> . </p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span> </p>\n",
 "<p><span translate=no>_^_0_^_</span> gets <span translate=no>_^_1_^_</span> where each node embedding is repeated <span translate=no>_^_2_^_</span> times. </p>\n": "<p><span translate=no>_^_0_^_</span> \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca node \u0d91\u0d9a\u0d9a\u0dca \u0db8\u0dad\u0db8 \u0d8a\u0da7 \u0d85\u0daf\u0dcf\u0dbd \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8 \u0db1\u0dd0\u0dc0\u0dad \u0db1\u0dd0\u0dc0\u0dad <span translate=no>_^_2_^_</span> \u0dc0\u0dad\u0dcf\u0dc0\u0d9a\u0dca <span translate=no>_^_1_^_</span> \u0d9a\u0ddc\u0dc4\u0dd9\u0daf \u0dbd\u0dd0\u0db6\u0dd9\u0db1. </p>\n",
 "<p>Apply dropout regularization </p>\n": "<p>\u0d85\u0dad\u0dc4\u0dd0\u0dbb\u0daf\u0dd0\u0db8\u0dd3\u0db8\u0dda \u0dc0\u0dd2\u0db0\u0dd2\u0db8\u0dad\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dba\u0ddc\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Calculate <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> is of shape <span translate=no>_^_2_^_</span> </p>\n": "<p>\u0d9c\u0dab\u0db1\u0dba\u0dc4\u0dd0\u0da9\u0dba\u0dd9\u0db1\u0dca <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> \u0dba\u0dd4\u0d9a\u0dca\u0dad \u0dc0\u0dda <span translate=no>_^_2_^_</span> </p>\n",
 "<p>Calculate final output for each head <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d91\u0d9a\u0dca\u0d91\u0d9a\u0dca \u0dc4\u0dd2\u0dc3 \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0dc0\u0dc3\u0dcf\u0db1 \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Calculate the number of dimensions per head </p>\n": "<p>\u0dc4\u0dd2\u0dc3\u0d9a\u0da7\u0db8\u0dcf\u0db1\u0dba\u0db1\u0dca \u0d9c\u0dab\u0db1 \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Concatenate the heads </p>\n": "<p>\u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1\u0dd3\u0db1\u0dca\u0dc3\u0d82\u0dba\u0dd4\u0d9a\u0dca\u0dad \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Dropout layer to be applied for attention </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dc3\u0db3\u0dc4\u0dcf \u0dba\u0dd9\u0daf\u0dd2\u0dba \u0dba\u0dd4\u0dad\u0dd4 \u0dc3\u0dca\u0dad\u0dbb\u0dba </p>\n",
 "<p>First we calculate <span translate=no>_^_0_^_</span> for all pairs of <span translate=no>_^_1_^_</span>.</p>\n<p><span translate=no>_^_2_^_</span> gets <span translate=no>_^_3_^_</span> where each node embedding is repeated <span translate=no>_^_4_^_</span> times. </p>\n": "<p>\u0db4\u0dc5\u0db8\u0dd4\u0dc0\u0d85\u0db4\u0dd2 \u0dc3\u0dd2\u0dba\u0dbd\u0dd4 \u0dba\u0dd4\u0d9c\u0dbd <span translate=no>_^_0_^_</span> \u0dc3\u0db3\u0dc4\u0dcf \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db8\u0dd4 <span translate=no>_^_1_^_</span>. </p>\n<p><span translate=no>_^_2_^_</span> \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca node \u0d91\u0d9a\u0d9a\u0dca \u0db8\u0dad\u0db8 \u0d8a\u0da7 \u0d85\u0daf\u0dcf\u0dbd \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8 \u0db1\u0dd0\u0dc0\u0dad \u0db1\u0dd0\u0dc0\u0dad <span translate=no>_^_4_^_</span> \u0dc0\u0dad\u0dcf\u0dc0\u0d9a\u0dca <span translate=no>_^_3_^_</span> \u0d9a\u0ddc\u0dc4\u0dd9\u0daf \u0dbd\u0dd0\u0db6\u0dd9\u0db1. </p>\n",
 "<p>If <span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> the same linear layer is used for the target nodes </p>\n": "<p><span translate=no>_^_0_^_</span> \u0db1\u0db8\u0dca <span translate=no>_^_1_^_</span> \u0d89\u0dbd\u0d9a\u0dca\u0d9a\u0d9c\u0dad \u0db1\u0ddd\u0da9\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0d91\u0d9a\u0db8 \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0dc0\u0dda </p>\n",
 "<p>If we are averaging the multiple heads </p>\n": "<p>\u0d85\u0db4\u0dd2\u0db6\u0dc4\u0dd4 \u0dc4\u0dd2\u0dc3\u0dca \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0db1\u0db8\u0dca </p>\n",
 "<p>If we are concatenating the multiple heads </p>\n": "<p>\u0d85\u0db4\u0dd2\u0db6\u0dc4\u0dd4 \u0dc4\u0dd2\u0dc3\u0dca \u0dc3\u0d82\u0d9a\u0ddd\u0da0\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0db1\u0db8\u0dca </p>\n",
 "<p>Linear layer for initial source transformation; i.e. to transform the source node embeddings before self-attention </p>\n": "<p>\u0d86\u0dbb\u0db8\u0dca\u0db7\u0d9a\u0db4\u0dca\u0dbb\u0db7\u0dc0 \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dba; \u0d91\u0db1\u0db8\u0dca \u0dc3\u0dca\u0dc0\u0dba\u0d82 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0da7 \u0db4\u0dd9\u0dbb \u0db4\u0dca\u0dbb\u0db7\u0dc0 \u0db1\u0ddd\u0da9\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 </p>\n",
 "<p>Linear layer to compute attention score <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dbd\u0d9a\u0dd4\u0dab\u0dd4 \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dba <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Mask <span translate=no>_^_0_^_</span> based on adjacency matrix. <span translate=no>_^_1_^_</span> is set to <span translate=no>_^_2_^_</span> if there is no edge from <span translate=no>_^_3_^_</span> to <span translate=no>_^_4_^_</span>. </p>\n": "<p>\u0db8\u0dd0\u0dc3\u0dca\u0dc3\u0dd2\u0d85\u0db1\u0dd4\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0db8\u0dad <span translate=no>_^_0_^_</span> \u0db4\u0daf\u0db1\u0db8\u0dca \u0dc0\u0dd6 \u0db8\u0dcf\u0dc3\u0dca\u0d9a\u0dca. <span translate=no>_^_1_^_</span> \u0dc3\u0dd2\u0da7 <span translate=no>_^_3_^_</span> \u0daf\u0dcf\u0dbb\u0dba\u0d9a\u0dca \u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2 <span translate=no>_^_2_^_</span> \u0db1\u0db8\u0dca \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0dc3\u0d9a\u0dc3\u0dcf <span translate=no>_^_4_^_</span>\u0d87\u0dad. </p>\n",
 "<p>Now we add the two tensors to get <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0daf\u0dd0\u0db1\u0dca\u0d85\u0db4\u0dd2 \u0dbd\u0db6\u0dcf \u0d9c\u0dd0\u0db1\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0da7\u0dd9\u0db1\u0dca\u0dc3\u0dbb \u0daf\u0dd9\u0d9a \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db8\u0dd4 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Number of nodes </p>\n": "<p>\u0db1\u0ddd\u0da9\u0dca\u0d9c\u0dab\u0db1 </p>\n",
 "<p>Remove the last dimension of size <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dda\u0d85\u0dc0\u0dc3\u0dcf\u0db1 \u0db8\u0dcf\u0db1\u0dba \u0d89\u0dc0\u0dad\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Reshape so that <span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> </p>\n": "<p>\u0d92\u0db1\u0dd2\u0dc3\u0dcf \u0db1\u0dd0\u0dc0\u0dad <span translate=no>_^_0_^_</span> \u0dc4\u0dd0\u0da9\u0d9c\u0dc3\u0dca\u0dc0\u0dcf <span translate=no>_^_1_^_</span> </p>\n",
 "<p>Softmax to compute attention <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0dc3\u0ddc\u0dc6\u0dca\u0da7\u0dca\u0db8\u0dd0\u0d9a\u0dca\u0dc3\u0dca <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Take the mean of the heads </p>\n": "<p>\u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1\u0dd3\u0db1\u0dca\u0d9c\u0dda\u0db8\u0db0\u0dca\u0dba\u0db1\u0dca\u0dba\u0dba \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>The activation for attention score <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dbd\u0d9a\u0dd4\u0dab\u0dd4 \u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0d9a\u0dca\u0dbb\u0dd2\u0dba <span translate=no>_^_0_^_</span> </p>\n",
 "<p>The adjacency matrix should have shape <span translate=no>_^_0_^_</span> or<span translate=no>_^_1_^_</span> </p>\n": "<p>\u0db8\u0dd9\u0db8adjacency \u0db1\u0dca\u0dba\u0dcf\u0dc3\u0dba \u0dc4\u0dd0\u0da9\u0dba <span translate=no>_^_0_^_</span> \u0dc4\u0ddd \u0dad\u0dd2\u0db6\u0dd2\u0dba \u0dba\u0dd4\u0dad\u0dd4\u0dba<span translate=no>_^_1_^_</span> </p>\n",
 "<p>The initial transformations, <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> for each head. We do two linear transformations and then split it up for each head. </p>\n": "<p>\u0d91\u0d9a\u0dca\u0d91\u0d9a\u0dca \u0dc4\u0dd2\u0dc3 <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> \u0dc3\u0db3\u0dc4\u0dcf \u0d86\u0dbb\u0db8\u0dca\u0db7\u0d9a \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0db1\u0dca. \u0d85\u0db4\u0dd2 \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0db1\u0dca \u0daf\u0dd9\u0d9a\u0d9a\u0dca \u0d9a\u0dbb \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0dc4\u0dd2\u0dc3 \u0dc3\u0db3\u0dc4\u0dcf \u0d91\u0dba \u0db6\u0dd9\u0daf\u0db1\u0dca\u0db1. </p>\n",
 "<p>We then normalize attention scores (or coefficients) <span translate=no>_^_0_^_</span></p>\n<p>where <span translate=no>_^_1_^_</span> is the set of nodes connected to <span translate=no>_^_2_^_</span>.</p>\n<p>We do this by setting unconnected <span translate=no>_^_3_^_</span> to <span translate=no>_^_4_^_</span> which makes <span translate=no>_^_5_^_</span> for unconnected pairs. </p>\n": "<p>\u0d89\u0db1\u0dca\u0db4\u0dc3\u0dd4\u0d85\u0db4\u0dd2 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dbd\u0d9a\u0dd4\u0dab\u0dd4 \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db8\u0dd4 (\u0dc4\u0ddd \u0dc3\u0d82\u0d9c\u0dd4\u0dab\u0d9a) <span translate=no>_^_0_^_</span></p>\n<p>\u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dc0\u0dd6 \u0db1\u0ddd\u0da9\u0dca \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba <span translate=no>_^_1_^_</span> <span translate=no>_^_2_^_</span>\u0d9a\u0ddc\u0dc4\u0dda\u0daf? </p>\n<p>\u0d85\u0db4\u0dd2\u0db8\u0dd9\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0d85\u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dd2\u0dad <span translate=no>_^_3_^_</span> \u0dba\u0dd4\u0d9c\u0dbd <span translate=no>_^_5_^_</span> \u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0 \u0db1\u0ddc\u0dc0\u0dd6 \u0dc3\u0dd0\u0d9a\u0dc3\u0dd3\u0db8\u0dd9\u0db1\u0dd2. <span translate=no>_^_4_^_</span> </p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> is the input node embeddings of shape <span translate=no>_^_2_^_</span>. </li>\n<li><span translate=no>_^_3_^_</span> is the adjacency matrix of shape <span translate=no>_^_4_^_</span>. We use shape <span translate=no>_^_5_^_</span> since the adjacency is the same for each head. Adjacency matrix represent the edges (or connections) among nodes. <span translate=no>_^_6_^_</span> is <span translate=no>_^_7_^_</span> if there is an edge from node <span translate=no>_^_8_^_</span> to node <span translate=no>_^_9_^_</span>.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d86\u0daf\u0dcf\u0db1 \u0db1\u0ddd\u0da9\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca <span translate=no>_^_2_^_</span>\u0dc0\u0dda. </li>\n<li><span translate=no>_^_3_^_</span> \u0dba\u0db1\u0dd4 \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0dc0\u0dd2\u0d9d\u0da7\u0db1 \u0d85\u0db1\u0dd4\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dd2 <span translate=no>_^_4_^_</span>. \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0dc4\u0dd2\u0dc3 \u0dc3\u0db3\u0dc4\u0dcf adjacency \u0d91\u0d9a \u0dc3\u0db8\u0dcf\u0db1 <span translate=no>_^_5_^_</span> \u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca \u0d85\u0db4\u0dd2 \u0dc4\u0dd0\u0da9\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd4. \u0dc3\u0db8\u0db4\u0dcf\u0dad \u0d85\u0db1\u0dd4\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0db1\u0ddd\u0da9\u0dca \u0d85\u0dad\u0dbb \u0daf\u0dcf\u0dbb (\u0dc4\u0ddd \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dad\u0dcf) \u0db1\u0dd2\u0dba\u0ddd\u0da2\u0db1\u0dba \u0d9a\u0dbb\u0dba\u0dd2. <span translate=no>_^_6_^_</span> \u0db1\u0ddd\u0da9\u0dca \u0dc3\u0dd2\u0da7 \u0db1\u0ddd\u0da9\u0dca \u0dc3\u0dd2\u0da7 \u0db1\u0ddd\u0da9\u0dca <span translate=no>_^_8_^_</span> \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0daf\u0dcf\u0dbb\u0dba\u0d9a\u0dca \u0dad\u0dd2\u0db6\u0dda <span translate=no>_^_7_^_</span> \u0db1\u0db8\u0dca <span translate=no>_^_9_^_</span>. </li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span>, is the number of input features per node </li>\n<li><span translate=no>_^_2_^_</span>, <span translate=no>_^_3_^_</span>, is the number of output features per node </li>\n<li><span translate=no>_^_4_^_</span>, <span translate=no>_^_5_^_</span>, is the number of attention heads </li>\n<li><span translate=no>_^_6_^_</span> whether the multi-head results should be concatenated or averaged </li>\n<li><span translate=no>_^_7_^_</span> is the dropout probability </li>\n<li><span translate=no>_^_8_^_</span> is the negative slope for leaky relu activation </li>\n<li><span translate=no>_^_9_^_</span> if set to <span translate=no>_^_10_^_</span>, the same matrix will be applied to the source and the target node of every edge</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span>, node \u0d91\u0d9a\u0d9a\u0dca \u0db8\u0dad\u0db8 \u0d8a\u0da7 \u0d85\u0daf\u0dcf\u0dbd \u0d86\u0daf\u0dcf\u0db1 \u0dbd\u0d9a\u0dca\u0dc2\u0dab \u0dc3\u0d82\u0d9b\u0dca\u0dba\u0dcf\u0dc0 </li>\n<li><span translate=no>_^_2_^_</span>, <span translate=no>_^_3_^_</span>, node \u0d91\u0d9a\u0d9a\u0dca \u0db8\u0dad\u0db8 \u0d8a\u0da7 \u0d85\u0daf\u0dcf\u0dbd \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0dc3\u0d82\u0d9b\u0dca\u0dba\u0dcf\u0dc0 \u0dc0\u0dda </li>\n<li><span translate=no>_^_4_^_</span>, <span translate=no>_^_5_^_</span>, \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1\u0dd3\u0db1\u0dca \u0dc3\u0d82\u0d9b\u0dca\u0dba\u0dcf\u0dc0 \u0dc0\u0dda </li>\n<li><span translate=no>_^_6_^_</span> \u0db6\u0dc4\u0dd4-\u0dc4\u0dd2\u0dc3 \u0db4\u0dca\u0dbb\u0dad\u0dd2 results \u0dbd \u0dc3\u0d82\u0dba\u0dd4\u0d9a\u0dca\u0dad \u0d9a\u0dc5 \u0dba\u0dd4\u0dad\u0dd4\u0daf \u0db1\u0dd0\u0dad\u0dc4\u0ddc\u0dad\u0dca \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0dba \u0dc0\u0dd2\u0dba \u0dba\u0dd4\u0dad\u0dd4\u0daf \u0dba\u0db1\u0dca\u0db1 </li>\n<li><span translate=no>_^_7_^_</span> \u0d85\u0dad\u0dc4\u0dd0\u0dbb \u0daf\u0dd0\u0db8\u0dd3\u0db8\u0dda \u0dc3\u0db8\u0dca\u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf\u0dc0 </li>\n<li><span translate=no>_^_8_^_</span> \u0dba\u0db1\u0dd4 \u0d9a\u0dcf\u0db1\u0dca\u0daf\u0dd4 \u0dc0\u0db1 \u0dbb\u0dd2\u0dbd\u0dd6 \u0dc3\u0d9a\u0dca\u0dbb\u0dd2\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf negative \u0dab \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dba\u0dd2 </li>\n<li><span translate=no>_^_9_^_</span> \u0dc3\u0d9a\u0dc3\u0dcf \u0d87\u0dad\u0dca\u0db1\u0db8\u0dca <span translate=no>_^_10_^_</span>, \u0dc3\u0dd1\u0db8 \u0daf\u0dcf\u0dbb\u0dba\u0d9a\u0db8 \u0db4\u0dca\u0dbb\u0db7\u0dc0\u0dba\u0da7 \u0dc3\u0dc4 \u0d89\u0dbd\u0d9a\u0dca\u0d9a\u0d9c\u0dad \u0db1\u0ddd\u0da9\u0dba\u0da7 \u0d91\u0d9a\u0db8 \u0d85\u0db1\u0dd4\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0dba\u0ddc\u0daf\u0db1\u0dd4 \u0dbd\u0dd0\u0db6\u0dda</li></ul>\n",
 "A PyTorch implementation/tutorial of Graph Attention Networks v2.": "\u0db4\u0dca\u0dbb\u0dc3\u0dca\u0dad\u0dcf\u0dbb \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0da2\u0dcf\u0dbd v2 \u0dc4\u0dd2 PyTorch \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8/\u0db1\u0dd2\u0db6\u0db1\u0dca\u0db0\u0db1\u0dba.",
 "Graph Attention Networks v2 (GATv2)": "\u0db4\u0dca\u0dbb\u0dc3\u0dca\u0dad\u0dcf\u0dbb\u0dba \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0da2\u0dcf\u0dbd v2 (GATV2)"
}