{
 "<h1>LLM.int() on GPT-NeoX</h1>\n<p>This implements a utility function to transform a <span translate=no>_^_0_^_</span> layer to LLM.int8() linear layer.</p>\n<p><a href=\"https://arxiv.org/abs/eb2bcaee1d0011edaa66a71c10a887e7\">LLM.int8() paper</a>  shows you can use int8 quantization while handling outliers to reduce memory footprint without performance degradation in large language models. They convert weights and inputs to scaled 8-bit integers and does matrix multiplication producing int32 results which is then converted back to float16 and rescaled. They show that in large langauge models, some features can give extreme values (outliers) that dominate the model&#x27;s output. These features get clamped in 8-bit integer space which causes the model performance to degrade. As a solution they pick these outliers (greater than a specified threshold) and compute their multiplications separately in float16 space. Since the percentage of outliers is around 0.01% this doesn&#x27;t increase memory usage, and prevents the model from degrading performance.</p>\n<p>The code to transform GPT-NoeX layers is defined in <a href=\"../model.html#post_load_prepare\">model.py</a>.</p>\n<p>Here are example uses of GPT-NeoX with int8 quantization.</p>\n<ul><li><a href=\"../samples/llm_int8.html\">Generate Text</a> </li>\n<li><a href=\"../evaluation/llm_int8.html\">Run Evaluation Tests</a></li></ul>\n": "<h1>LLM.int() \u0da2\u0dd3\u0db4\u0dd3\u0da7\u0dd3-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca \u0db8\u0dad</h1>\n<p>\u0dc3\u0dca\u0dad\u0dbb\u0dba\u0d9a\u0dcaLLM.INT8 () \u0dbb\u0dda\u0d9b\u0dd3\u0dba <span translate=no>_^_0_^_</span> \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0da7 \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db8\u0dd9\u0dba \u0d8b\u0db4\u0dba\u0ddd\u0d9c\u0dd3\u0dad\u0dcf \u0dc1\u0dca\u0dbb\u0dd2\u0dad\u0dba\u0d9a\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<p><a href=\"https://arxiv.org/abs/eb2bcaee1d0011edaa66a71c10a887e7\">LLM.INT8 () \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2</a> \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0db7\u0dcf\u0dc2\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dc0\u0dbd \u0d9a\u0dcf\u0dbb\u0dca\u0dba \u0dc3\u0dcf\u0db0\u0db1\u0dba \u0db4\u0dd2\u0dbb\u0dd2\u0dc4\u0dd3\u0db8\u0d9a\u0dd2\u0db1\u0dca \u0dad\u0ddc\u0dbb\u0dc0 \u0db8\u0dad\u0d9a \u0d85\u0da9\u0dd2\u0db4\u0dcf\u0dbb \u0d85\u0da9\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dd2\u0da7\u0dc3\u0dca\u0dad\u0dbb\u0dba\u0db1\u0dca \u0dc4\u0dd0\u0dc3\u0dd2\u0dbb\u0dc0\u0dd3\u0db8\u0dda\u0daf\u0dd3 \u0d94\u0db6\u0da7 int8 quantization \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2 \u0db6\u0dc0 \u0db4\u0dd9\u0db1\u0dca\u0dc0\u0dba\u0dd2. \u0d92\u0dc0\u0dcf \u0db6\u0dbb \u0dc3\u0dc4 \u0dba\u0dd9\u0daf\u0dc0\u0dd4\u0db8\u0dca \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dc5 8-\u0db6\u0dd2\u0da7\u0dca \u0db1\u0dd2\u0d9b\u0dd2\u0dbd \u0db6\u0dc0\u0da7 \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dbb \u0db1\u0dca\u0dba\u0dcf\u0dc3 \u0d9c\u0dd4\u0dab \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 INT32 \u0db4\u0dca\u0dbb\u0dad\u0dd2 results \u0dbd \u0db1\u0dd2\u0db4\u0daf\u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0d91\u0dba \u0db1\u0dd0\u0dc0\u0dad \u0db4\u0dcf\u0dc0\u0dd9\u0db1 16 \u0db6\u0dc0\u0da7 \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0d9a\u0dbb \u0db1\u0dd0\u0dc0\u0dad \u0dc3\u0d9a\u0dc3\u0dca \u0d9a\u0dbb\u0db1\u0dd4 \u0dbd\u0dd0\u0db6\u0dda. \u0d94\u0dc0\u0dd4\u0db1\u0dca \u0db4\u0dd9\u0db1\u0dca\u0dc0\u0dcf \u0daf\u0dd9\u0db1\u0dca\u0db1\u0dda \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0dbd\u0dd0\u0db1\u0dca\u0d9c\u0ddd\u0da2\u0dca \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dc0\u0dbd\u0daf\u0dd3, \u0dc3\u0db8\u0dc4\u0dbb \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0db8\u0d9f\u0dd2\u0db1\u0dca \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0dda \u0db1\u0dd2\u0db8\u0dd0\u0dc0\u0dd4\u0db8\u0dda \u0d86\u0db0\u0dd2\u0db4\u0dad\u0dca\u0dba\u0dba \u0daf\u0dbb\u0db1 \u0d86\u0db1\u0dca\u0dad\u0dd2\u0d9a \u0d85\u0d9c\u0dba\u0db1\u0dca (outliers) \u0dbd\u0db6\u0dcf \u0daf\u0dd2\u0dba \u0dc4\u0dd0\u0d9a\u0dd2 \u0db6\u0dc0\u0dba\u0dd2. \u0db8\u0dd9\u0db8 \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0d9a\u0dcf\u0dbb\u0dca\u0dba \u0dc3\u0dcf\u0db0\u0db1\u0dba \u0dc4\u0dcf\u0dba\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0dc4\u0dda\u0dad\u0dd4 \u0dc0\u0db1 8-bit \u0db4\u0dd6\u0dbb\u0dca\u0dab \u0dc3\u0d82\u0d9b\u0dca\u0dba\u0dcf\u0db8\u0dba \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dba \u0d9a\u0dbd\u0db8\u0dca\u0db4 \u0d9c\u0dc3\u0dcf \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1. \u0dc0\u0dd2\u0dc3\u0db3\u0dd4\u0db8\u0d9a\u0dca \u0dbd\u0dd9\u0dc3 \u0d94\u0dc0\u0dd4\u0db1\u0dca \u0db8\u0dd9\u0db8 \u0db4\u0dd2\u0da7\u0dc3\u0dca\u0dad\u0dbb\u0dba\u0db1\u0dca (\u0db1\u0dd2\u0dc1\u0dca\u0da0\u0dd2\u0dad \u0dc3\u0dd3\u0db8\u0dcf\u0dc0\u0d9a\u0da7 \u0dc0\u0da9\u0dcf \u0dc0\u0dd0\u0da9\u0dd2) \u0dad\u0ddd\u0dbb\u0dcf\u0d9c\u0dd9\u0db1 \u0d92\u0dc0\u0dcf\u0dba\u0dda \u0d9c\u0dd4\u0dab\u0db1\u0dba \u0dc0\u0dd9\u0db1 \u0dc0\u0dd9\u0db1\u0db8 \u0db4\u0dcf\u0dc0\u0dd9\u0db1 16 \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dba\u0dda \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0dba\u0dd2. \u0db4\u0dd2\u0da7\u0dc3\u0dca\u0dad\u0dbb\u0dba\u0dd2\u0db1\u0dca\u0d9c\u0dda \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0dc1\u0dad\u0dba 0.01% \u0d9a\u0dca \u0db4\u0db8\u0dab \u0dc0\u0db1 \u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca \u0db8\u0dd9\u0dba \u0db8\u0dad\u0d9a \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dba \u0dc0\u0dd0\u0da9\u0dd2 \u0db1\u0ddc\u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dbb \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0d9a\u0dcf\u0dbb\u0dca\u0dba \u0dc3\u0dcf\u0db0\u0db1\u0dba \u0db4\u0dd2\u0dbb\u0dd2\u0dc4\u0dd3\u0db8 \u0dc0\u0dc5\u0d9a\u0dca\u0dc0\u0dba\u0dd2. </p>\n<p>\u0da2\u0dd3\u0db4\u0dd3\u0da7\u0dd3-\u0db1\u0ddc\u0d9a\u0dca\u0dc3\u0dca\u0dc3\u0dca\u0dae\u0dbb \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d9a\u0dda\u0dad\u0dba <a href=\"../model.html#post_load_prepare\">model.py</a>\u0dc4\u0dd2 \u0d85\u0dbb\u0dca\u0dae \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0d87\u0dad. </p>\n<p>INT8\u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0d9a\u0dbb\u0dab\u0dba \u0dc3\u0dc4\u0dd2\u0dad \u0da2\u0dd3\u0db4\u0dd3\u0da7\u0dd3-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca \u0dc4\u0dd2 \u0d8b\u0daf\u0dcf\u0dc4\u0dbb\u0dab \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dba\u0db1\u0dca \u0db8\u0dd9\u0db1\u0dca\u0db1. </p>\n<ul><li><a href=\"../samples/llm_int8.html\">\u0db4\u0dd9\u0dc5 \u0da2\u0db1\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1</a> </li>\n<li><a href=\"../evaluation/llm_int8.html\">\u0d87\u0d9c\u0dba\u0dd3\u0db8\u0dca \u0db4\u0dbb\u0dd3\u0d9a\u0dca\u0dc2\u0dab \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb\u0db1\u0dca\u0db1</a></li></ul>\n",
 "<h2>Transform a <span translate=no>_^_0_^_</span> layer to LLM.int8() linear layer</h2>\n<ul><li><span translate=no>_^_1_^_</span>  is the <span translate=no>_^_2_^_</span> layer to transform </li>\n<li><span translate=no>_^_3_^_</span>  is the device of the model </li>\n<li><span translate=no>_^_4_^_</span>  is the threshold <span translate=no>_^_5_^_</span> to use for outlier detection</li></ul>\n": "<h2>LLM.INT8() \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0da7 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0dca \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> </h2>\n<ul><li><span translate=no>_^_1_^_</span> \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 <span translate=no>_^_2_^_</span> \u0dc3\u0dca\u0dad\u0dbb\u0dba \u0dc0\u0dda </li>\n<li><span translate=no>_^_3_^_</span> \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0dda \u0d8b\u0db4\u0dcf\u0d82\u0d9c\u0dba \u0dc0\u0dda </li>\n<li><span translate=no>_^_4_^_</span> \u0db4\u0dd2\u0da7\u0dad \u0dc4\u0db3\u0dd4\u0db1\u0dcf\u0d9c\u0dd0\u0db1\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf <span translate=no>_^_5_^_</span> \u0d9a\u0dc5 \u0dba\u0dd4\u0dad\u0dd4 \u0d91\u0dc5\u0dd2\u0db4\u0dad\u0dca\u0dad \u0dc0\u0dda</li></ul>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p>Create an empty Linear8bitLt module </p>\n": "<p>\u0dc4\u0dd2\u0dc3\u0dca\u0dbb\u0dda\u0d9b\u0dd3\u0dba 8BitLT \u0db8\u0ddc\u0da9\u0dd2\u0dba\u0dd4\u0dbd\u0dba\u0d9a\u0dca \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Import <a href=\"https://github.com/timdettmers/bitsandbytes\"><span translate=no>_^_0_^_</span></a> package </p>\n": "<p>\u0d86\u0db1\u0dba\u0db1 <a href=\"https://github.com/timdettmers/bitsandbytes\"><span translate=no>_^_0_^_</span></a> \u0db4\u0dd0\u0d9a\u0dda\u0da2\u0dba </p>\n",
 "<p>Quantize the weights </p>\n": "<p>\u0db6\u0dbb\u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Set the bias in float16 space </p>\n": "<p>\u0db4\u0dcf\u0dc0\u0dd9\u0db116 \u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dba\u0dda \u0db1\u0dd0\u0db9\u0dd4\u0dbb\u0dd4\u0dc0 \u0dc3\u0d9a\u0dc3\u0db1\u0dca\u0db1 </p>\n",
 "LLM.int8() on GPT-NeoX": "LLM.INT8 () \u0da2\u0dd3\u0db4\u0dd3\u0da7\u0dd3-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca \u0db8\u0dad",
 "Transform nn.Linear layers to 8-bit integer layers.": "NN.linear \u0dc3\u0dca\u0dae\u0dbb 8-bit \u0db4\u0dd6\u0dbb\u0dca\u0dab \u0dc3\u0d82\u0d9b\u0dca\u0dba\u0dcf\u0dc0\u0d9a\u0dca \u0dc3\u0dca\u0dae\u0dbb \u0db6\u0dc0\u0da7 \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1."
}