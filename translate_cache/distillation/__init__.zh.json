{
 "<h1>Distilling the Knowledge in a Neural Network</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation/tutorial of the paper <a href=\"https://arxiv.org/abs/1503.02531\">Distilling the Knowledge in a Neural Network</a>.</p>\n<p>It&#x27;s a way of training a small network using the knowledge in a trained larger network; i.e. distilling the knowledge from the large network.</p>\n<p>A large model with regularization or an ensemble of models (using dropout) generalizes better than a small model when trained directly on the data and labels. However, a small model can be trained to generalize better with help of a large model. Smaller models are better in production: faster, less compute, less memory.</p>\n<p>The output probabilities of a trained model give more information than the labels because it assigns non-zero probabilities to incorrect classes as well. These probabilities tell us that a sample has a chance of belonging to certain classes. For instance, when classifying digits, when given an image of digit <em>7</em>, a generalized model will give a high probability to 7 and a small but non-zero probability to 2, while assigning almost zero probability to other digits. Distillation uses this information to train a small model better.</p>\n<h2>Soft Targets</h2>\n<p>The probabilities are usually computed with a softmax operation,</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p>where <span translate=no>_^_1_^_</span> is the probability for class <span translate=no>_^_2_^_</span> and <span translate=no>_^_3_^_</span> is the logit.</p>\n<p>We train the small model to minimize the Cross entropy or KL Divergence between its output probability distribution and the large network&#x27;s output probability distribution (soft targets).</p>\n<p>One of the problems here is that the probabilities assigned to incorrect classes by the large network are often very small and don&#x27;t contribute to the loss. So they soften the probabilities by applying a temperature <span translate=no>_^_4_^_</span>,</p>\n<p><span translate=no>_^_5_^_</span></p>\n<p>where higher values for <span translate=no>_^_6_^_</span> will produce softer probabilities.</p>\n<h2>Training</h2>\n<p>Paper suggests adding a second loss term for predicting the actual labels when training the small model. We calculate the composite loss as the weighted sum of the two loss terms:  soft targets and actual labels.</p>\n<p>The dataset for distillation is called <em>the transfer set</em>, and the paper suggests using the same training data.</p>\n<h2>Our experiment</h2>\n<p>We train on CIFAR-10 dataset. We <a href=\"large.html\">train a large model</a> that has <span translate=no>_^_7_^_</span> parameters with dropout and it gives an accuracy of 85% on the validation set. A <a href=\"small.html\">small model</a> with <span translate=no>_^_8_^_</span> parameters gives an accuracy of 80%.</p>\n<p>We then train the small model with distillation from the large model, and it gives an accuracy of 82%; a 2% increase in the accuracy.</p>\n": "<h1>\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u63d0\u70bc\u77e5\u8bc6</h1>\n<p>\u8fd9\u662f\u8bba\u6587\u300a<a href=\"https://arxiv.org/abs/1503.02531\">\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u63d0\u70bc\u77e5\u8bc6\u300b\u7684 PyT</a> <a href=\"https://pytorch.org\">orch</a> \u5b9e\u73b0/\u6559\u7a0b\u3002</p>\n<p>\u8fd9\u662f\u4e00\u79cd\u4f7f\u7528\u7ecf\u8fc7\u8bad\u7ec3\u7684\u5927\u578b\u7f51\u7edc\u4e2d\u7684\u77e5\u8bc6\u6765\u8bad\u7ec3\u5c0f\u578b\u7f51\u7edc\u7684\u65b9\u6cd5\uff1b\u5373\u4ece\u5927\u578b\u7f51\u7edc\u4e2d\u63d0\u70bc\u77e5\u8bc6\u3002</p>\n<p>\u76f4\u63a5\u5728\u6570\u636e\u548c\u6807\u7b7e\u4e0a\u8bad\u7ec3\u65f6\uff0c\u5177\u6709\u6b63\u5219\u5316\u6216\u6a21\u578b\u96c6\u5408\uff08\u4f7f\u7528 dropout\uff09\u7684\u5927\u578b\u6a21\u578b\u6bd4\u5c0f\u578b\u6a21\u578b\u7684\u6982\u5316\u6548\u679c\u66f4\u597d\u3002\u4f46\u662f\uff0c\u5728\u5927\u578b\u6a21\u578b\u7684\u5e2e\u52a9\u4e0b\uff0c\u53ef\u4ee5\u8bad\u7ec3\u5c0f\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u8fdb\u884c\u6982\u62ec\u3002\u8f83\u5c0f\u7684\u6a21\u578b\u5728\u751f\u4ea7\u4e2d\u66f4\u597d\uff1a\u901f\u5ea6\u66f4\u5feb\u3001\u8ba1\u7b97\u66f4\u5c11\u3001\u5185\u5b58\u66f4\u5c11\u3002</p>\n<p>\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u8f93\u51fa\u6982\u7387\u6bd4\u6807\u7b7e\u63d0\u4f9b\u7684\u4fe1\u606f\u66f4\u591a\uff0c\u56e0\u4e3a\u5b83\u4e5f\u4f1a\u4e3a\u9519\u8bef\u7684\u7c7b\u5206\u914d\u975e\u96f6\u6982\u7387\u3002\u8fd9\u4e9b\u6982\u7387\u544a\u8bc9\u6211\u4eec\uff0c\u6837\u672c\u6709\u53ef\u80fd\u5c5e\u4e8e\u67d0\u4e9b\u7c7b\u522b\u3002\u4f8b\u5982\uff0c\u5728\u5bf9\u6570\u5b57\u8fdb\u884c\u5206\u7c7b\u65f6\uff0c\u5f53\u7ed9\u5b9a\u6570\u5b57 <em>7</em> \u7684\u56fe\u50cf\u65f6\uff0c\u5e7f\u4e49\u6a21\u578b\u4f1a\u7ed9\u51fa7\u7684\u9ad8\u6982\u7387\uff0c\u7ed92\u7684\u6982\u7387\u5f88\u5c0f\u4f46\u4e0d\u662f\u96f6\uff0c\u800c\u7ed9\u5176\u4ed6\u6570\u5b57\u5206\u914d\u51e0\u4e4e\u4e3a\u96f6\u7684\u6982\u7387\u3002\u84b8\u998f\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u6765\u66f4\u597d\u5730\u8bad\u7ec3\u5c0f\u578b\u6a21\u578b\u3002</p>\n<h2>\u8f6f\u76ee\u6807</h2>\n<p>\u6982\u7387\u901a\u5e38\u662f\u4f7f\u7528 softmax \u8fd0\u7b97\u8ba1\u7b97\u7684\uff0c</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p>\u5176\u4e2d<span translate=no>_^_1_^_</span>\uff0c\u662f\u7c7b\u7684\u6982\u7387<span translate=no>_^_2_^_</span>\uff0c<span translate=no>_^_3_^_</span>\u662f\u5bf9\u6570\u3002</p>\n<p>\u6211\u4eec\u8bad\u7ec3\u5c0f\u578b\u6a21\u578b\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5176\u8f93\u51fa\u6982\u7387\u5206\u5e03\u548c\u5927\u578b\u7f51\u7edc\u7684\u8f93\u51fa\u6982\u7387\u5206\u5e03\uff08\u8f6f\u76ee\u6807\uff09\u4e4b\u95f4\u7684\u4ea4\u53c9\u71b5\u6216 KL \u5dee\u5f02\u3002</p>\n<p>\u8fd9\u91cc\u7684\u95ee\u9898\u4e4b\u4e00\u662f\uff0c\u5927\u578b\u7f51\u7edc\u5206\u914d\u7ed9\u9519\u8bef\u7c7b\u522b\u7684\u6982\u7387\u901a\u5e38\u5f88\u5c0f\uff0c\u4e0d\u4f1a\u5bfc\u81f4\u635f\u5931\u3002\u6240\u4ee5\u4ed6\u4eec\u901a\u8fc7\u65bd\u52a0\u6e29\u5ea6\u6765\u8f6f\u5316\u6982\u7387<span translate=no>_^_4_^_</span>\uff0c</p>\n<p><span translate=no>_^_5_^_</span></p>\n<p>\u5176\u4e2d\uff0c\u7684<span translate=no>_^_6_^_</span>\u503c\u8d8a\u9ad8\uff0c\u4ea7\u751f\u7684\u6982\u7387\u8d8a\u4f4e\u3002</p>\n<h2>\u8bad\u7ec3</h2>\n<p>\u8bba\u6587\u5efa\u8bae\u5728\u8bad\u7ec3\u5c0f\u578b\u6a21\u578b\u65f6\u6dfb\u52a0\u7b2c\u4e8c\u4e2a\u635f\u5931\u9879\u6765\u9884\u6d4b\u5b9e\u9645\u6807\u7b7e\u3002\u6211\u4eec\u5c06\u7efc\u5408\u635f\u5931\u8ba1\u7b97\u4e3a\u4e24\u4e2a\u635f\u5931\u9879\u7684\u52a0\u6743\u603b\u548c\uff1a\u8f6f\u76ee\u6807\u548c\u5b9e\u9645\u6807\u7b7e\u3002</p>\n<p>\u7528\u4e8e\u84b8\u998f\u7684\u6570\u636e\u96c6\u79f0\u4e3a<em>\u4f20\u8f93\u96c6</em>\uff0c\u8be5\u8bba\u6587\u5efa\u8bae\u4f7f\u7528\u76f8\u540c\u7684\u8bad\u7ec3\u6570\u636e\u3002</p>\n<h2>\u6211\u4eec\u7684\u5b9e\u9a8c</h2>\n<p>\u6211\u4eec\u5728 CIFAR-10 \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002\u6211\u4eec<a href=\"large.html\">\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5927\u578b\u6a21\u578b\uff0c\u8be5\u6a21\u578b</a>\u7684<span translate=no>_^_7_^_</span>\u53c2\u6570\u5e26\u6709 dropout\uff0c\u5b83\u5728\u9a8c\u8bc1\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u4e3a 85%\u3002\u5e26\u6709<span translate=no>_^_8_^_</span>\u53c2\u6570<a href=\"small.html\">\u7684\u5c0f\u578b\u6a21\u578b</a>\u7684\u51c6\u786e\u5ea6\u4e3a80\uff05\u3002</p>\n<p>\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5927\u578b\u6a21\u578b\u7684\u84b8\u998f\u6cd5\u8bad\u7ec3\u5c0f\u578b\u6a21\u578b\uff0c\u5176\u7cbe\u5ea6\u4e3a82\uff05\uff1b\u7cbe\u5ea6\u63d0\u9ad8\u4e862\uff05\u3002</p>\n",
 "<h2>Configurations</h2>\n<p>This extends from <a href=\"../experiments/cifar10.html\"><span translate=no>_^_0_^_</span></a> which defines all the dataset related configurations, optimizer, and a training loop.</p>\n": "<h2>\u914d\u7f6e</h2>\n<p>\u4ece\u6b64\u6269\u5c55\u5b9a\u4e49<a href=\"../experiments/cifar10.html\"><span translate=no>_^_0_^_</span></a>\u4e86\u6240\u6709\u4e0e\u6570\u636e\u96c6\u76f8\u5173\u7684\u914d\u7f6e\u3001\u4f18\u5316\u5668\u548c\u8bad\u7ec3\u5faa\u73af\u3002</p>\n",
 "<h3>Create large model</h3>\n": "<h3>\u521b\u5efa\u5927\u578b\u6a21\u578b</h3>\n",
 "<h3>Create small model</h3>\n": "<h3>\u521b\u5efa\u5c0f\u6a21\u578b</h3>\n",
 "<h3>Load <a href=\"large.html\">trained large model</a></h3>\n": "<h3>\u8f7d\u8377<a href=\"large.html\">\u8bad\u7ec3\u7684\u5927\u578b\u6a21\u578b</a></h3>\n",
 "<h3>Training/validation step</h3>\n<p>We define a custom training/validation step to include the distillation</p>\n": "<h3>\u57f9\u8bad/\u9a8c\u8bc1\u6b65\u9aa4</h3>\n<p>\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5b9a\u5236\u7684\u8bad\u7ec3/\u9a8c\u8bc1\u6b65\u9aa4\uff0c\u5305\u62ec\u84b8\u998f</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> Train a small model with distillation</p>\n": "<p>\u4f7f\u7528\u84b8\u998f\u8bad\u7ec3\u5c0f\u578b\u6a21\u578b</p>\n",
 "<p>Calculate and log accuracy </p>\n": "<p>\u8ba1\u7b97\u548c\u8bb0\u5f55\u7cbe\u5ea6</p>\n",
 "<p>Calculate gradients </p>\n": "<p>\u8ba1\u7b97\u68af\u5ea6</p>\n",
 "<p>Calculate the soft targets loss </p>\n": "<p>\u8ba1\u7b97\u8f6f\u76ee\u6807\u635f\u5931</p>\n",
 "<p>Calculate the true label loss </p>\n": "<p>\u8ba1\u7b97\u771f\u5b9e\u6807\u7b7e\u4e22\u5931</p>\n",
 "<p>Clear the gradients </p>\n": "<p>\u6e05\u9664\u6e10\u53d8</p>\n",
 "<p>Create configurations </p>\n": "<p>\u521b\u5efa\u914d\u7f6e</p>\n",
 "<p>Create experiment </p>\n": "<p>\u521b\u5efa\u5b9e\u9a8c</p>\n",
 "<p>Cross entropy loss for true label loss </p>\n": "<p>\u771f\u5b9e\u6807\u7b7e\u4e22\u5931\u7684\u4ea4\u53c9\u71b5\u635f\u5931</p>\n",
 "<p>Get the output logits, <span translate=no>_^_0_^_</span>, from the large model </p>\n": "<p>\u4ece\u5927\u578b\u6a21\u578b\u4e2d\u83b7\u53d6\u8f93\u51fa logit<span translate=no>_^_0_^_</span>\u3001</p>\n",
 "<p>Get the output logits, <span translate=no>_^_0_^_</span>, from the small model </p>\n": "<p>\u4ece\u5c0f\u578b\u6a21\u578b\u4e2d\u83b7\u53d6\u8f93\u51fa logits<span translate=no>_^_0_^_</span>\u3001</p>\n",
 "<p>In evaluation mode (no recording) </p>\n": "<p>\u5728\u8bc4\u4f30\u6a21\u5f0f\u4e0b\uff08\u65e0\u5f55\u97f3\uff09</p>\n",
 "<p>Initialize configs of the large model training experiment </p>\n": "<p>\u521d\u59cb\u5316\u5927\u578b\u6a21\u578b\u8bad\u7ec3\u5b9e\u9a8c\u7684\u914d\u7f6e</p>\n",
 "<p>KL Divergence loss for soft targets </p>\n": "<p>\u8f6f\u76ee\u6807\u7684 KL \u5206\u6563\u635f\u5931</p>\n",
 "<p>Large model in evaluation mode </p>\n": "<p>\u8bc4\u4f30\u6a21\u5f0f\u4e2d\u7684\u5927\u578b\u6a21\u578b</p>\n",
 "<p>Load configurations </p>\n": "<p>\u88c5\u8f7d\u914d\u7f6e</p>\n",
 "<p>Load saved configs </p>\n": "<p>\u52a0\u8f7d\u4fdd\u5b58\u7684\u914d\u7f6e</p>\n",
 "<p>Load saved model </p>\n": "<p>\u52a0\u8f7d\u5df2\u4fdd\u5b58\u7684\u6a21\u578b</p>\n",
 "<p>Log the losses </p>\n": "<p>\u8bb0\u5f55\u635f\u5931</p>\n",
 "<p>Log the model parameters and gradients on last batch of every epoch </p>\n": "<p>\u8bb0\u5f55\u6bcf\u4e2a\u7eaa\u5143\u6700\u540e\u4e00\u6279\u7684\u6a21\u578b\u53c2\u6570\u548c\u68af\u5ea6</p>\n",
 "<p>Move data to the device </p>\n": "<p>\u5c06\u6570\u636e\u79fb\u52a8\u5230\u8bbe\u5907</p>\n",
 "<p>Return the model </p>\n": "<p>\u8fd4\u56de\u6a21\u578b</p>\n",
 "<p>Save the tracked metrics </p>\n": "<p>\u4fdd\u5b58\u8ddf\u8e2a\u7684\u6307\u6807</p>\n",
 "<p>Set model for saving/loading </p>\n": "<p>\u8bbe\u7f6e\u4fdd\u5b58/\u52a0\u8f7d\u7684\u6a21\u578b</p>\n",
 "<p>Set models for saving/loading </p>\n": "<p>\u8bbe\u7f6e\u7528\u4e8e\u4fdd\u5b58/\u52a0\u8f7d\u7684\u6a21\u578b</p>\n",
 "<p>Set the loaded large model </p>\n": "<p>\u8bbe\u7f6e\u52a0\u8f7d\u7684\u5927\u578b\u6a21\u578b</p>\n",
 "<p>Set which run and checkpoint to load </p>\n": "<p>\u8bbe\u7f6e\u8981\u52a0\u8f7d\u7684\u8fd0\u884c\u548c\u68c0\u67e5\u70b9</p>\n",
 "<p>Soft targets <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8f6f\u76ee\u6807<span translate=no>_^_0_^_</span></p>\n",
 "<p>Start experiment from scratch </p>\n": "<p>\u4ece\u5934\u5f00\u59cb\u5b9e\u9a8c</p>\n",
 "<p>Start the experiment - this will load the model, and prepare everything </p>\n": "<p>\u5f00\u59cb\u5b9e\u9a8c-\u8fd9\u5c06\u52a0\u8f7d\u6a21\u578b\uff0c\u5e76\u51c6\u5907\u6240\u6709\u5185\u5bb9</p>\n",
 "<p>Start the experiment and run the training loop </p>\n": "<p>\u5f00\u59cb\u5b9e\u9a8c\u5e76\u8fd0\u884c\u8bad\u7ec3\u5faa\u73af</p>\n",
 "<p>Take optimizer step </p>\n": "<p>\u91c7\u53d6\u4f18\u5316\u5668\u6b65\u9aa4</p>\n",
 "<p>Temperature adjusted probabilities of the small model <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5c0f\u6a21\u578b\u7684\u6e29\u5ea6\u8c03\u6574\u6982\u7387<span translate=no>_^_0_^_</span></p>\n",
 "<p>Temperature, <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6e29\u5ea6\uff0c<span translate=no>_^_0_^_</span></p>\n",
 "<p>The large model </p>\n": "<p>\u5927\u578b\u6a21\u578b</p>\n",
 "<p>The small model </p>\n": "<p>\u5c0f\u6a21\u578b</p>\n",
 "<p>Train the model </p>\n": "<p>\u8bad\u7ec3\u6a21\u578b</p>\n",
 "<p>Training/Evaluation mode for the small model </p>\n": "<p>\u5c0f\u6a21\u578b\u7684\u8bad\u7ec3/\u8bc4\u4f30\u6a21\u5f0f</p>\n",
 "<p>Update global step (number of samples processed) when in training mode </p>\n": "<p>\u5728\u8bad\u7ec3\u6a21\u5f0f\u4e0b\u66f4\u65b0\u5168\u5c40\u6b65\u957f\uff08\u5904\u7406\u7684\u6837\u672c\u6570\uff09</p>\n",
 "<p>Weight for soft targets loss.</p>\n<p>The gradients produced by soft targets get scaled by <span translate=no>_^_0_^_</span>. To compensate for this the paper suggests scaling the soft targets loss by a factor of <span translate=no>_^_1_^_</span> </p>\n": "<p>\u8f6f\u76ee\u6807\u635f\u5931\u7684\u6743\u91cd\u3002</p>\n<p>\u8f6f\u76ee\u6807\u4ea7\u751f\u7684\u68af\u5ea6\u4f1a\u88ab\u7f29\u653e<span translate=no>_^_0_^_</span>\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u70b9\uff0c\u672c\u6587\u5efa\u8bae\u5c06\u8f6f\u76ee\u6807\u7684\u635f\u5931\u7f29\u5c0f\u4e00\u500d<span translate=no>_^_1_^_</span></p>\n",
 "<p>Weight for true label cross entropy loss </p>\n": "<p>\u771f\u5b9e\u6807\u7b7e\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u6743\u91cd</p>\n",
 "<p>Weighted sum of the two losses </p>\n": "<p>\u4e24\u6b21\u4e8f\u635f\u7684\u52a0\u6743\u603b\u548c</p>\n",
 "Distilling the Knowledge in a Neural Network": "\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u63d0\u70bc\u77e5\u8bc6",
 "PyTorch implementation and tutorial of the paper Distilling the Knowledge in a Neural Network.": "PyTorch \u5b9e\u73b0\u548c\u8bba\u6587\u300a\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u63d0\u70bc\u77e5\u8bc6\u300b\u7684\u6559\u7a0b\u3002"
}