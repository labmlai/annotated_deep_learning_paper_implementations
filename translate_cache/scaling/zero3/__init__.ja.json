{
 "<h1>Zero-DP Memory Optimization</h1>\n<p>This is an implementation of Zero-DP introduced in the paper <a href=\"https://arxiv.org/abs/1910.02054\">ZeRO: Memory Optimization Towards Training A Trillion Parameter Models</a>,</p>\n<p>It keeps shards of the optimizer state, gradients and parameters into multiple devices/nodes. It reduces the memory consumption to <span translate=no>_^_0_^_</span> of the original model, where <span translate=no>_^_1_^_</span> is the number of parameters, <span translate=no>_^_2_^_</span> is the number of shards,  and <span translate=no>_^_3_^_</span> is number of optimizer bytes per parameter. <span translate=no>_^_4_^_</span> are the parameter and gradient memory assuming 16-bit precision; i.e. 2 bytes per parameter and gradient. <span translate=no>_^_5_^_</span> for Adam optimizer because it maintains a copy of parameters, and two moments per parameter in fp32.</p>\n<p>The communication volume of Zero-DP is <span translate=no>_^_6_^_</span>. For comparison data-parallel training has a communication volume of <span translate=no>_^_7_^_</span>.</p>\n<p>Although this is named <span translate=no>_^_8_^_</span>, we have only implemented the Zero-DP part of it and not the  Zero-R memory optimizations which target residual memory consumption. Out implementation supports training only a subset of parameters.</p>\n<p>This implementation is inspired by <a href=\"https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html\">Fairscale FSDP</a>.</p>\n<p><a href=\"finetune_neox.html\">Here&#x27;s a script to fine-tune</a> GPT NeoX using Zero-DP memory optimization.</p>\n": "<h1>\u30bc\u30ed DP \u30e1\u30e2\u30ea\u6700\u9069\u5316</h1>\n<p>\u3053\u308c\u306f\u3001\u8ad6\u6587\u300c<a href=\"https://arxiv.org/abs/1910.02054\">Zero: 1\u5146\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306b\u5411\u3051\u305f\u30e1\u30e2\u30ea\u6700\u9069\u5316\u300d\u3067\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u30bc\u30edDP\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</p>\n<p>\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u306e\u72b6\u614b\u3001\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u65ad\u7247\u3092\u8907\u6570\u306e\u30c7\u30d0\u30a4\u30b9/\u30ce\u30fc\u30c9\u306b\u4fdd\u6301\u3057\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30e1\u30e2\u30ea\u6d88\u8cbb\u91cf\u304c\u5143\u306e\u30e2\u30c7\u30eb\u3068\u540c\u3058\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u3053\u3067\u3001<span translate=no>_^_1_^_</span>\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u6570\u3001<span translate=no>_^_2_^_</span>\u306f\u30b7\u30e3\u30fc\u30c9\u306e\u6570\u3001<span translate=no>_^_3_^_</span>\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3054\u3068\u306e\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306e\u30d0\u30a4\u30c8\u6570\u3067\u3059\u3002<span translate=no>_^_0_^_</span><span translate=no>_^_4_^_</span>\u306f\u300116 \u30d3\u30c3\u30c8\u306e\u7cbe\u5ea6\u3092\u524d\u63d0\u3068\u3057\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3068\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u30e1\u30e2\u30ea\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3068\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3054\u3068\u306b 2 \u30d0\u30a4\u30c8\u3067\u3059\u3002<span translate=no>_^_5_^_</span>Adam \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u7528\u3067\u3059\u3002\u3053\u308c\u306f\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u30b3\u30d4\u30fc\u3068 fp32 \u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3054\u3068\u306b 2 \u3064\u306e\u30e2\u30fc\u30e1\u30f3\u30c8\u3092\u4fdd\u6301\u3057\u3066\u3044\u308b\u305f\u3081\u3067\u3059</p>\u3002\n<p>\u30bc\u30edDP\u306e\u901a\u4fe1\u91cf\u306f\u3002<span translate=no>_^_6_^_</span><span translate=no>_^_7_^_</span>\u6bd4\u8f03\u306e\u305f\u3081\u306b\u30c7\u30fc\u30bf\u4e26\u884c\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u901a\u4fe1\u91cf\u306f</p>.\n<p>\u3053\u308c\u306f\u540d\u524d\u304c\u4ed8\u3051\u3089\u308c\u3066\u3044\u307e\u3059\u304c<span translate=no>_^_8_^_</span>\u3001\u6b8b\u7559\u30e1\u30e2\u30ea\u6d88\u8cbb\u3092\u5bfe\u8c61\u3068\u3059\u308b\u30bc\u30edR\u30e1\u30e2\u30ea\u6700\u9069\u5316\u306f\u5b9f\u88c5\u3057\u3066\u304a\u3089\u305a\u3001DP\u304c\u30bc\u30ed\u306e\u90e8\u5206\u306e\u307f\u3092\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u5b9f\u88c5\u3067\u306f\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30b5\u30d6\u30bb\u30c3\u30c8\u306e\u307f\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059</p>\u3002\n<p><a href=\"https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html\">\u3053\u306e\u5b9f\u88c5\u306f\u30d5\u30a7\u30a2\u30b9\u30b1\u30fc\u30ebFSDP\u306b\u89e6\u767a\u3055\u308c\u3066\u3044\u307e\u3059</a>\u3002</p>\n<p><a href=\"finetune_neox.html\">\u30bc\u30edDP\u30e1\u30e2\u30ea\u6700\u9069\u5316\u3092\u4f7f\u7528\u3057\u3066GPT NeoX\u3092\u5fae\u8abf\u6574\u3059\u308b\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u6b21\u306b\u793a\u3057\u307e\u3059</a>\u3002</p>\n",
 "<h2>Sequential module for <span translate=no>_^_0_^_</span> layers</h2>\n": "<h2><span translate=no>_^_0_^_</span>\u30ec\u30a4\u30e4\u30fc\u7528\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30e2\u30b8\u30e5\u30fc\u30eb</h2>\n",
 "<h2>Zero3 Layer</h2>\n<p>Each layer of the model (or a combination of a few consecutive layers) should be wrapped in this module.</p>\n": "<h2>\u30bc\u30ed 3 \u30ec\u30a4\u30e4\u30fc</h2>\n<p>\u30e2\u30c7\u30eb\u306e\u5404\u30ec\u30a4\u30e4\u30fc\uff08\u307e\u305f\u306f\u3044\u304f\u3064\u304b\u306e\u9023\u7d9a\u3057\u305f\u30ec\u30a4\u30e4\u30fc\u306e\u7d44\u307f\u5408\u308f\u305b\uff09\u3092\u3053\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u30e9\u30c3\u30d7\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>\n",
 "<h3>Backup the gradients of the current layer</h3>\n": "<h3>\u73fe\u5728\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u3057\u307e\u3059</h3>\n",
 "<h3>Fetch the parameters from all shards</h3>\n<p>This will fetch all the parameter data from all the nodes and rebuild the parameters on each node.</p>\n": "<h3>\u3059\u3079\u3066\u306e\u30b7\u30e3\u30fc\u30c9\u304b\u3089\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u53d6\u5f97</h3>\n<p>\u3053\u308c\u306b\u3088\u308a\u3001\u3059\u3079\u3066\u306e\u30ce\u30fc\u30c9\u304b\u3089\u3059\u3079\u3066\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30c7\u30fc\u30bf\u304c\u53d6\u5f97\u3055\u308c\u3001\u5404\u30ce\u30fc\u30c9\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u518d\u69cb\u7bc9\u3055\u308c\u307e\u3059\u3002</p>\n",
 "<h3>Forward pass</h3>\n": "<h3>\u30d5\u30a9\u30ef\u30fc\u30c9\u30d1\u30b9</h3>\n",
 "<h3>Get trainable chunk/shard of the parameters.</h3>\n<p>This is what we pass on to the optimizer on the current node.</p>\n": "<h3>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30c1\u30e3\u30f3\u30af/\u30b7\u30e3\u30fc\u30c9\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002</h3>\n<p>\u3053\u308c\u3092\u73fe\u5728\u306e\u30ce\u30fc\u30c9\u306e\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306b\u6e21\u3057\u307e\u3059\u3002</p>\n",
 "<h4>Add backward hooks to the parameters of the current layer.</h4>\n": "<h4>\u73fe\u5728\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306b\u9006\u65b9\u5411\u30d5\u30c3\u30af\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002</h4>\n",
 "<h4>Cleanup the parameter data</h4>\n<p>This will release all the memory used by the layer parameters.</p>\n": "<h4>\u30d1\u30e9\u30e1\u30fc\u30bf\u30c7\u30fc\u30bf\u3092\u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7\u3059\u308b</h4>\n<p>\u3053\u308c\u306b\u3088\u308a\u3001\u30ec\u30a4\u30e4\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3067\u4f7f\u7528\u3055\u308c\u3066\u3044\u305f\u3059\u3079\u3066\u306e\u30e1\u30e2\u30ea\u304c\u89e3\u653e\u3055\u308c\u307e\u3059\u3002</p>\n",
 "<h4>Create an empty tensor of the given shape.</h4>\n": "<h4>\u4e0e\u3048\u3089\u308c\u305f\u5f62\u72b6\u306e\u7a7a\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</h4>\n",
 "<h4>Handle a backward event</h4>\n<p>This gets called by parameter backward hooks and the module backward hook.</p>\n": "<h4>\u9006\u65b9\u5411\u30a4\u30d9\u30f3\u30c8\u306e\u51e6\u7406</h4>\n<p>\u3053\u308c\u306f\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u9006\u65b9\u5411\u30d5\u30c3\u30af\u3068\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u9006\u65b9\u5411\u30d5\u30c3\u30af\u306b\u3088\u3063\u3066\u547c\u3073\u51fa\u3055\u308c\u307e\u3059\u3002</p>\n",
 "<h4>Merge all the parameters and pad it so that it&#x27;s divisible by <span translate=no>_^_0_^_</span>.</h4>\n": "<h4>\u3059\u3079\u3066\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u7d50\u5408\u3057\u3001<span translate=no>_^_0_^_</span>\u6b21\u306e\u6570\u3067\u5272\u308a\u5207\u308c\u308b\u3088\u3046\u306b\u30d1\u30c7\u30a3\u30f3\u30b0\u3057\u307e\u3059\u3002</h4>\n",
 "<h4>Module backward hook</h4>\n": "<h4>\u30e2\u30b8\u30e5\u30fc\u30eb\u5f8c\u65b9\u30d5\u30c3\u30af</h4>\n",
 "<h4>Parameter backward hook</h4>\n": "<h4>\u30d1\u30e9\u30e1\u30fc\u30bf\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u30d5\u30c3\u30af</h4>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p>Accumulate the gradients of each shard. It scatters the buffers across the nodes, and each node accumulates (reduces) the tensors it receives. </p>\n": "<p>\u5404\u30b7\u30e3\u30fc\u30c9\u306e\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u7d2f\u7a4d\u3057\u307e\u3059\u3002\u30d0\u30c3\u30d5\u30a1\u3092\u30ce\u30fc\u30c9\u5168\u4f53\u306b\u5206\u6563\u3055\u305b\u3001\u5404\u30ce\u30fc\u30c9\u306f\u53d7\u3051\u53d6\u3063\u305f\u30c6\u30f3\u30bd\u30eb\u3092\u84c4\u7a4d\uff08\u524a\u6e1b\uff09\u3057\u307e\u3059</p>\u3002\n",
 "<p>Add a backward hook. This gets called when the gradients relative to the module are computed. </p>\n": "<p>\u5f8c\u65b9\u30d5\u30c3\u30af\u3092\u8ffd\u52a0\u3002\u3053\u308c\u306f\u3001\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u57fa\u6e96\u3068\u3057\u305f\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u304c\u8a08\u7b97\u3055\u308c\u308b\u3068\u304d\u306b\u547c\u3073\u51fa\u3055\u308c\u307e\u3059</p>\u3002\n",
 "<p>Add backward hooks to the parameters of the current layer if autograd is enabled. </p>\n": "<p>autograd \u304c\u6709\u52b9\u306b\u306a\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u73fe\u5728\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306b\u9006\u65b9\u5411\u30d5\u30c3\u30af\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002</p>\n",
 "<p>Add the backward hook </p>\n": "<p>\u5f8c\u65b9\u30d5\u30c3\u30af\u3092\u8ffd\u52a0</p>\n",
 "<p>All parameters should have the same type </p>\n": "<p>\u3059\u3079\u3066\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u540c\u3058\u30bf\u30a4\u30d7\u3067\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093</p>\n",
 "<p>An empty tensor to receive the trainable and fixed parameters combined </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3068\u56fa\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u53d7\u3051\u53d6\u308b\u7a7a\u306e\u30c6\u30f3\u30bd\u30eb</p>\n",
 "<p>Assign the values from the continuous tensor </p>\n": "<p>\u9023\u7d9a\u30c6\u30f3\u30bd\u30eb\u304b\u3089\u5024\u3092\u5272\u308a\u5f53\u3066\u307e\u3059</p>\n",
 "<p>Broadcast the sizes </p>\n": "<p>\u30b5\u30a4\u30ba\u3092\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8</p>\n",
 "<p>Buffer to store the gradients </p>\n": "<p>\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u4fdd\u5b58\u3059\u308b\u30d0\u30c3\u30d5\u30a1</p>\n",
 "<p>CUDA stream to back up (accumulate) gradients </p>\n": "<p>\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\uff08\u84c4\u7a4d\uff09\u3059\u308bCUDA\u30b9\u30c8\u30ea\u30fc\u30e0</p>\n",
 "<p>CUDA stream to backup/accumulate gradients </p>\n": "<p>\u52fe\u914d\u3092\u30d0\u30c3\u30af\u30a2\u30c3\u30d7/\u84c4\u7a4d\u3059\u308b\u305f\u3081\u306eCUDA\u30b9\u30c8\u30ea\u30fc\u30e0</p>\n",
 "<p>CUDA stream to featch parameters </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b CUDA \u30b9\u30c8\u30ea\u30fc\u30e0</p>\n",
 "<p>CUDA stream to fetch parameters </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e CUDA \u30b9\u30c8\u30ea\u30fc\u30e0</p>\n",
 "<p>Calculate the chunk sizes of trainable and fixed params </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u56fa\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30c1\u30e3\u30f3\u30af\u30b5\u30a4\u30ba\u3092\u8a08\u7b97\u3057\u307e\u3059</p>\n",
 "<p>Change the storage size of the parameter. This was set to <span translate=no>_^_0_^_</span> when we cleaned up the parameters. </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30b9\u30c8\u30ec\u30fc\u30b8\u30b5\u30a4\u30ba\u3092\u5909\u66f4\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001<span translate=no>_^_0_^_</span>\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7\u3057\u305f\u3068\u304d\u306b\u8a2d\u5b9a\u3055\u308c\u307e\u3057\u305f\u3002</p>\n",
 "<p>Check to make sure the parameter is not sharing storage with anything else </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u4ed6\u306e\u3082\u306e\u3068\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u5171\u6709\u3057\u3066\u3044\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Clean the gradients </p>\n": "<p>\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u304d\u308c\u3044\u306b\u3059\u308b</p>\n",
 "<p>Cleanup the normal parameters </p>\n": "<p>\u6a19\u6e96\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7</p>\n",
 "<p>Cleanup the parameters of the layer.</p>\n<p><em>Skip cleaning up if autograd is enabled and this is the last layer in the network, because we will need to fetch the parameters again for the backward pass.</em> </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7\u3057\u307e\u3059\u3002</p>\n<p><em>autograd \u304c\u6709\u52b9\u306b\u306a\u3063\u3066\u3044\u3066\u3001\u3053\u308c\u304c\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6700\u5f8c\u306e\u30ec\u30a4\u30e4\u30fc\u3067\u3042\u308b\u5834\u5408\u306f\u3001\u5f8c\u65b9\u30d1\u30b9\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u518d\u5ea6\u53d6\u5f97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7\u3092\u30b9\u30ad\u30c3\u30d7\u3057\u3066\u304f\u3060\u3055\u3044\u3002</em></p>\n",
 "<p>Collect all the parameters of the layer </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u306e\u3059\u3079\u3066\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002</p>\n",
 "<p>Collect gradients </p>\n": "<p>\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u96c6\u3081\u308b</p>\n",
 "<p>Collect the chunk data </p>\n": "<p>\u30c1\u30e3\u30f3\u30af\u30c7\u30fc\u30bf\u3092\u53ce\u96c6\u3059\u308b</p>\n",
 "<p>Collect the individual parameter tensors </p>\n": "<p>\u500b\u3005\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30c6\u30f3\u30bd\u30eb\u3092\u53ce\u96c6\u3059\u308b</p>\n",
 "<p>Compute the outputs of the current layer </p>\n": "<p>\u73fe\u5728\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u51fa\u529b\u3092\u8a08\u7b97\u3057\u307e\u3059</p>\n",
 "<p>Concatenate all the parameters and pad it </p>\n": "<p>\u3059\u3079\u3066\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u9023\u7d50\u3057\u3066\u30d1\u30c7\u30a3\u30f3\u30b0\u3057\u307e\u3059\u3002</p>\n",
 "<p>Concatenate both trainable and fixed chunks </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30c1\u30e3\u30f3\u30af\u3068\u56fa\u5b9a\u30c1\u30e3\u30f3\u30af\u306e\u4e21\u65b9\u3092\u9023\u7d50\u3059\u308b</p>\n",
 "<p>Concatenate both trainable and fixed params </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u56fa\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u4e21\u65b9\u3092\u9023\u7d50\u3059\u308b</p>\n",
 "<p>Create an empty padding tensor </p>\n": "<p>\u7a7a\u306e\u30d1\u30c7\u30a3\u30f3\u30b0\u30c6\u30f3\u30bd\u30eb\u306e\u4f5c\u6210</p>\n",
 "<p>Create an empty tensor to receive the parameters </p>\n": "<p>\u7a7a\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u4f5c\u6210\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u53d7\u3051\u53d6\u308b</p>\n",
 "<p>Create an empty tensor to receive the sizes </p>\n": "<p>\u7a7a\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u4f5c\u6210\u3057\u3066\u30b5\u30a4\u30ba\u3092\u53d7\u3051\u53d6\u308b</p>\n",
 "<p>Create parameters for trainable (<span translate=no>_^_0_^_</span>) and fixed (<span translate=no>_^_1_^_</span>) parameters to be stored in current device/node </p>\n": "<p>trainable (<span translate=no>_^_0_^_</span>) \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3068 fixed () \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u4f5c\u6210\u3057\u3066\u3001\u73fe\u5728\u306e\u30c7\u30d0\u30a4\u30b9/\u30ce\u30fc\u30c9\u306b\u4fdd\u5b58\u3057\u307e\u3059 <span translate=no>_^_1_^_</span></p>\n",
 "<p>Data type of the layer </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u306e\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7</p>\n",
 "<p>Decrement the hooks counter </p>\n": "<p>\u30d5\u30c3\u30af\u30ab\u30a6\u30f3\u30bf\u30fc\u3092\u30c7\u30af\u30ea\u30e1\u30f3\u30c8\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Device of the layer </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u306e\u30c7\u30d0\u30a4\u30b9</p>\n",
 "<p>Each shard keeps parameters in <span translate=no>_^_0_^_</span> list. The <span translate=no>_^_1_^_</span> is for trainable parameters and <span translate=no>_^_2_^_</span> is for fixed parameters. </p>\n": "<p><span translate=no>_^_0_^_</span>\u5404\u30b7\u30e3\u30fc\u30c9\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30ea\u30b9\u30c8\u306b\u4fdd\u6301\u3057\u307e\u3059\u3002<span translate=no>_^_1_^_</span>\u306f\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u7528\u3067\u3001<span translate=no>_^_2_^_</span>\u56fa\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u7528\u3067\u3059</p>\u3002\n",
 "<p>Empty tensor to accumulate the gradients of the current shard </p>\n": "<p>\u30c6\u30f3\u30bd\u30eb\u3092\u7a7a\u306b\u3059\u308b\u3068\u3001\u73fe\u5728\u306e\u30b7\u30e3\u30fc\u30c9\u306e\u52fe\u914d\u304c\u84c4\u7a4d\u3055\u308c\u307e\u3059</p>\n",
 "<p>Fetch all the parameters of the current node. This gets called by the previous layer so this call is just to make sure parameters are fetched. </p>\n": "<p>\u73fe\u5728\u306e\u30ce\u30fc\u30c9\u306e\u3059\u3079\u3066\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u524d\u306e\u30ec\u30a4\u30e4\u30fc\u304b\u3089\u547c\u3073\u51fa\u3055\u308c\u308b\u306e\u3067\u3001\u3053\u306e\u547c\u3073\u51fa\u3057\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u30d5\u30a7\u30c3\u30c1\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u3060\u3051\u306e\u3082\u306e\u3067\u3059</p>\u3002\n",
 "<p>Forward pass </p>\n": "<p>\u30d5\u30a9\u30ef\u30fc\u30c9\u30d1\u30b9</p>\n",
 "<p>Gather the parameters from all the nodes/devices </p>\n": "<p>\u3059\u3079\u3066\u306e\u30ce\u30fc\u30c9/\u30c7\u30d0\u30a4\u30b9\u304b\u3089\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u53ce\u96c6\u3057\u307e\u3059</p>\n",
 "<p>Get a handle to add the backward hook. <a href=\"https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00\">This blog discusses about <span translate=no>_^_0_^_</span></a>. </p>\n": "<p>\u5f8c\u308d\u5411\u304d\u30d5\u30c3\u30af\u3092\u53d6\u308a\u4ed8\u3051\u308b\u305f\u3081\u306e\u30cf\u30f3\u30c9\u30eb\u3092\u7528\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002<a href=\"https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00\">\u3053\u306e\u30d6\u30ed\u30b0\u3067\u306f\u3001. \u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059<span translate=no>_^_0_^_</span></a></p>\u3002\n",
 "<p>Handle a backward event </p>\n": "<p>\u9006\u65b9\u5411\u30a4\u30d9\u30f3\u30c8\u306e\u51e6\u7406</p>\n",
 "<p>If all the hooks (including the module hook) have been called, then we can back up gradients and clean up the parameters. </p>\n": "<p>\u3059\u3079\u3066\u306e\u30d5\u30c3\u30af (\u30e2\u30b8\u30e5\u30fc\u30eb\u30d5\u30c3\u30af\u3092\u542b\u3080) \u304c\u547c\u3073\u51fa\u3055\u308c\u305f\u3089\u3001\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7\u3067\u304d\u307e\u3059\u3002</p>\n",
 "<p>If it is not divisible by <span translate=no>_^_0_^_</span>, pad it </p>\n": "<p><span translate=no>_^_0_^_</span>\u5272\u308a\u5207\u308c\u306a\u3044\u5834\u5408\u306f\u30d1\u30c7\u30a3\u30f3\u30b0\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>If there are no parameters, skip </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u306a\u3044\u5834\u5408\u306f\u30b9\u30ad\u30c3\u30d7\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Increment the number of hooks added </p>\n": "<p>\u8ffd\u52a0\u3059\u308b\u30d5\u30c3\u30af\u306e\u6570\u3092\u5897\u3084\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Initialize the properties </p>\n": "<p>\u30d7\u30ed\u30d1\u30c6\u30a3\u3092\u521d\u671f\u5316</p>\n",
 "<p>Iterate through all parameters </p>\n": "<p>\u3059\u3079\u3066\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u53cd\u5fa9\u51e6\u7406</p>\n",
 "<p>Iterate through model parameters and assign the values from the continuous tensor </p>\n": "<p>\u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u7e70\u308a\u8fd4\u3057\u51e6\u7406\u3057\u3001\u9023\u7d9a\u30c6\u30f3\u30bd\u30eb\u304b\u3089\u5024\u3092\u5272\u308a\u5f53\u3066\u307e\u3059</p>\n",
 "<p>Iterate through trainable parameters </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u7e70\u308a\u8fd4\u3057\u51e6\u7406</p>\n",
 "<p>Keep a reference to the handle </p>\n": "<p>\u30cf\u30f3\u30c9\u30eb\u3078\u306e\u53c2\u7167\u3092\u5fd8\u308c\u305a\u306b</p>\n",
 "<p>List of layers right after this layer </p>\n": "<p>\u3053\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u76f4\u5f8c\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u30ea\u30b9\u30c8</p>\n",
 "<p>List of layers right before this layer </p>\n": "<p>\u3053\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u76f4\u524d\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u30ea\u30b9\u30c8</p>\n",
 "<p>Loop through trainable parameters of the current layer </p>\n": "<p>\u73fe\u5728\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u30eb\u30fc\u30d7\u30b9\u30eb\u30fc\u3059\u308b</p>\n",
 "<p>Make sure a hook hasn&#x27;t already been added </p>\n": "<p>\u30d5\u30c3\u30af\u304c\u307e\u3060\u8ffd\u52a0\u3055\u308c\u3066\u3044\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Make sure gradient back up is complete </p>\n": "<p>\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u304c\u5b8c\u4e86\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Make sure the parameter has no gradient data </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u52fe\u914d\u30c7\u30fc\u30bf\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Merge and pad trainable (<span translate=no>_^_0_^_</span>) and fixed (<span translate=no>_^_1_^_</span>) parameters </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd (<span translate=no>_^_0_^_</span>) \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3068\u56fa\u5b9a (<span translate=no>_^_1_^_</span>) \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u30de\u30fc\u30b8\u3057\u3066\u30d1\u30c7\u30a3\u30f3\u30b0\u3059\u308b</p>\n",
 "<p>Number of backward hooks added </p>\n": "<p>\u8ffd\u52a0\u3055\u308c\u305f\u5f8c\u65b9\u30d5\u30c3\u30af\u306e\u6570</p>\n",
 "<p>Number of nodes/devices the data is sharded across </p>\n": "<p>\u30c7\u30fc\u30bf\u304c\u30b7\u30e3\u30fc\u30c7\u30a3\u30f3\u30b0\u3055\u308c\u308b\u30ce\u30fc\u30c9/\u30c7\u30d0\u30a4\u30b9\u306e\u6570</p>\n",
 "<p>Offset of the continuous buffer </p>\n": "<p>\u9023\u7d9a\u30d0\u30c3\u30d5\u30a1\u306e\u30aa\u30d5\u30bb\u30c3\u30c8</p>\n",
 "<p>Offset of the continuous tensor </p>\n": "<p>\u9023\u7d9a\u30c6\u30f3\u30bd\u30eb\u306e\u30aa\u30d5\u30bb\u30c3\u30c8</p>\n",
 "<p>Original parameter shape </p>\n": "<p>\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30b7\u30a7\u30a4\u30d7</p>\n",
 "<p>Otherwise, no need to pad </p>\n": "<p>\u305d\u308c\u4ee5\u5916\u306e\u5834\u5408\u306f\u3001\u30d1\u30c3\u30c9\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093</p>\n",
 "<p>Receive the parameters </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u53d7\u3051\u53d6\u308b</p>\n",
 "<p>Receive the sizes </p>\n": "<p>\u30b5\u30a4\u30ba\u3092\u53d7\u3051\u53d6\u308b</p>\n",
 "<p>Remove the handle from the parameter </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u304b\u3089\u30cf\u30f3\u30c9\u30eb\u3092\u524a\u9664\u3057\u307e\u3059</p>\n",
 "<p>Reshape the trainable and fixed parameters to continuous tensors </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3068\u56fa\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u9023\u7d9a\u30c6\u30f3\u30bd\u30eb\u306b\u30ea\u30b7\u30a7\u30a4\u30d7</p>\n",
 "<p>Resize the storage to <span translate=no>_^_0_^_</span>. This will release the memory used by the parameter.</p>\n<p><strong>Setting <span translate=no>_^_1_^_</span> will not release the memory, since the autograd graph keeps a reference to it.</strong> </p>\n": "<p>\u30b9\u30c8\u30ec\u30fc\u30b8\u306e\u30b5\u30a4\u30ba\u3092\u306b\u5909\u66f4\u3057\u307e\u3059\u3002<span translate=no>_^_0_^_</span>\u3053\u308c\u306b\u3088\u308a\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u4f7f\u7528\u3057\u3066\u3044\u305f\u30e1\u30e2\u30ea\u304c\u89e3\u653e\u3055\u308c\u307e\u3059\u3002</p>\n<p><strong>autograd <span translate=no>_^_1_^_</span> \u30b0\u30e9\u30d5\u306f\u30e1\u30e2\u30ea\u3078\u306e\u53c2\u7167\u3092\u4fdd\u6301\u3059\u308b\u306e\u3067\u3001\u8a2d\u5b9a\u3057\u3066\u3082\u30e1\u30e2\u30ea\u306f\u89e3\u653e\u3055\u308c\u307e\u305b\u3093\u3002</strong></p>\n",
 "<p>Return and empty list if there are no trainable parameters </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u306a\u3044\u5834\u5408\u306f\u30ea\u30b9\u30c8\u3092\u8fd4\u3057\u3066\u7a7a\u306b\u3059\u308b</p>\n",
 "<p>Return the list of trainable chunks from each layer </p>\n": "<p>\u5404\u30ec\u30a4\u30e4\u30fc\u304b\u3089\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30c1\u30e3\u30f3\u30af\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3057\u307e\u3059</p>\n",
 "<p>Return the trainable chunk as a list </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30c1\u30e3\u30f3\u30af\u3092\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8fd4\u3059</p>\n",
 "<p>Scatter them to all the nodes/devices </p>\n": "<p>\u305d\u308c\u3089\u3092\u3059\u3079\u3066\u306e\u30ce\u30fc\u30c9/\u30c7\u30d0\u30a4\u30b9\u306b\u6563\u3089\u3059</p>\n",
 "<p>Separate parameters as trainable and fixed </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u56fa\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5206\u96e2</p>\n",
 "<p>Set layer index </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8a2d\u5b9a</p>\n",
 "<p>Set preceding layers </p>\n": "<p>\u524d\u306e\u30ec\u30a4\u30e4\u30fc\u3092\u8a2d\u5b9a</p>\n",
 "<p>Set proceeding layers </p>\n": "<p>\u9032\u884c\u4e2d\u306e\u30ec\u30a4\u30e4\u30fc\u3092\u8a2d\u5b9a</p>\n",
 "<p>Set streams </p>\n": "<p>\u30b9\u30c8\u30ea\u30fc\u30e0\u3092\u8a2d\u5b9a</p>\n",
 "<p>Set the chunk gradients. This is what the optimizer sees. </p>\n": "<p>\u30c1\u30e3\u30f3\u30af\u306e\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\u3053\u308c\u304c\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306e\u898b\u89e3\u3067\u3059</p>\u3002\n",
 "<p>Set the flag </p>\n": "<p>\u30d5\u30e9\u30b0\u3092\u8a2d\u5b9a</p>\n",
 "<p>Set the flag to indicate that the parameters are not fetched </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u30d5\u30a7\u30c3\u30c1\u3055\u308c\u306a\u3044\u3053\u3068\u3092\u793a\u3059\u30d5\u30e9\u30b0\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002</p>\n",
 "<p>Set the streams and preceding and proceeding layers for each <span translate=no>_^_0_^_</span> layer </p>\n": "<p>\u5404\u30ec\u30a4\u30e4\u30fc\u306e\u30b9\u30c8\u30ea\u30fc\u30e0\u3068\u5148\u884c\u30ec\u30a4\u30e4\u30fc\u3068\u5f8c\u7d9a\u30ec\u30a4\u30e4\u30fc\u3092\u8a2d\u5b9a\u3057\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Skip if there are no trainable parameters </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u306a\u3044\u5834\u5408\u306f\u30b9\u30ad\u30c3\u30d7</p>\n",
 "<p>Skip if there&#x27;s nothing to fetch or share. </p>\n": "<p>\u53d6\u5f97\u307e\u305f\u306f\u5171\u6709\u3059\u308b\u3082\u306e\u304c\u306a\u3044\u5834\u5408\u306f\u30b9\u30ad\u30c3\u30d7\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>\n",
 "<p>Skip is already fetched </p>\n": "<p>\u30b9\u30ad\u30c3\u30d7\u306f\u3059\u3067\u306b\u53d6\u5f97\u3055\u308c\u3066\u3044\u307e\u3059</p>\n",
 "<p>Split the continuous buffer into number of nodes. These splits are views of `buffer&#x27;. </p>\n": "<p>\u9023\u7d9a\u30d0\u30c3\u30d5\u30a1\u3092\u8907\u6570\u306e\u30ce\u30fc\u30c9\u306b\u5206\u5272\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u5206\u5272\u306f\u300cbuffer\u300d\u306e\u30d3\u30e5\u30fc\u3067\u3059</p>\u3002\n",
 "<p>Split the continuous buffer into the number of nodes. These splits are views of `buffer&#x27;. </p>\n": "<p>\u9023\u7d9a\u30d0\u30c3\u30d5\u30a1\u3092\u30ce\u30fc\u30c9\u6570\u306b\u5206\u5272\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u5206\u5272\u306f\u300cbuffer\u300d\u306e\u30d3\u30e5\u30fc\u3067\u3059</p>\u3002\n",
 "<p>Split the gathered parameters into the trainable and fixed chunks </p>\n": "<p>\u53ce\u96c6\u3057\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30c1\u30e3\u30f3\u30af\u3068\u56fa\u5b9a\u30c1\u30e3\u30f3\u30af\u306b\u5206\u5272\u3057\u307e\u3059</p>\n",
 "<p>Start fetch parameters of the previous layer, because autograd will next process the gradients of it. </p>\n": "<p>autograd \u304c\u6b21\u306b\u305d\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u51e6\u7406\u3059\u308b\u306e\u3067\u3001\u524d\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u53d6\u5f97\u3092\u958b\u59cb\u3057\u307e\u3059\u3002</p>\n",
 "<p>Start fetching parameters of the proceeding layers, so that they will fetch them which the current layer does its computations. </p>\n": "<p>\u51e6\u7406\u4e2d\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u53d6\u5f97\u3092\u958b\u59cb\u3057\u307e\u3059\u3002\u305d\u3046\u3059\u308c\u3070\u3001\u73fe\u5728\u306e\u30ec\u30a4\u30e4\u30fc\u304c\u8a08\u7b97\u3092\u884c\u3046\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u53d6\u5f97\u3055\u308c\u307e\u3059\u3002</p>\n",
 "<p>Store list of modules </p>\n": "<p>\u30e2\u30b8\u30e5\u30fc\u30eb\u4e00\u89a7\u3092\u4fdd\u5b58</p>\n",
 "<p>Store the shape of the parameters because we need it later to reconstruct them </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5f62\u72b6\u3092\u4fdd\u5b58\u3057\u3066\u304a\u304d\u307e\u3059\u3002\u5f8c\u3067\u518d\u69cb\u7bc9\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3089\u3067\u3059\u3002</p>\n",
 "<p>The <span translate=no>_^_0_^_</span> node will calculate the size each device/node should store, and distribute the parameters accordingly. </p>\n": "<p><span translate=no>_^_0_^_</span>\u30ce\u30fc\u30c9\u306f\u3001\u5404\u30c7\u30d0\u30a4\u30b9/\u30ce\u30fc\u30c9\u304c\u4fdd\u5b58\u3059\u308b\u30b5\u30a4\u30ba\u3092\u8a08\u7b97\u3057\u3001\u305d\u308c\u306b\u5fdc\u3058\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5206\u6563\u3057\u307e\u3059\u3002</p>\n",
 "<p>The first chunk is for trainable parameters. </p>\n": "<p>\u6700\u521d\u306e\u30c1\u30e3\u30f3\u30af\u306f\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u7528\u3067\u3059\u3002</p>\n",
 "<p>The module to be wrapped </p>\n": "<p>\u30e9\u30c3\u30d7\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb</p>\n",
 "<p>The position of the current layer; used this for debugging logs </p>\n": "<p>\u73fe\u5728\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u4f4d\u7f6e\u3002\u3053\u308c\u3092\u30ed\u30b0\u306e\u30c7\u30d0\u30c3\u30b0\u306b\u4f7f\u7528\u3057\u307e\u3057\u305f</p>\n",
 "<p>The previous layer will start computing gradients. We need to make sure it has finished fetching params. </p>\n": "<p>\u524d\u306e\u30ec\u30a4\u30e4\u30fc\u304c\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u8a08\u7b97\u3092\u958b\u59cb\u3057\u307e\u3059\u3002\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u53d6\u5f97\u304c\u5b8c\u4e86\u3057\u305f\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</p>\u3002\n",
 "<p>This is the list of parameters split into lists as trainable and fixed parameters. </p>\n": "<p>\u3053\u308c\u306f\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3068\u56fa\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3068\u3057\u3066\u30ea\u30b9\u30c8\u306b\u5206\u5272\u3055\u308c\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u30ea\u30b9\u30c8\u3067\u3059\u3002</p>\n",
 "<p>This is the sizes of the chunks in <span translate=no>_^_0_^_</span> list. </p>\n": "<p><span translate=no>_^_0_^_</span>\u3053\u308c\u306f\u30ea\u30b9\u30c8\u5185\u306e\u30c1\u30e3\u30f3\u30af\u306e\u30b5\u30a4\u30ba\u3067\u3059\u3002</p>\n",
 "<p>Total number of parameters </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u7dcf\u6570</p>\n",
 "<p>Update the offset </p>\n": "<p>\u30aa\u30d5\u30bb\u30c3\u30c8\u306e\u66f4\u65b0</p>\n",
 "<p>Use <span translate=no>_^_0_^_</span> to create an autograd step which we can intercept </p>\n": "<p><span translate=no>_^_0_^_</span>\u30a4\u30f3\u30bf\u30fc\u30bb\u30d7\u30c8\u3067\u304d\u308b\u30aa\u30fc\u30c8\u30b0\u30e9\u30fc\u30c9\u306e\u30b9\u30c6\u30c3\u30d7\u3092\u4f5c\u308b\u306e\u306b\u4f7f\u3046</p>\n",
 "<p>Use <span translate=no>_^_0_^_</span> to fetch the parameters from all the shards </p>\n": "<p><span translate=no>_^_0_^_</span>\u3092\u4f7f\u7528\u3057\u3066\u3059\u3079\u3066\u306e\u30b7\u30e3\u30fc\u30c9\u304b\u3089\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002</p>\n",
 "<p>Use the backup stream to backup the gradients </p>\n": "<p>\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30b9\u30c8\u30ea\u30fc\u30e0\u3092\u4f7f\u7528\u3057\u3066\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u3057\u307e\u3059</p>\n",
 "<p>Wait for operations on the parameters to complete before any new operations </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u64cd\u4f5c\u304c\u5b8c\u4e86\u3059\u308b\u307e\u3067\u5f85\u3063\u3066\u304b\u3089\u3001\u65b0\u3057\u3044\u64cd\u4f5c\u3092\u884c\u3044\u307e\u3059\u3002</p>\n",
 "<p>Wait for parameter fetching to complete. </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u53d6\u5f97\u304c\u5b8c\u4e86\u3059\u308b\u307e\u3067\u304a\u5f85\u3061\u304f\u3060\u3055\u3044\u3002</p>\n",
 "<p>Wait for the gather operation to complete and then clear the references to the buffers </p>\n": "<p>\u53ce\u96c6\u64cd\u4f5c\u304c\u5b8c\u4e86\u3059\u308b\u306e\u3092\u5f85\u3063\u3066\u304b\u3089\u3001\u30d0\u30c3\u30d5\u30a1\u3078\u306e\u53c2\u7167\u3092\u30af\u30ea\u30a2\u3057\u307e\u3059\u3002</p>\n",
 "<p>Wait for the operation to complete and then clear the references to the buffers </p>\n": "<p>\u64cd\u4f5c\u304c\u5b8c\u4e86\u3059\u308b\u306e\u3092\u5f85\u3063\u3066\u304b\u3089\u3001\u30d0\u30c3\u30d5\u30a1\u3078\u306e\u53c2\u7167\u3092\u30af\u30ea\u30a2\u3057\u307e\u3059\u3002</p>\n",
 "<p>Wait for the operation to complete before other operations can be performed </p>\n": "<p>\u64cd\u4f5c\u304c\u5b8c\u4e86\u3059\u308b\u307e\u3067\u5f85\u3063\u3066\u304b\u3089\u3001\u4ed6\u306e\u64cd\u4f5c\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Wait for the operations to complete before other operations can be performed </p>\n": "<p>\u64cd\u4f5c\u304c\u5b8c\u4e86\u3059\u308b\u307e\u3067\u5f85\u3063\u3066\u304b\u3089\u3001\u4ed6\u306e\u64cd\u4f5c\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Whether parameters have been fetched </p>\n": "<p>\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u53d6\u5f97\u3055\u308c\u3066\u3044\u308b\u304b\u3069\u3046\u304b</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  List of <span translate=no>_^_1_^_</span> layers</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u30ec\u30a4\u30e4\u30fc\u306e\u30ea\u30b9\u30c8</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  The module to be wrapped. </li>\n<li><span translate=no>_^_1_^_</span>  The rank of the current node. </li>\n<li><span translate=no>_^_2_^_</span>  The number of nodes/devices the data is sharded across. </li>\n<li><span translate=no>_^_3_^_</span>  The device of the layer. </li>\n<li><span translate=no>_^_4_^_</span>  The data type of the layer.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30e9\u30c3\u30d7\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u3002</li>\n<li><span translate=no>_^_1_^_</span>\u73fe\u5728\u306e\u30ce\u30fc\u30c9\u306e\u30e9\u30f3\u30af\u3002</li>\n<li><span translate=no>_^_2_^_</span>\u30c7\u30fc\u30bf\u304c\u30b7\u30e3\u30fc\u30c7\u30a3\u30f3\u30b0\u3055\u308c\u308b\u30ce\u30fc\u30c9/\u30c7\u30d0\u30a4\u30b9\u306e\u6570\u3002</li>\n<li><span translate=no>_^_3_^_</span>\u30ec\u30a4\u30e4\u30fc\u306e\u30c7\u30d0\u30a4\u30b9\u3002</li>\n<li><span translate=no>_^_4_^_</span>\u30ec\u30a4\u30e4\u30fc\u306e\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u3002</li></ul>\n",
 "This is an implementation of Zero-DP Memory Optimization written in PyTorch.": "\u3053\u308c\u306f PyTorch \u3067\u66f8\u304b\u308c\u305f\u30bc\u30ed DP \u30e1\u30e2\u30ea\u6700\u9069\u5316\u306e\u5b9f\u88c5\u3067\u3059\u3002",
 "Zero-DP Memory Optimization": "\u30bc\u30ed DP \u30e1\u30e2\u30ea\u6700\u9069\u5316"
}